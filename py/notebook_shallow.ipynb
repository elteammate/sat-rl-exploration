{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:52:58.420577Z",
     "start_time": "2024-12-07T15:52:58.415414Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch_geometric.nn as gnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import pyvis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import srunner\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Protocol, cast\n",
    "import enum\n",
    "import dataclasses\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import uuid\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:52:58.476065Z",
     "start_time": "2024-12-07T15:52:58.471669Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"notebook\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "np.seterr(all='raise')\n",
    "DEV = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Counters:\n",
    "    episodes: int = 0 \n",
    "    epochs: int = 0\n",
    "    runs: int = 0\n",
    "    steps: int = 0\n",
    "    batches: int = 0\n",
    "    train_steps: int = 0\n",
    "\n",
    "    def from_dict(self, d):\n",
    "        for k, v in d.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "COUNTERS = Counters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:52:58.698139Z",
     "start_time": "2024-12-07T15:52:58.695677Z"
    }
   },
   "outputs": [],
   "source": [
    "from primitives import Clause\n",
    "\n",
    "class NodeType(enum.Enum):\n",
    "    VARIABLE = 0\n",
    "    REDUNDANT = 1\n",
    "    IRREDUNDANT = 2\n",
    "\n",
    "class CnfGraph(Protocol):\n",
    "    global_data: torch.Tensor\n",
    "    x: torch.Tensor\n",
    "    edge_index: torch.Tensor\n",
    "    edge_attr: torch.Tensor\n",
    "    node_type: torch.Tensor\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DimInfo:\n",
    "    num_global_features: int\n",
    "    num_var_features: int\n",
    "    num_clause_features: int\n",
    "    num_edge_features: int\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class ReductionProblem:\n",
    "    num_vars: int\n",
    "    levels: list[int]\n",
    "    vals: list[int]\n",
    "    clauses: list[Clause]\n",
    "    reducible_ids: list[int]\n",
    "    conflicts: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:52:58.722997Z",
     "start_time": "2024-12-07T15:52:58.702363Z"
    }
   },
   "outputs": [],
   "source": [
    "def problem_to_cnf_graph(problem: ReductionProblem) -> tuple[CnfGraph, DimInfo]:\n",
    "    logger.info(\"Converting problem to CNF graph\")\n",
    "    num_vars = problem.num_vars\n",
    "\n",
    "    fixed_vals = [problem.vals[i] if problem.levels[i] == 0 else 0 for i in range(num_vars)]\n",
    "\n",
    "    clauses = []\n",
    "    for clause in problem.clauses:\n",
    "        new_lits = []\n",
    "        for lit in clause.lits:\n",
    "            v = abs(lit) - 1\n",
    "            if fixed_vals[v] == 0:\n",
    "                new_lits.append(lit)\n",
    "            elif lit > 0 and fixed_vals[v] == 1 or lit < 0 and fixed_vals[v] == -1:\n",
    "                break\n",
    "            # else skip literal because it's falsified\n",
    "        else:\n",
    "            clauses.append(clause.with_lits(new_lits))\n",
    "\n",
    "    num_clauses = len(clauses)\n",
    "    num_free_vars = sum(1 for v in fixed_vals if v == 0)\n",
    "\n",
    "    c_clause_sizes = np.array([len(clause.lits) for clause in clauses])\n",
    "    c_clause_lbd = np.array([clause.lbd for clause in clauses])\n",
    "    g_max_clause_size = c_clause_sizes.max()\n",
    "    g_min_clause_size = c_clause_sizes.min()\n",
    "    g_mean_clause_size = c_clause_sizes.mean()\n",
    "    g_min_clause_lbd = c_clause_lbd.min()\n",
    "    g_max_clause_lbd = c_clause_lbd.max()\n",
    "    g_lbd_spread = g_max_clause_lbd - g_min_clause_lbd\n",
    "    g_mean_clause_lbd = c_clause_lbd.mean()\n",
    "    c_lbd_size_ratio = c_clause_lbd / c_clause_sizes\n",
    "    c_num_pos_literals = np.array([sum(1 for lit in clause.lits if lit > 0) for clause in clauses])\n",
    "    c_num_neg_literals = c_clause_sizes - c_num_pos_literals\n",
    "    c_pos_neg_ratios = c_num_pos_literals / c_clause_sizes\n",
    "    g_max_pos_neg_ratio = c_pos_neg_ratios.max()\n",
    "    g_min_pos_neg_ratio = c_pos_neg_ratios.min()\n",
    "    g_mean_pos_neg_ratio = c_pos_neg_ratios.mean()\n",
    "    c_horn = c_num_pos_literals <= 1\n",
    "    g_horn_ratio = c_horn.mean()\n",
    "    c_inv_horn = c_num_neg_literals <= 1\n",
    "    g_inv_horn_ratio = c_inv_horn.mean()\n",
    "    g_size_spread = g_max_clause_size - g_min_clause_size\n",
    "    g_pos_neg_ratio_spread = g_max_pos_neg_ratio - g_min_pos_neg_ratio\n",
    "    c_conflicts_on_creation = np.array([clause.conflicts_on_creation for clause in clauses])\n",
    "    c_time = c_conflicts_on_creation / problem.conflicts\n",
    "    c_activity = np.array([clause.activity for clause in clauses], dtype=np.float32)\n",
    "    c_activity /= max(c_activity.max(), 1.0)\n",
    "    c_activity_top10 = c_activity > np.percentile(c_activity, 90)\n",
    "    c_time_top10 = c_time > np.percentile(c_time, 90)\n",
    "    c_times_reason = np.array([clause.times_reason for clause in clauses], dtype=np.float32)\n",
    "    c_times_reason /= max(c_times_reason.max(), 1.0)\n",
    "    c_times_reason_top10 = c_times_reason > np.percentile(c_times_reason, 90)\n",
    "\n",
    "\n",
    "    v_pos = np.zeros(num_vars)\n",
    "    v_neg = np.zeros(num_vars)\n",
    "    v_horn = np.zeros(num_vars)\n",
    "    v_invhorn = np.zeros(num_vars)\n",
    "\n",
    "    for i, clause in enumerate(clauses):\n",
    "        for lit in clause.lits:\n",
    "            v = abs(lit) - 1\n",
    "            if lit > 0:\n",
    "                v_pos[v] += 1\n",
    "            else:\n",
    "                v_neg[v] += 1\n",
    "            if c_horn[i]:\n",
    "                v_horn[v] += 1\n",
    "            if c_inv_horn[i]:\n",
    "                v_invhorn[v] += 1\n",
    "\n",
    "    v_count = v_pos + v_neg\n",
    "    v_count_non_zero = np.maximum(v_count, 1)\n",
    "    v_pos_neg_ratios = v_pos / v_count_non_zero\n",
    "    v_horn_ratio = v_horn / v_count_non_zero\n",
    "    v_invhorn_ratio = v_invhorn / v_count_non_zero\n",
    "    g_var_horn_min = v_horn.min()\n",
    "    g_var_horn_max = v_horn.max()\n",
    "    g_var_horn_mean = v_horn.mean()\n",
    "    g_var_invhorn_min = v_invhorn.min()\n",
    "    g_var_invhorn_max = v_invhorn.max()\n",
    "    g_var_invhorn_mean = v_invhorn.mean()\n",
    "    g_var_horn_std = v_horn.std()\n",
    "    g_var_invhorn_std = v_invhorn.std()\n",
    "    g_var_pos_neg_ratio_std = v_pos_neg_ratios.std()\n",
    "    g_var_count_std = v_count.std()\n",
    "    g_var_clause_ratio = num_free_vars / num_clauses\n",
    "    g_free_ratio = num_free_vars / num_vars\n",
    "\n",
    "    c_clause_keep = np.array([clause.keep for clause in clauses])\n",
    "    c_clause_redundant = np.array([clause.redundant for clause in clauses])\n",
    "\n",
    "    global_features = [\n",
    "        g_mean_clause_size,\n",
    "        g_mean_clause_lbd,\n",
    "        g_mean_pos_neg_ratio,\n",
    "        g_horn_ratio,\n",
    "        g_inv_horn_ratio,\n",
    "        g_var_horn_mean,\n",
    "        g_var_invhorn_mean,\n",
    "        g_var_horn_std,\n",
    "        g_var_invhorn_std,\n",
    "        g_var_pos_neg_ratio_std,\n",
    "        g_var_count_std,\n",
    "        g_var_clause_ratio,\n",
    "        g_free_ratio,\n",
    "    ]\n",
    "\n",
    "    clause_features = [\n",
    "        c_clause_sizes,\n",
    "        c_clause_sizes / num_vars,\n",
    "        (c_clause_sizes - g_min_clause_size) / g_size_spread if g_size_spread > 0 else 0,\n",
    "        c_clause_lbd,\n",
    "        (c_clause_lbd - g_min_clause_lbd) / g_lbd_spread if g_lbd_spread > 0 else 0,\n",
    "        c_lbd_size_ratio,\n",
    "        c_pos_neg_ratios,\n",
    "        c_horn,\n",
    "        c_inv_horn,\n",
    "        c_clause_redundant,\n",
    "        c_clause_keep,\n",
    "        c_time,\n",
    "        c_time_top10,\n",
    "        c_activity,\n",
    "        c_activity_top10,\n",
    "        c_times_reason,\n",
    "        c_times_reason_top10,\n",
    "    ]\n",
    "\n",
    "    var_features = [\n",
    "        v_pos,\n",
    "        v_neg,\n",
    "        v_pos_neg_ratios,\n",
    "        v_horn,\n",
    "        v_invhorn,\n",
    "        v_horn_ratio,\n",
    "        v_invhorn_ratio,\n",
    "    ]\n",
    "\n",
    "    embedding_size = max(len(var_features), len(clause_features))\n",
    "\n",
    "    global_data = torch.tensor(global_features, dtype=torch.float32)\n",
    "    x = torch.zeros(num_vars + num_clauses, embedding_size)\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    node_type = torch.zeros(num_vars + num_clauses, dtype=torch.int8)\n",
    "\n",
    "    node_type[:num_vars] = NodeType.VARIABLE.value\n",
    "    node_type[num_vars:] = NodeType.IRREDUNDANT.value\n",
    "\n",
    "    reducible_ids = set(problem.reducible_ids)\n",
    "\n",
    "    for i, clause in enumerate(clauses):\n",
    "        if clause.id_ in reducible_ids:\n",
    "            node_type[num_vars + i] = NodeType.REDUNDANT.value\n",
    "\n",
    "    for i, f in enumerate(var_features):\n",
    "        x[:num_vars, i] = torch.tensor(f, dtype=torch.float32)\n",
    "\n",
    "    for i, f in enumerate(clause_features):\n",
    "        x[num_vars:, i] = torch.tensor(f, dtype=torch.float32)\n",
    "\n",
    "    for i, clause in enumerate(clauses):\n",
    "        for lit in clause.lits:\n",
    "            v = abs(lit) - 1\n",
    "            edge_index.append((v, num_vars + i))\n",
    "            edge_attr.append(-1 if lit < 0 else 1)\n",
    "\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    logger.info(\"Finished converting problem to CNF graph\")\n",
    "\n",
    "    return cast(CnfGraph, Data(\n",
    "        global_data=global_data,\n",
    "        x=x,\n",
    "        edge_index=torch.tensor(edge_index, dtype=torch.int64).t(),\n",
    "        edge_attr=edge_attr,\n",
    "        node_type=node_type,\n",
    "    )), DimInfo(\n",
    "        num_clause_features=len(clause_features),\n",
    "        num_edge_features=edge_attr.size(dim=1),\n",
    "        num_global_features=len(global_features),\n",
    "        num_var_features=len(var_features),\n",
    "    )\n",
    "\n",
    "\n",
    "def ordered_reducible_ids(problem: ReductionProblem) -> list[int]:\n",
    "    reducible_ids = set(problem.reducible_ids)\n",
    "    return [clause.id_ for clause in problem.clauses if clause.id_ in reducible_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:52:58.923191Z",
     "start_time": "2024-12-07T15:52:58.920153Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:notebook:Converting problem to CNF graph\n",
      "INFO:notebook:Finished converting problem to CNF graph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DimInfo(num_global_features=13, num_var_features=7, num_clause_features=17, num_edge_features=1)\n"
     ]
    }
   ],
   "source": [
    "minimal_example_data, DIM_INFO = problem_to_cnf_graph(ReductionProblem(\n",
    "    2,\n",
    "    [1, 1],\n",
    "    [0, 0],\n",
    "    [\n",
    "        Clause(0, [1, 2], 0, False, True, False),\n",
    "        Clause(5, [-1, -2], 0, False, False, False),\n",
    "        Clause(8, [-1, 2], 0, False, False, False),\n",
    "    ],\n",
    "    [5, 8],\n",
    "    3,\n",
    "))\n",
    "\n",
    "print(DIM_INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:01.952139Z",
     "start_time": "2024-12-07T15:52:59.084258Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:notebook:Converting problem to CNF graph\n",
      "INFO:notebook:Finished converting problem to CNF graph\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"../archives/example-problem.pkl\", \"rb\") as f:\n",
    "    problem = pickle.load(f)\n",
    "\n",
    "example_data, _ = problem_to_cnf_graph(problem)\n",
    "\n",
    "nx_graph = torch_geometric.utils.to_networkx(example_data, to_undirected=True)\n",
    "\n",
    "g = pyvis.network.Network(width=1800, height=1000, cdn_resources='in_line')\n",
    "g.toggle_hide_edges_on_drag = True\n",
    "g.barnes_hut()\n",
    "g.from_nx(nx_graph)\n",
    "g.save_graph(\"example.html\")\n",
    "\n",
    "eccentricities = list(nx.eccentricity(nx_graph).values())\n",
    "plt.bar(*np.unique(eccentricities, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:01.965857Z",
     "start_time": "2024-12-07T15:53:01.962998Z"
    }
   },
   "outputs": [],
   "source": [
    "class CnfProcessingBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, edge_dim: int, disambiguate_clauses: bool):\n",
    "        super().__init__()\n",
    "\n",
    "        self.disambiguate_clauses = disambiguate_clauses\n",
    "\n",
    "        if disambiguate_clauses:\n",
    "            self.conv_redundant = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "            self.conv_irredundant = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "        else:\n",
    "            self.conv_clauses = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "\n",
    "        self.conv_variables = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, h, data):\n",
    "        edge_index, edge_attr, node_type = data.edge_index, data.edge_attr, data.node_type\n",
    "\n",
    "        out = torch.zeros_like(h)\n",
    "\n",
    "        variable_mask = (node_type == NodeType.VARIABLE.value)\n",
    "        out[variable_mask, :] = self.conv_variables(h, edge_index, edge_attr)[variable_mask, :]\n",
    "\n",
    "        if self.disambiguate_clauses:\n",
    "            redundant_mask = (node_type == NodeType.REDUNDANT.value)\n",
    "            irredundant_mask = (node_type == NodeType.IRREDUNDANT.value)\n",
    "\n",
    "            out[redundant_mask, :] = self.conv_redundant(h, edge_index, edge_attr)[redundant_mask, :]\n",
    "            out[irredundant_mask, :] = self.conv_irredundant(h, edge_index, edge_attr)[irredundant_mask, :]\n",
    "        else:\n",
    "            clause_mask = (node_type != NodeType.VARIABLE.value)\n",
    "\n",
    "            out[clause_mask] = self.conv_clauses(h, edge_index, edge_attr)[clause_mask, :]\n",
    "\n",
    "\n",
    "        out = self.activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:02.067169Z",
     "start_time": "2024-12-07T15:53:02.063995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = torch.tensor([\\n    [1., 0.5, 0.5, 0.5, 0.1],\\n    [0.3, 0.5, 0.5, 0.8, 0.2],\\n], requires_grad=True)\\noptim = torch.optim.SGD([x], lr=0.1)\\nfor _ in range(100):\\n    optim.zero_grad()\\n    y = ExpectedValueNormalization.apply(x, torch.tensor([2.0, 1.0]))\\n    loss = y.pow(3.0).sum()\\n    loss.backward()\\n    optim.step()\\n    print(x.sum(axis=1))\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExpectedValueNormalization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, p: torch.Tensor, ex: torch.Tensor):\n",
    "        ex = torch.as_tensor(ex)\n",
    "\n",
    "        gamma = torch.ones(p.shape[:-1], device=p.device)\n",
    "\n",
    "        for _ in range(10):\n",
    "            normalized = p ** gamma.unsqueeze(-1)\n",
    "            f_gamma = normalized.sum(dim=-1) - ex\n",
    "            f_prime_gamma = (normalized * p.log()).sum(dim=-1)\n",
    "            new_gamma = gamma - f_gamma / f_prime_gamma\n",
    "            gamma = torch.maximum(new_gamma, gamma / 10)\n",
    "\n",
    "        normalized = p ** gamma.unsqueeze(-1)\n",
    "        ctx.save_for_backward(p, normalized, gamma)\n",
    "        return normalized\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        p, normalized, gamma = ctx.saved_tensors\n",
    "\n",
    "        normalized_dp = normalized * p.log()\n",
    "        denom = normalized_dp.sum(dim=-1)\n",
    "\n",
    "        jac = (\n",
    "            torch.diag_embed(gamma.unsqueeze(-1) * normalized / p) -\n",
    "            (gamma / denom).unsqueeze(-1).unsqueeze(-1) * normalized_dp.unsqueeze(-2) * (normalized / p).unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        return (jac @ grad_output.unsqueeze(-1)).squeeze(-1), None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = torch.tensor([\n",
    "    [1., 0.5, 0.5, 0.5, 0.1],\n",
    "    [0.3, 0.5, 0.5, 0.8, 0.2],\n",
    "], requires_grad=True)\n",
    "optim = torch.optim.SGD([x], lr=0.1)\n",
    "for _ in range(100):\n",
    "    optim.zero_grad()\n",
    "    y = ExpectedValueNormalization.apply(x, torch.tensor([2.0, 1.0]))\n",
    "    loss = y.pow(3.0).sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(x.sum(axis=1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedValueNormalizationLogits(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits: torch.Tensor, ex: torch.Tensor):\n",
    "        ex = torch.as_tensor(ex)\n",
    "\n",
    "        b = torch.zeros(logits.shape[:-1], device=logits.device)\n",
    "\n",
    "        for _ in range(100):\n",
    "            normalized = torch.sigmoid(logits + b.unsqueeze(-1))\n",
    "            f_gamma = normalized.sum(dim=-1) - ex\n",
    "            f_prime_gamma = (normalized * (1 - normalized)).sum(dim=-1)\n",
    "            diff = torch.clamp(f_gamma / f_prime_gamma, -2, 2)\n",
    "            if torch.all(diff.abs() < 1e-6):\n",
    "                break\n",
    "            b = b - diff\n",
    "\n",
    "        normalized = torch.sigmoid(logits + b.unsqueeze(-1))\n",
    "        ctx.save_for_backward(normalized)\n",
    "        return normalized\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        normalized, = ctx.saved_tensors\n",
    "        p_grad = normalized * (1 - normalized)\n",
    "        denom = p_grad.sum(dim=-1)\n",
    "        coordwise = p_grad * g\n",
    "\n",
    "        grad = coordwise - p_grad * coordwise.sum(axis=-1).unsqueeze(-1) / denom.unsqueeze(-1)\n",
    "\n",
    "        return grad, None\n",
    "\n",
    "\n",
    "probs = torch.tensor([\n",
    "    [0.999, 0.5, 0.5, 0.5, 0.1],\n",
    "    [0.3, 0.5, 0.5, 0.8, 0.2],\n",
    "], requires_grad=True)\n",
    "x = -(1 / probs - 1).log()\n",
    "y = ExpectedValueNormalizationLogits.apply(x, torch.tensor([2.0, 1.0]))\n",
    "# print(x, y, y.sum(axis=-1), sep=\"\\n\")\n",
    "y.sum().backward()\n",
    "# print(probs.grad)\n",
    "\n",
    "optim = torch.optim.SGD([probs], lr=0.1)\n",
    "for _ in range(100):\n",
    "    optim.zero_grad()\n",
    "    x = -(1 / probs - 1).log()\n",
    "    y = ExpectedValueNormalizationLogits.apply(x, torch.tensor([2.0, 1.0]))\n",
    "    loss = y.pow(3.0).sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    # print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:02.267511Z",
     "start_time": "2024-12-07T15:53:02.263690Z"
    }
   },
   "outputs": [],
   "source": [
    "class CnfGraphModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_global_dim: int,\n",
    "        input_variable_dim: int,\n",
    "        input_clause_dim: int,\n",
    "        hidden_dims: list[int],\n",
    "        disambiguate_clauses_in_first: int = 2,\n",
    "        edge_dim: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_variable_dim = input_variable_dim\n",
    "        self.input_clause_dim = input_clause_dim\n",
    "\n",
    "        self.variable_fc = nn.Linear(input_global_dim + input_variable_dim, hidden_dims[0])\n",
    "        self.clause_fc = nn.Linear(input_global_dim + input_clause_dim, hidden_dims[0])\n",
    "\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "        self.processing_blocks = nn.ModuleList([\n",
    "            CnfProcessingBlock(\n",
    "                hidden_dims[i],\n",
    "                hidden_dims[i + 1],\n",
    "                edge_dim,\n",
    "                i < disambiguate_clauses_in_first\n",
    "            )\n",
    "            for i in range(len(hidden_dims) - 1)\n",
    "        ])\n",
    "\n",
    "        self.output_fc = nn.Linear(hidden_dims[-1], 1, bias=False)\n",
    "\n",
    "        self.value_fc_clauses = nn.Linear(hidden_dims[-1], 1, bias=False)\n",
    "        self.value_fc_variables = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, cnf_graph: CnfGraph, ex: torch.Tensor | float):\n",
    "        logger.info(\"Starting model forward pass\")\n",
    "\n",
    "        num_vertices = cnf_graph.x.shape[0]\n",
    "        variable_mask = (cnf_graph.node_type == NodeType.VARIABLE.value)\n",
    "        num_variables = variable_mask.sum()\n",
    "\n",
    "        variable_input = torch.cat([\n",
    "            cnf_graph.global_data.expand(num_variables, -1),\n",
    "            cnf_graph.x[:num_variables, :self.input_variable_dim],\n",
    "        ], dim=-1)\n",
    "\n",
    "        variable_embedding = self.silu(self.variable_fc(variable_input))\n",
    "\n",
    "        clause_input = torch.cat([\n",
    "            cnf_graph.global_data.expand(num_vertices - num_variables, -1),\n",
    "            cnf_graph.x[num_variables:, :self.input_clause_dim],\n",
    "        ], dim=-1)\n",
    "\n",
    "        clause_embedding = self.silu(self.clause_fc(clause_input))\n",
    "\n",
    "        x = torch.cat([variable_embedding, clause_embedding], dim=0)\n",
    "\n",
    "        for block in self.processing_blocks:\n",
    "            x = block(x, cnf_graph)\n",
    "\n",
    "        logits = self.output_fc(x[cnf_graph.node_type == NodeType.REDUNDANT.value]).view(-1)\n",
    "        # logits = (logits - logits.mean()) / logits.std()\n",
    "        # assert not torch.isnan(logits).any()\n",
    "        # denorm_probs = torch.sigmoid(logits)\n",
    "        # assert not torch.isnan(denorm_probs).any()\n",
    "        # probs = ExpectedValueNormalization.apply(denorm_probs, ex)\n",
    "        # if torch.isnan(probs).any():\n",
    "        #     print(denorm_probs)\n",
    "        #     print(probs)\n",
    "        #     raise AssertionError()\n",
    "        probs = ExpectedValueNormalizationLogits.apply(logits * HP.temperature, ex)\n",
    "        if torch.isnan(probs).any():\n",
    "            print(logits)\n",
    "            print(probs)\n",
    "            raise AssertionError()\n",
    "\n",
    "        value_vars = self.value_fc_variables(x[variable_mask])\n",
    "        value_clauses = self.value_fc_clauses(x[~variable_mask])\n",
    "        value = torch.cat([value_vars, value_clauses]).mean()\n",
    "\n",
    "        logger.info(\"Finished model forward pass\")\n",
    "\n",
    "        return probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:02.418064Z",
     "start_time": "2024-12-07T15:53:02.409306Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.3699, 0.3801], grad_fn=<ExpectedValueNormalizationLogitsBackward>),\n",
       " tensor(0.0250, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_model = CnfGraphModel(\n",
    "    input_global_dim=DIM_INFO.num_global_features,\n",
    "    input_variable_dim=DIM_INFO.num_var_features,\n",
    "    input_clause_dim=DIM_INFO.num_clause_features,\n",
    "    edge_dim=DIM_INFO.num_edge_features,\n",
    "    hidden_dims=[32],\n",
    ")\n",
    "\n",
    "testing_model(minimal_example_data, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:02.586603Z",
     "start_time": "2024-12-07T15:53:02.583733Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Hyperparameters:\n",
    "    batch_size: int = 64\n",
    "    runs_per_episode: int = 64\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 2e-5\n",
    "    eps_clip: float = 0.2\n",
    "    entropy_coef: float = 0.01\n",
    "    weight_decay: float = 1e-3\n",
    "    value_weight: float = 0.5\n",
    "    policy_weight: float = 1.0\n",
    "    gae_gamma: float = 0.95\n",
    "    gae_lambda: float = 0.8\n",
    "    penalty_per_conflict: float = 5e-5\n",
    "    temperature: float = 4.0\n",
    "\n",
    "\n",
    "HP = Hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:02.744156Z",
     "start_time": "2024-12-07T15:53:02.742414Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_returns_advantages(rewards: list[float], values: list[float]) -> tuple[list[float], list[float]]:\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + HP.gae_gamma * (values[i + 1] if i + 1 < len(values) else 0) - values[i]\n",
    "        gae = delta + HP.gae_gamma * HP.gae_lambda * gae\n",
    "        returns.append(gae + values[i])\n",
    "        advantages.append(gae)\n",
    "    return returns[::-1], advantages[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:02.789252Z",
     "start_time": "2024-12-07T15:53:02.785330Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class EpisodeResult:\n",
    "    states: list[CnfGraph]\n",
    "    dists: list[torch.distributions.Bernoulli]\n",
    "    actions: list[torch.Tensor]\n",
    "    rewards: list[float]\n",
    "    values: list[float]\n",
    "    returns: list[float]\n",
    "    advantages: list[float]\n",
    "\n",
    "    stats: list[dict]\n",
    "\n",
    "    @staticmethod\n",
    "    def empty() -> 'EpisodeResult':\n",
    "        return EpisodeResult([], [], [], [], [], [], [], [])\n",
    "\n",
    "    def merge_with(self, other: 'EpisodeResult') -> 'EpisodeResult':\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards) == len(self.returns) ==\n",
    "            len(self.advantages)\n",
    "        )\n",
    "        assert (\n",
    "            len(other.states) == len(other.dists) == len(other.actions) ==\n",
    "            len(other.values) == len(other.rewards) == len(other.returns) ==\n",
    "            len(other.advantages)\n",
    "        )\n",
    "        return EpisodeResult(\n",
    "            self.states + other.states,\n",
    "            self.dists + other.dists,\n",
    "            self.actions + other.actions,\n",
    "            self.rewards + other.rewards,\n",
    "            self.values + other.values,\n",
    "            self.returns + other.returns,\n",
    "            self.advantages + other.advantages,\n",
    "            self.stats + other.stats,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_all(results: list['EpisodeResult']) -> 'EpisodeResult':\n",
    "        result = results[0]\n",
    "        for other in results[1:]:\n",
    "            result = result.merge_with(other)\n",
    "        return result\n",
    "\n",
    "    def add(self, *, state, dist, action, reward, value):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards)\n",
    "        )\n",
    "        self.states.append(state)\n",
    "        self.dists.append(dist)\n",
    "        self.actions.append(action)\n",
    "        if reward is not None:\n",
    "            self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def add_reward(self, reward):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards) + 1\n",
    "        )\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def complete(self, stats: dict):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards)\n",
    "        )\n",
    "        assert len(self.stats) == 0\n",
    "        self.returns, self.advantages = compute_returns_advantages(self.rewards, self.values)\n",
    "        self.stats = [stats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:02.894079Z",
     "start_time": "2024-12-07T15:53:02.890871Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.to(DEV)\n",
    "        self.optim = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=HP.learning_rate,\n",
    "            weight_decay=HP.weight_decay\n",
    "        )\n",
    "\n",
    "    def act(self, cnf_graph: CnfGraph, ex: float):\n",
    "        COUNTERS.steps += 1\n",
    "        logger.info(\"Running agent act\")\n",
    "        with torch.no_grad():\n",
    "            cnf_graph = cnf_graph.to(DEV)\n",
    "            assert torch.isnan(cnf_graph.x).sum() == 0\n",
    "            assert torch.isnan(cnf_graph.edge_attr).sum() == 0\n",
    "            assert torch.isnan(cnf_graph.global_data).sum() == 0\n",
    "            probs, value = self.model(cnf_graph, ex)\n",
    "            logger.info(\"Finished agent act\")\n",
    "            return torch.distributions.Bernoulli(probs), value.item()\n",
    "\n",
    "    def update(self, results: EpisodeResult, silent: bool = False):\n",
    "        logger.info(\"Training agent\")\n",
    "        r = results\n",
    "\n",
    "        n = len(r.states)\n",
    "        batch_size = HP.batch_size\n",
    "        advantages = torch.tensor(r.advantages, dtype=torch.float32)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for num_epoch in range(HP.epochs):\n",
    "            COUNTERS.epochs += 1\n",
    "            logger.info(\"Training agent, epoch %d\", num_epoch)\n",
    "            indices = random.sample(range(n), n)\n",
    "            for batch_start in range(0, n, batch_size):\n",
    "                COUNTERS.batches += 1\n",
    "                idx = indices[batch_start:batch_start + batch_size]\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "                for i in idx:\n",
    "                    COUNTERS.train_steps += 1\n",
    "                    graph, ex = r.states[i]\n",
    "                    new_probs, value = self.model(graph, ex)\n",
    "                    value = value.cpu()\n",
    "                    dist = torch.distributions.Bernoulli(probs=new_probs)\n",
    "                    log_probs = dist.log_prob(r.actions[i].float())\n",
    "                    entropy = dist.entropy().mean()\n",
    "\n",
    "                    ratio = torch.exp(log_probs - r.dists[i].log_prob(r.actions[i].float()))\n",
    "                    surr1 = ratio * advantages[i]\n",
    "                    surr2 = torch.clamp(ratio, 1.0 - HP.eps_clip, 1.0 + HP.eps_clip) * advantages[i]\n",
    "\n",
    "                    value_loss = torch.nn.functional.huber_loss(value, torch.tensor(r.returns[i]))\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                    if not silent:\n",
    "                        writer.add_scalar(\"value loss\", value_loss.item(), COUNTERS.train_steps)\n",
    "\n",
    "                    loss = (\n",
    "                        HP.policy_weight * policy_loss\n",
    "                        + HP.value_weight * value_loss\n",
    "                        - HP.entropy_coef * entropy\n",
    "                    ) / len(idx)\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:03.333825Z",
     "start_time": "2024-12-07T15:53:02.991607Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:notebook:Running agent act\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Finished agent act\n",
      "INFO:notebook:Running agent act\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Finished agent act\n",
      "INFO:notebook:Running agent act\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Finished agent act\n",
      "INFO:notebook:Training agent\n",
      "INFO:notebook:Training agent, epoch 0\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Training agent, epoch 1\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Training agent, epoch 2\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Training agent, epoch 3\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Training agent, epoch 4\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Training agent, epoch 5\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Training agent, epoch 6\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Training agent, epoch 7\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Training agent, epoch 8\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Training agent, epoch 9\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n",
      "INFO:notebook:Starting model forward pass\n",
      "INFO:notebook:Finished model forward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025018393993377686, 0.025018393993377686, 0.025018393993377686]\n",
      "[3.3504000000000005, 5.79, 9.0]\n"
     ]
    }
   ],
   "source": [
    "testing_agent = Agent(testing_model)\n",
    "\n",
    "states = [\n",
    "    (minimal_example_data, 0.75),\n",
    "    (minimal_example_data, 0.5),\n",
    "    (minimal_example_data, 0.25),\n",
    "]\n",
    "\n",
    "dist1, value1 = testing_agent.act(*states[0])\n",
    "dist2, value2 = testing_agent.act(*states[1])\n",
    "dist3, value3 = testing_agent.act(*states[2])\n",
    "\n",
    "dists = [dist1, dist2, dist3]\n",
    "values = [value1, value2, value3]\n",
    "print(values)\n",
    "\n",
    "actions = [dist.sample() > 0.5 for dist in dists]\n",
    "rewards = [-1.0, -1.0, 10.0]\n",
    "\n",
    "returns, advantages = compute_returns_advantages(rewards, [1.0, 1.0, 1.0])\n",
    "print(advantages)\n",
    "\n",
    "results = EpisodeResult(\n",
    "    states,\n",
    "    dists,\n",
    "    actions,\n",
    "    rewards,\n",
    "    values,\n",
    "    returns,\n",
    "    advantages,\n",
    "    [],\n",
    ")\n",
    "\n",
    "testing_agent.update(results, silent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:03.536386Z",
     "start_time": "2024-12-07T15:53:03.372424Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class CnfProblemInstance:\n",
    "    result: bool\n",
    "    path: Path\n",
    "    stats: dict[str, any]\n",
    "    stats_no_reductions: dict[str, any]\n",
    "\n",
    "\n",
    "with open(\"../archives/runs.json\", \"r\") as f:\n",
    "    runs = [json.loads(line) for line in f.readlines()]\n",
    "    runs = [CnfProblemInstance(\n",
    "        result=run[\"result\"],\n",
    "        path=Path(run[\"path\"]),\n",
    "        stats=run[\"stats\"],\n",
    "        stats_no_reductions=run[\"stats_no_reductions\"],\n",
    "    ) for run in runs]\n",
    "\n",
    "\n",
    "def get_random_instance(unsat_only: bool = True) -> CnfProblemInstance:\n",
    "    while True:\n",
    "        run = random.choice(runs)\n",
    "        if not unsat_only or not run.result:\n",
    "            return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:03.548501Z",
     "start_time": "2024-12-07T15:53:03.544466Z"
    }
   },
   "outputs": [],
   "source": [
    "class CadicalEnv:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cadical_path: Path,\n",
    "        runs_per_episode: int = 16,\n",
    "    ):\n",
    "        self.cadical_path = cadical_path\n",
    "        self.runs_per_episode = runs_per_episode\n",
    "\n",
    "    def run_instance(self, agent: Agent, instance: CnfProblemInstance) -> EpisodeResult:\n",
    "        COUNTERS.runs += 1\n",
    "        result = EpisodeResult.empty()\n",
    "        router = srunner.Router()\n",
    "\n",
    "        reduce_number = 0\n",
    "        last_conflicts = 0\n",
    "\n",
    "        @router.route(\"reduce\")\n",
    "        def _reduce_route(conn: srunner.Connection, info: srunner.RunInfo, _data):\n",
    "            nonlocal reduce_number\n",
    "            nonlocal last_conflicts\n",
    "\n",
    "            logger.info(\"Received clause database reduction\")\n",
    "\n",
    "            num_vars = conn.read_u64()\n",
    "            levels = [-1] * num_vars\n",
    "            vals = [-1] * num_vars\n",
    "            for i in range(num_vars):\n",
    "                vals[i] = conn.read_i8()\n",
    "                levels[i] = conn.read_i32()\n",
    "            num_clauses = conn.read_u64()\n",
    "            clauses = [conn.read_clause() for _ in range(num_clauses)]\n",
    "            num_reducible = conn.read_u64()\n",
    "            num_target = conn.read_u64()\n",
    "            reducible_ids = [conn.read_u64() for _ in range(num_reducible)]\n",
    "            conflicts = conn.read_u64()\n",
    "            conn.write_ok()\n",
    "\n",
    "            if reduce_number != 0:\n",
    "                result.add_reward(-(conflicts - last_conflicts) * HP.penalty_per_conflict)\n",
    "            last_conflicts = conflicts\n",
    "\n",
    "            problem = ReductionProblem(\n",
    "                num_vars=num_vars,\n",
    "                levels=levels,\n",
    "                vals=vals,\n",
    "                clauses=clauses,\n",
    "                reducible_ids=reducible_ids,\n",
    "                conflicts=conflicts,\n",
    "            )\n",
    "\n",
    "            logger.debug(\"Finished reading reduction data\")\n",
    "\n",
    "            cnf, dim_info = problem_to_cnf_graph(problem)\n",
    "            assert dim_info == DIM_INFO\n",
    "\n",
    "            ex = float(num_target)\n",
    "\n",
    "            logger.debug(\"Converted problem to CNF graph\")\n",
    "\n",
    "            dist, value = agent.act(cnf, ex)\n",
    "            print(dist.probs)\n",
    "            action = dist.sample()\n",
    "\n",
    "            logger.debug(\"Agent acted\")\n",
    "\n",
    "            result.add(\n",
    "                state=(cnf, ex),\n",
    "                dist=dist,\n",
    "                action=action,\n",
    "                reward=None,\n",
    "                value=value,\n",
    "            )\n",
    "\n",
    "            logger.debug(\"Results are written to replay buffer\")\n",
    "\n",
    "            reducible_ids = ordered_reducible_ids(problem)\n",
    "\n",
    "            to_reduces = []\n",
    "            for id_, a in zip(reducible_ids, action):\n",
    "                if a > 0.5:\n",
    "                    to_reduces.append(id_)\n",
    "\n",
    "            logger.debug(\"Sending reduction data\")\n",
    "\n",
    "            conn.write_u32(len(to_reduces))\n",
    "            for id_ in to_reduces:\n",
    "                conn.write_u64(id_)\n",
    "\n",
    "            reduce_number += 1\n",
    "\n",
    "        @router.route(\"stats\")\n",
    "        def _stats_route(conn: srunner.Connection, info: srunner.RunInfo, data):\n",
    "            nonlocal last_conflicts\n",
    "\n",
    "            logger.info(\"Received stats\")\n",
    "            stats = {}\n",
    "            while (name := conn.read_str()) != \"end\":\n",
    "                if name == \"time\":\n",
    "                    stats[name] = conn.read_f64()\n",
    "                else:\n",
    "                    stats[name] = conn.read_u64()\n",
    "            conn.write_ok()\n",
    "\n",
    "            for name, value in stats.items():\n",
    "                writer.add_scalar(f\"Runs/{name}\", value, COUNTERS.runs)\n",
    "                if instance.stats[name]:\n",
    "                    writer.add_scalar(f\"Runs relative/{name}\", value / instance.stats[name], COUNTERS.runs)\n",
    "\n",
    "            writer.add_scalar(f\"run_conflicts\", stats[\"conflicts\"], COUNTERS.runs)\n",
    "            writer.add_scalar(f\"relative_lbd_conflicts\", stats[\"conflicts\"] / instance.stats[\"conflicts\"], COUNTERS.runs)\n",
    "            writer.add_scalar(f\"relative_random_conflicts\", stats[\"conflicts\"] / instance.stats_no_reductions[\"conflicts\"], COUNTERS.runs)\n",
    "\n",
    "            result.add_reward(-(stats[\"conflicts\"] - last_conflicts) * HP.penalty_per_conflict)\n",
    "            result.complete(stats)\n",
    "\n",
    "        srunner.run_instance(\n",
    "            self.cadical_path,\n",
    "            [\"--reduce-mode\", \"2\"],\n",
    "            instance.path,\n",
    "            router.routes,\n",
    "            silent=True,\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def run_episode(self, agent: Agent) -> EpisodeResult:\n",
    "        results = []\n",
    "        for _ in range(self.runs_per_episode):\n",
    "            instance = get_random_instance()\n",
    "            results.append(self.run_instance(agent, instance))\n",
    "\n",
    "        return EpisodeResult.merge_all(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:53:03.611486Z",
     "start_time": "2024-12-07T15:53:03.600833Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "model = CnfGraphModel(\n",
    "    input_global_dim=DIM_INFO.num_global_features,\n",
    "    input_variable_dim=DIM_INFO.num_var_features,\n",
    "    input_clause_dim=DIM_INFO.num_clause_features,\n",
    "    edge_dim=DIM_INFO.num_edge_features,\n",
    "    hidden_dims=[32],\n",
    ")\n",
    "\n",
    "agent = Agent(model)\n",
    "\n",
    "env = CadicalEnv(\n",
    "    Path(\"../cadical/build/cadical\"),\n",
    "    runs_per_episode=HP.runs_per_episode,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31435/3183735522.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(saved_checkpoints[-1])\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_ROOT = Path(\"../checkpoints-shallow\")\n",
    "CHECKPOINT_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "saved_checkpoints = list(CHECKPOINT_ROOT.glob(\"*.pt\"))\n",
    "saved_checkpoints.sort(key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "if len(saved_checkpoints) > 0:\n",
    "    checkpoint = torch.load(saved_checkpoints[-1])\n",
    "    agent.model.load_state_dict(checkpoint[\"model\"])\n",
    "    agent.optim.load_state_dict(checkpoint[\"optim\"])\n",
    "    COUNTERS.from_dict(checkpoint[\"counters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory._record_memory_history(max_entries=10 ** 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T15:56:10.709821Z",
     "start_time": "2024-12-07T15:53:03.663537Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:srunner:Running ../cadical/build/cadical on /home/elt/projects/sat-rl-exploration/instances/unif-fb82a721-4941-41d4-a20c-bba9ad0a8aaa.cnf\n",
      "INFO:srunner:Running /home/elt/projects/sat-rl-exploration/py/../cadical/build/cadical --reduce-mode 2 --socket /tmp/89f35a7e-ecc6-45e1-9d0d-e7391a2c6bb9.sock -t 1000000000 /home/elt/projects/sat-rl-exploration/instances/unif-fb82a721-4941-41d4-a20c-bba9ad0a8aaa.cnf\n",
      "DEBUG:srunner:Starting connection to /tmp/89f35a7e-ecc6-45e1-9d0d-e7391a2c6bb9.sock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "tensor([0.7271, 0.7613, 0.7455, 0.6876, 0.7347, 0.7190, 0.7505, 0.7464, 0.7025,\n",
      "        0.7358, 0.7057, 0.7611, 0.7585, 0.7263, 0.7435, 0.7461, 0.7600, 0.7775,\n",
      "        0.7669, 0.7166, 0.6758, 0.7537, 0.7547, 0.7063, 0.7557, 0.7515, 0.7366,\n",
      "        0.7086, 0.7092, 0.7175, 0.7346, 0.7311, 0.7443, 0.7431, 0.7523, 0.7628,\n",
      "        0.7546, 0.8003, 0.7807, 0.6830, 0.7713, 0.7488, 0.7932, 0.6957, 0.7122,\n",
      "        0.7015, 0.7503, 0.7614, 0.7781, 0.7722, 0.7250, 0.7300, 0.7300, 0.8040,\n",
      "        0.7835, 0.7694, 0.7249, 0.7931, 0.7089, 0.7186, 0.6966, 0.7192, 0.7398,\n",
      "        0.7511, 0.7289, 0.7342, 0.7338, 0.7444, 0.7823, 0.7423, 0.7619, 0.7604,\n",
      "        0.7058, 0.7409, 0.7709, 0.7341, 0.7191, 0.7565, 0.7491, 0.7597, 0.7935,\n",
      "        0.7658, 0.7581, 0.7446, 0.7511, 0.7304, 0.7510, 0.7047, 0.8332, 0.7605,\n",
      "        0.7448, 0.7640, 0.7651, 0.7340, 0.7851, 0.7771, 0.7568, 0.7611, 0.7661,\n",
      "        0.7708, 0.7161, 0.7739, 0.7529, 0.7882, 0.7436, 0.7591, 0.7338, 0.7174,\n",
      "        0.7276, 0.7836, 0.7611, 0.7737, 0.7276, 0.7329, 0.7329, 0.7329, 0.7180,\n",
      "        0.7242, 0.7498, 0.7666, 0.7584, 0.7374, 0.7440, 0.7981, 0.7527, 0.7603,\n",
      "        0.7599, 0.7590, 0.7940, 0.7263, 0.7881, 0.7683, 0.7848, 0.7506, 0.7658,\n",
      "        0.7583, 0.7792, 0.7589, 0.7891, 0.7751, 0.7621, 0.7598, 0.7491, 0.7836,\n",
      "        0.7974, 0.7899, 0.7671, 0.7827, 0.7688, 0.7819, 0.7677, 0.7161],\n",
      "       device='cuda:0')\n",
      "tensor([0.8018, 0.7220, 0.7360, 0.7784, 0.7595, 0.7176, 0.7075, 0.6926, 0.8033,\n",
      "        0.7149, 0.7835, 0.7998, 0.6975, 0.7407, 0.7568, 0.7645, 0.7061, 0.7793,\n",
      "        0.7836, 0.8016, 0.8030, 0.7419, 0.7844, 0.7616, 0.7698, 0.7211, 0.7514,\n",
      "        0.7250, 0.7446, 0.7393, 0.7402, 0.7455, 0.7521, 0.7182, 0.7246, 0.8019,\n",
      "        0.7523, 0.7569, 0.7605, 0.8248, 0.7950, 0.7492, 0.7633, 0.7756, 0.8289,\n",
      "        0.7380, 0.7992, 0.8177, 0.8118, 0.6739, 0.7326, 0.7383, 0.6985, 0.7965,\n",
      "        0.7250, 0.7522, 0.7440, 0.7385, 0.7350, 0.7572, 0.7813, 0.7565, 0.6952,\n",
      "        0.7269, 0.7500, 0.7739, 0.7590, 0.7599, 0.7815, 0.7519, 0.7548, 0.7441,\n",
      "        0.7174, 0.7555, 0.6989, 0.7159, 0.7488, 0.7614, 0.7209, 0.7569, 0.7633,\n",
      "        0.7648, 0.7971, 0.7925, 0.7268, 0.7397, 0.8480, 0.7532, 0.8022, 0.7409,\n",
      "        0.7630, 0.7440, 0.6569, 0.7020, 0.6824, 0.7509, 0.7763, 0.6559, 0.7906,\n",
      "        0.6744, 0.7869, 0.7626, 0.7483, 0.7269, 0.7635, 0.7635, 0.7344, 0.7459,\n",
      "        0.7459, 0.8076, 0.7628, 0.7746, 0.6759, 0.7905, 0.7498, 0.8003, 0.7674,\n",
      "        0.7393, 0.7771, 0.6666, 0.6631, 0.6119, 0.7945, 0.7579, 0.7355, 0.6715,\n",
      "        0.7774, 0.7152, 0.7724, 0.7415, 0.7570, 0.7435, 0.7735, 0.7299, 0.7146,\n",
      "        0.7533, 0.7479, 0.7731, 0.7240, 0.8065, 0.7813, 0.8028, 0.7259, 0.7695,\n",
      "        0.7800, 0.7368, 0.7401, 0.7787, 0.8132, 0.7685, 0.7213, 0.7676, 0.6812,\n",
      "        0.7792, 0.6878, 0.7897, 0.7896, 0.7889, 0.6953, 0.6953, 0.6783, 0.6882,\n",
      "        0.7952, 0.7584, 0.7596, 0.7826, 0.7706, 0.8063, 0.6951, 0.7853, 0.7867,\n",
      "        0.7760, 0.7625, 0.7241, 0.7130, 0.7624, 0.7724, 0.7817, 0.7229, 0.7511,\n",
      "        0.7121, 0.7146, 0.7845, 0.7517, 0.7837, 0.8018, 0.7405, 0.6560, 0.7865,\n",
      "        0.7563, 0.7698, 0.8002, 0.8143, 0.7760, 0.7396, 0.7102, 0.7206, 0.6553,\n",
      "        0.7453, 0.7468, 0.7031, 0.6918, 0.6927, 0.7817, 0.7582, 0.7332, 0.7452,\n",
      "        0.7696, 0.7806, 0.7743, 0.7230, 0.7471, 0.8015, 0.7701, 0.7183, 0.7696,\n",
      "        0.7545, 0.7671, 0.7629, 0.7697, 0.7550, 0.7527, 0.7511, 0.7323, 0.7260,\n",
      "        0.7359, 0.8069, 0.7067, 0.7411, 0.7872, 0.7684, 0.7253, 0.7839, 0.7261,\n",
      "        0.6676, 0.6511, 0.7372, 0.7385, 0.7411, 0.6805, 0.7840, 0.7544, 0.6808,\n",
      "        0.6408, 0.7453, 0.7758, 0.8196, 0.7483, 0.7926, 0.7003, 0.7448, 0.7425,\n",
      "        0.7168, 0.7937, 0.8087, 0.7832, 0.7881, 0.7947, 0.7530, 0.7284, 0.7018,\n",
      "        0.7032, 0.7553, 0.7602, 0.7620, 0.7354, 0.7583, 0.7205, 0.7312, 0.6685,\n",
      "        0.7657, 0.8068, 0.7142, 0.6751, 0.7783, 0.7967, 0.7912, 0.7730, 0.7447,\n",
      "        0.7830, 0.7507, 0.7452, 0.7192, 0.7772, 0.7171, 0.6650, 0.8106, 0.7375,\n",
      "        0.7843, 0.7557, 0.7377, 0.7387, 0.7224, 0.7340, 0.7375, 0.7536, 0.6915,\n",
      "        0.7245, 0.6805, 0.7408, 0.7698, 0.7262, 0.7753, 0.7142, 0.6018, 0.7781,\n",
      "        0.7356, 0.7381, 0.7822, 0.7252, 0.7459, 0.7683, 0.7585, 0.7661, 0.7482,\n",
      "        0.7984, 0.7845, 0.7254, 0.7441, 0.7262, 0.7959, 0.7614, 0.7883, 0.7433,\n",
      "        0.7551, 0.7443, 0.6853, 0.7196, 0.6874, 0.7633, 0.7635, 0.7693, 0.7165,\n",
      "        0.8322, 0.7710, 0.7610, 0.7452, 0.7344, 0.8022, 0.7695, 0.7795, 0.7955,\n",
      "        0.7746, 0.6553, 0.6545, 0.7079, 0.7261, 0.8131, 0.8022, 0.7835, 0.7582,\n",
      "        0.7474, 0.8037, 0.7405, 0.7484, 0.7289, 0.7505, 0.7244, 0.7501, 0.7964,\n",
      "        0.7441, 0.7324, 0.7625, 0.7523, 0.7715, 0.6739, 0.7735, 0.7426, 0.7446,\n",
      "        0.6408, 0.7754, 0.7347, 0.7457, 0.7405, 0.7664, 0.7912, 0.7515, 0.8241,\n",
      "        0.7537, 0.7836, 0.7531, 0.7523, 0.7634, 0.7574, 0.7347, 0.7594, 0.7829],\n",
      "       device='cuda:0')\n",
      "tensor([0.8399, 0.7800, 0.7807, 0.8406, 0.7333, 0.7885, 0.7154, 0.8189, 0.7809,\n",
      "        0.8288, 0.8398, 0.8148, 0.7550, 0.7920, 0.7728, 0.7713, 0.7822, 0.8004,\n",
      "        0.8052, 0.8377, 0.7529, 0.7871, 0.7894, 0.8462, 0.8049, 0.7036, 0.8379,\n",
      "        0.8232, 0.8094, 0.7724, 0.8431, 0.7234, 0.7566, 0.8402, 0.7788, 0.7718,\n",
      "        0.8146, 0.8121, 0.7864, 0.8166, 0.7537, 0.7727, 0.8035, 0.7439, 0.8275,\n",
      "        0.7908, 0.8196, 0.7547, 0.8100, 0.7870, 0.8138, 0.8072, 0.8215, 0.7726,\n",
      "        0.8355, 0.6767, 0.6759, 0.7995, 0.8107, 0.8473, 0.8163, 0.7738, 0.8250,\n",
      "        0.8279, 0.8117, 0.8248, 0.7745, 0.7734, 0.8409, 0.7977, 0.8175, 0.8184,\n",
      "        0.8482, 0.8233, 0.8326, 0.7838, 0.7531, 0.8301, 0.7863, 0.8084, 0.8247,\n",
      "        0.7464, 0.8410, 0.8057, 0.8397, 0.8144, 0.7970, 0.8125, 0.8414, 0.8413,\n",
      "        0.8246, 0.8228, 0.8264, 0.8376, 0.8196, 0.8394, 0.8132, 0.7155, 0.7036,\n",
      "        0.7996, 0.7794, 0.7790, 0.7233, 0.8107, 0.7404, 0.8367, 0.8283, 0.8304,\n",
      "        0.7268, 0.8290, 0.7992, 0.8148, 0.8128, 0.8273, 0.7724, 0.6760, 0.8110,\n",
      "        0.7921, 0.8329, 0.8470, 0.8373, 0.8239, 0.8294, 0.8175, 0.7818, 0.8078,\n",
      "        0.7835, 0.8263, 0.8116, 0.8005, 0.8023, 0.7326, 0.8051, 0.7663, 0.8317,\n",
      "        0.7870, 0.8456, 0.8401, 0.7975, 0.8023, 0.8022, 0.8012, 0.8267, 0.7987,\n",
      "        0.8116, 0.7822, 0.8434, 0.7169, 0.8165, 0.8016, 0.8154, 0.7178, 0.8018,\n",
      "        0.8416, 0.7918, 0.8158, 0.8194, 0.7576, 0.7372, 0.8246, 0.8060, 0.8163,\n",
      "        0.7651, 0.8150, 0.8150, 0.8093, 0.8174, 0.7965, 0.8427, 0.7736, 0.6906,\n",
      "        0.8166, 0.7969, 0.7815, 0.7772, 0.8262, 0.7997, 0.8388, 0.7524, 0.8396,\n",
      "        0.7554, 0.8297, 0.8209, 0.7741, 0.8013, 0.8089, 0.7524, 0.8479, 0.8377,\n",
      "        0.7999, 0.8013, 0.8134, 0.7724, 0.8027, 0.8295, 0.8063, 0.7894, 0.7431,\n",
      "        0.8161, 0.8002, 0.8380, 0.8264, 0.8316, 0.8247, 0.8063, 0.8008, 0.8043,\n",
      "        0.6854, 0.6849, 0.8140, 0.7985, 0.8395, 0.8005, 0.7984, 0.8285, 0.7650,\n",
      "        0.8046, 0.7724, 0.8079, 0.7688, 0.8042, 0.7972, 0.8151, 0.8384, 0.7745,\n",
      "        0.8135, 0.8061, 0.8364, 0.8099, 0.8127, 0.8027, 0.8066, 0.6679, 0.8266,\n",
      "        0.7617, 0.8407, 0.8023, 0.8366, 0.8380, 0.7999, 0.8365, 0.8162, 0.8392,\n",
      "        0.8142, 0.8177, 0.8355, 0.8340, 0.7875, 0.8359, 0.8143, 0.7997, 0.7215,\n",
      "        0.6584, 0.7985, 0.7935, 0.7290, 0.7736, 0.7750, 0.7384, 0.7723, 0.7958,\n",
      "        0.7729, 0.7805, 0.7466, 0.8144, 0.7803, 0.7320, 0.7870, 0.7295, 0.7215,\n",
      "        0.7846, 0.7813, 0.7554, 0.7470, 0.7653, 0.7883, 0.7357, 0.7509, 0.7547,\n",
      "        0.6564, 0.6647, 0.7759, 0.6870, 0.6950, 0.7023, 0.7902, 0.6823, 0.7172,\n",
      "        0.7222, 0.7755, 0.7296, 0.7475, 0.7751, 0.7753, 0.7937, 0.7356, 0.7553,\n",
      "        0.7032, 0.7435, 0.7475, 0.7771, 0.7614, 0.7453, 0.7838, 0.7687, 0.6484,\n",
      "        0.6634, 0.6491, 0.6643, 0.6327, 0.6879, 0.6850, 0.6845, 0.6974, 0.7166,\n",
      "        0.6224, 0.6701, 0.6560, 0.6550, 0.6412, 0.6228, 0.6351, 0.7423, 0.6986,\n",
      "        0.6202, 0.6348, 0.6499, 0.5961, 0.6579, 0.7077, 0.6806, 0.7268, 0.7432,\n",
      "        0.7344, 0.8305, 0.7574, 0.6906, 0.6968, 0.7066, 0.7068, 0.7410, 0.7515,\n",
      "        0.6698, 0.7725, 0.7900, 0.7192, 0.7099, 0.6980, 0.6566, 0.7108, 0.7204,\n",
      "        0.7408, 0.6692, 0.7392, 0.6969, 0.7424, 0.6961, 0.7217, 0.7717, 0.6325,\n",
      "        0.7507, 0.7423, 0.6832, 0.7136, 0.6487, 0.7113, 0.6624, 0.7387, 0.7825,\n",
      "        0.6682, 0.7007, 0.7547, 0.7882, 0.6835, 0.6910, 0.6300, 0.7232, 0.6673,\n",
      "        0.7804, 0.7467, 0.7900, 0.7720, 0.6479, 0.7051, 0.7048, 0.7011, 0.6821,\n",
      "        0.6953, 0.6177, 0.7439, 0.6760, 0.7387, 0.6910, 0.7143, 0.6614, 0.7297,\n",
      "        0.7258, 0.7623, 0.7490, 0.7745, 0.7508, 0.6292, 0.7900, 0.7275, 0.7720,\n",
      "        0.7432, 0.7661, 0.6880, 0.7066, 0.7540, 0.6820, 0.6630, 0.6962, 0.7163,\n",
      "        0.6345, 0.6749, 0.6420, 0.7520, 0.8234, 0.7854, 0.6593, 0.6505, 0.7332,\n",
      "        0.6588, 0.7640, 0.7367, 0.7072, 0.7908, 0.7809, 0.7807, 0.6467, 0.7003,\n",
      "        0.7676, 0.7503, 0.7489, 0.7168, 0.7076, 0.7463, 0.6494, 0.6727, 0.6504,\n",
      "        0.6954, 0.7492, 0.7774, 0.7217, 0.6898, 0.6343, 0.7526, 0.7571, 0.6959,\n",
      "        0.6874, 0.6349, 0.6786, 0.7059, 0.6620, 0.7598, 0.6807, 0.6733, 0.6570,\n",
      "        0.6846, 0.7850, 0.7391, 0.8079, 0.7016, 0.7086, 0.7598, 0.7834, 0.6962,\n",
      "        0.7564, 0.7149, 0.7548, 0.7562, 0.8011, 0.8384, 0.7421, 0.8428, 0.7194,\n",
      "        0.7443, 0.7379, 0.7978, 0.7393, 0.7838, 0.7332, 0.7435, 0.6796, 0.7332,\n",
      "        0.7722, 0.7256, 0.7906, 0.7465, 0.7836, 0.6565, 0.7197, 0.7381, 0.7663,\n",
      "        0.7232, 0.7589, 0.6841, 0.7221, 0.6485, 0.7026, 0.7868, 0.6843, 0.6649,\n",
      "        0.6833, 0.6899, 0.6973, 0.7908, 0.7083, 0.6702, 0.6701, 0.7477, 0.7521,\n",
      "        0.7725, 0.7491, 0.7867, 0.7125, 0.7374, 0.7379, 0.6996, 0.7708, 0.7214,\n",
      "        0.8242, 0.6690, 0.7723, 0.7434, 0.7028, 0.7025, 0.6831, 0.7280, 0.5871,\n",
      "        0.6036, 0.8437, 0.7526, 0.7402, 0.7030, 0.6844, 0.6712, 0.7553, 0.6900,\n",
      "        0.7325, 0.7335, 0.7663, 0.6901, 0.6564, 0.7910, 0.7048, 0.7339, 0.6954,\n",
      "        0.7334, 0.7704, 0.8234, 0.8187, 0.6578, 0.6703, 0.7826, 0.7729, 0.7251,\n",
      "        0.7729, 0.7343, 0.7466, 0.6844, 0.7880, 0.7388, 0.7211, 0.6791, 0.6227,\n",
      "        0.6514, 0.7024, 0.6310, 0.6557, 0.6687, 0.6951, 0.6492, 0.7885, 0.7959,\n",
      "        0.7197, 0.6637, 0.6623, 0.7563, 0.7068, 0.6503, 0.6850, 0.7358, 0.6077,\n",
      "        0.6747, 0.6839, 0.6344, 0.6908, 0.6628, 0.6988, 0.8147, 0.7486, 0.7409,\n",
      "        0.6787, 0.7498, 0.7735, 0.7703, 0.7721, 0.7883, 0.7443, 0.7925, 0.7864,\n",
      "        0.6792, 0.7430, 0.7580, 0.6409, 0.6292, 0.7924, 0.7399, 0.7922, 0.7870,\n",
      "        0.7684, 0.7455, 0.6756, 0.6694, 0.8242, 0.7914, 0.7762, 0.7436, 0.7192,\n",
      "        0.6637, 0.7527, 0.6556, 0.7474, 0.7861, 0.7888, 0.7856, 0.7206, 0.6034,\n",
      "        0.8108, 0.7741, 0.7811, 0.6377, 0.7162, 0.7760, 0.7791, 0.7692, 0.7779,\n",
      "        0.7405, 0.6715, 0.7909, 0.7174, 0.7778, 0.7168, 0.7731, 0.6709, 0.6837,\n",
      "        0.8039, 0.7557, 0.7572, 0.6335, 0.6342, 0.6707, 0.6918, 0.6796, 0.6635,\n",
      "        0.6764, 0.7342, 0.6750, 0.7568, 0.8076, 0.7678, 0.7383, 0.7436, 0.7222,\n",
      "        0.7763, 0.7701, 0.7301, 0.7008, 0.7704, 0.6213, 0.6987, 0.7906, 0.6128,\n",
      "        0.7543, 0.7880, 0.7746, 0.7967, 0.6629, 0.6928, 0.6922, 0.7656, 0.7850,\n",
      "        0.7466, 0.7071, 0.7771, 0.7903, 0.7440, 0.7879, 0.8162, 0.7571, 0.7932,\n",
      "        0.6437, 0.6359, 0.6221, 0.7074, 0.6904, 0.6821, 0.6826, 0.7225, 0.7548,\n",
      "        0.7369, 0.7503, 0.7410, 0.7840, 0.7330, 0.6531, 0.7253, 0.7507, 0.7511,\n",
      "        0.7094, 0.7119, 0.8247, 0.6614, 0.7076, 0.6781, 0.7438, 0.7098, 0.7271,\n",
      "        0.7559, 0.6550, 0.7939, 0.7362], device='cuda:0')\n",
      "tensor([0.7949, 0.7956, 0.7954,  ..., 0.7573, 0.7946, 0.7454], device='cuda:0')\n",
      "tensor([0.7885, 0.8043, 0.8035,  ..., 0.7615, 0.6652, 0.7066], device='cuda:0')\n",
      "tensor([0.8097, 0.8090, 0.8225,  ..., 0.6579, 0.7404, 0.7579], device='cuda:0')\n",
      "tensor([0.7957, 0.5761, 0.7948,  ..., 0.7132, 0.6499, 0.7075], device='cuda:0')\n",
      "tensor([0.7946, 0.5797, 0.6346,  ..., 0.6676, 0.6815, 0.7141], device='cuda:0')\n",
      "tensor([0.6394, 0.8131, 0.7164,  ..., 0.7650, 0.7639, 0.7638], device='cuda:0')\n",
      "tensor([0.6377, 0.6352, 0.7418,  ..., 0.7386, 0.7065, 0.7127], device='cuda:0')\n",
      "tensor([0.6265, 0.8131, 0.7814,  ..., 0.7347, 0.7383, 0.7588], device='cuda:0')\n",
      "tensor([0.7760, 0.8133, 0.7857,  ..., 0.6959, 0.7956, 0.7500], device='cuda:0')\n",
      "tensor([0.8182, 0.7954, 0.8088,  ..., 0.6914, 0.7483, 0.7547], device='cuda:0')\n",
      "tensor([0.8242, 0.7700, 0.7412,  ..., 0.7487, 0.7108, 0.6968], device='cuda:0')\n",
      "tensor([0.8123, 0.7740, 0.8162,  ..., 0.7567, 0.7225, 0.7382], device='cuda:0')\n",
      "tensor([0.7413, 0.7523, 0.7582,  ..., 0.7330, 0.7284, 0.7534], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:srunner:Running ../cadical/build/cadical on /home/elt/projects/sat-rl-exploration/instances/unif-76755b06-0ccd-444c-9c89-ff98940a2b64.cnf\n",
      "INFO:srunner:Running /home/elt/projects/sat-rl-exploration/py/../cadical/build/cadical --reduce-mode 2 --socket /tmp/3a9ffe83-0025-4b74-92e0-445b40d7f51e.sock -t 1000000000 /home/elt/projects/sat-rl-exploration/instances/unif-76755b06-0ccd-444c-9c89-ff98940a2b64.cnf\n",
      "DEBUG:srunner:Starting connection to /tmp/3a9ffe83-0025-4b74-92e0-445b40d7f51e.sock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7272, 0.7294, 0.7466, 0.7327, 0.6862, 0.6704, 0.7490, 0.7260, 0.6852,\n",
      "        0.6834, 0.6861, 0.6826, 0.7103, 0.7270, 0.7437, 0.6834, 0.7242, 0.7423,\n",
      "        0.7497, 0.7146, 0.7286, 0.7110, 0.7250, 0.7308, 0.7310, 0.7036, 0.7552,\n",
      "        0.7490, 0.7589, 0.7588, 0.7448, 0.7154, 0.7071, 0.7389, 0.7536, 0.7670,\n",
      "        0.7566, 0.7439, 0.7803, 0.7736, 0.7361, 0.8059, 0.7238, 0.7496, 0.7357,\n",
      "        0.7553, 0.7685, 0.7229, 0.7412, 0.8301, 0.7535, 0.7286, 0.7348, 0.7741,\n",
      "        0.7541, 0.7777, 0.7634, 0.7748, 0.7551, 0.7355, 0.7452, 0.7726, 0.7783,\n",
      "        0.7594, 0.7266, 0.7921, 0.7679, 0.8062, 0.7421, 0.7359, 0.7619, 0.7698,\n",
      "        0.7435, 0.8255, 0.7398, 0.7466, 0.7703, 0.7939, 0.7799, 0.7554, 0.7520,\n",
      "        0.7601, 0.7697, 0.7946, 0.7792, 0.7549, 0.7584, 0.7224, 0.7056, 0.7323,\n",
      "        0.7336, 0.7324, 0.7056, 0.7074, 0.7517, 0.7184, 0.7596, 0.7342, 0.7649,\n",
      "        0.7175, 0.7255, 0.7098, 0.7865, 0.7714, 0.7737, 0.7811, 0.7102, 0.7701,\n",
      "        0.7956, 0.7997, 0.7531, 0.7779, 0.7599, 0.7517, 0.7069, 0.7736, 0.7864,\n",
      "        0.7662, 0.8188, 0.7755, 0.7282, 0.7374, 0.7412, 0.7181, 0.7542, 0.7386,\n",
      "        0.7245, 0.7201, 0.7522, 0.7353, 0.7563, 0.7266, 0.7166, 0.7166, 0.6935,\n",
      "        0.7236, 0.7627, 0.7370, 0.7325, 0.7494, 0.7469, 0.7839, 0.7764, 0.7157,\n",
      "        0.7263, 0.7257, 0.7697, 0.7611, 0.8214, 0.7685, 0.7659, 0.7523, 0.7425,\n",
      "        0.7652], device='cuda:0')\n",
      "tensor([0.7705, 0.8009, 0.7508, 0.7462, 0.7217, 0.7359, 0.7633, 0.7435, 0.7073,\n",
      "        0.7268, 0.7652, 0.7119, 0.7549, 0.7244, 0.7332, 0.7644, 0.7074, 0.7307,\n",
      "        0.7609, 0.8031, 0.7736, 0.7543, 0.7247, 0.7122, 0.7501, 0.7593, 0.7157,\n",
      "        0.7470, 0.7324, 0.7501, 0.7154, 0.7523, 0.6745, 0.7412, 0.7613, 0.7289,\n",
      "        0.7644, 0.7651, 0.7259, 0.7620, 0.7554, 0.7615, 0.7363, 0.7639, 0.8038,\n",
      "        0.7869, 0.7613, 0.7207, 0.7201, 0.7694, 0.7233, 0.6951, 0.7032, 0.7998,\n",
      "        0.7040, 0.7648, 0.7713, 0.7730, 0.7279, 0.7664, 0.7663, 0.7868, 0.6881,\n",
      "        0.7998, 0.7218, 0.7414, 0.7834, 0.7480, 0.7538, 0.7419, 0.7571, 0.7229,\n",
      "        0.7356, 0.6881, 0.7142, 0.7674, 0.7039, 0.7602, 0.7745, 0.7327, 0.8027,\n",
      "        0.7355, 0.7150, 0.7247, 0.7708, 0.7679, 0.7688, 0.7025, 0.7111, 0.7583,\n",
      "        0.7545, 0.7847, 0.7305, 0.7440, 0.7651, 0.7392, 0.7924, 0.7169, 0.7455,\n",
      "        0.7730, 0.7564, 0.7215, 0.7927, 0.7611, 0.7527, 0.8124, 0.7412, 0.7937,\n",
      "        0.7906, 0.7371, 0.7471, 0.7404, 0.7746, 0.6988, 0.7665, 0.7669, 0.7546,\n",
      "        0.7934, 0.7177, 0.7355, 0.7302, 0.7889, 0.7554, 0.7526, 0.7854, 0.7527,\n",
      "        0.7619, 0.7634, 0.7519, 0.7700, 0.7025, 0.7910, 0.7407, 0.7329, 0.7352,\n",
      "        0.7648, 0.7050, 0.7679, 0.7672, 0.6977, 0.7651, 0.7784, 0.7342, 0.7629,\n",
      "        0.6896, 0.7130, 0.7048, 0.7885, 0.7623, 0.7899, 0.7966, 0.7714, 0.7619,\n",
      "        0.8158, 0.7382, 0.7267, 0.7489, 0.7676, 0.7724, 0.8247, 0.7530, 0.7572,\n",
      "        0.7620, 0.7291, 0.7171, 0.7245, 0.6585, 0.7010, 0.7142, 0.7172, 0.7139,\n",
      "        0.6998, 0.6983, 0.7497, 0.7308, 0.7083, 0.7083, 0.7673, 0.7618, 0.7474,\n",
      "        0.7779, 0.6767, 0.7896, 0.7885, 0.7211, 0.7743, 0.7854, 0.7306, 0.7263,\n",
      "        0.7825, 0.7809, 0.7487, 0.7922, 0.7801, 0.7733, 0.7384, 0.7391, 0.7534,\n",
      "        0.7609, 0.6890, 0.7361, 0.8018, 0.7690, 0.7942, 0.7461, 0.7221, 0.7148,\n",
      "        0.7745, 0.7934, 0.7403, 0.7806, 0.7920, 0.7468, 0.7543, 0.7519, 0.7449,\n",
      "        0.7338, 0.7831, 0.7767, 0.8018, 0.7528, 0.7474, 0.7733, 0.7200, 0.6893,\n",
      "        0.7442, 0.7355, 0.7077, 0.7887, 0.8226, 0.7108, 0.7300, 0.7675, 0.7756,\n",
      "        0.7615, 0.8065, 0.7323, 0.7117, 0.7193, 0.7454, 0.7259, 0.6778, 0.6991,\n",
      "        0.7952, 0.7148, 0.7849, 0.7542, 0.7477, 0.7214, 0.7749, 0.7860, 0.7487,\n",
      "        0.7607, 0.7941, 0.7920, 0.7280, 0.7277, 0.7372, 0.7530, 0.7582, 0.7417,\n",
      "        0.7200, 0.7117, 0.7631, 0.7451, 0.7543, 0.7441, 0.7459, 0.7342, 0.7482,\n",
      "        0.7351, 0.7336, 0.7429, 0.7670, 0.7990, 0.7531, 0.7723, 0.8086, 0.7702,\n",
      "        0.7467, 0.7523, 0.7542, 0.7870, 0.7297, 0.6858, 0.7588, 0.7602, 0.7338,\n",
      "        0.7344, 0.7168, 0.7655, 0.7456, 0.7419, 0.7345, 0.7042, 0.6743, 0.7704,\n",
      "        0.7319, 0.6843, 0.7686, 0.7124, 0.7632, 0.7126, 0.7319, 0.7026, 0.7822,\n",
      "        0.7268, 0.7275, 0.7332, 0.7282, 0.7524, 0.7101, 0.7376, 0.7243, 0.7847,\n",
      "        0.7769, 0.7708, 0.6835, 0.7784, 0.7755, 0.8171, 0.7915, 0.7461, 0.7030,\n",
      "        0.7516, 0.6958, 0.7158, 0.7516, 0.8019, 0.7763, 0.7193, 0.7296, 0.7359,\n",
      "        0.7785, 0.7101, 0.7174, 0.7957, 0.7637, 0.7949, 0.7558, 0.7776, 0.7755,\n",
      "        0.7917, 0.7491, 0.7447, 0.7378, 0.7281, 0.6787, 0.7256, 0.7708, 0.7278,\n",
      "        0.7599, 0.7528, 0.7345, 0.7721, 0.7913, 0.7130, 0.7857, 0.7511, 0.7333,\n",
      "        0.7576, 0.8010, 0.7726, 0.7418, 0.7611, 0.7279, 0.7983, 0.7880, 0.7362,\n",
      "        0.7259, 0.7649, 0.6864, 0.7867, 0.7637, 0.7775, 0.7573, 0.7902, 0.7724,\n",
      "        0.7801, 0.7601, 0.7977, 0.8126, 0.7720, 0.7753, 0.7829, 0.7492, 0.7724,\n",
      "        0.7709, 0.7095, 0.7716, 0.7987, 0.7702], device='cuda:0')\n",
      "tensor([0.6824, 0.7509, 0.7587, 0.8128, 0.7982, 0.7395, 0.7959, 0.7813, 0.8085,\n",
      "        0.7194, 0.7378, 0.7793, 0.7276, 0.8077, 0.7981, 0.8264, 0.7608, 0.7112,\n",
      "        0.7322, 0.7921, 0.8185, 0.7844, 0.8195, 0.7550, 0.7930, 0.7515, 0.7597,\n",
      "        0.7206, 0.7515, 0.7412, 0.7599, 0.7924, 0.7787, 0.8068, 0.7926, 0.7634,\n",
      "        0.7754, 0.8082, 0.7644, 0.7627, 0.7282, 0.7623, 0.7696, 0.7916, 0.7256,\n",
      "        0.7659, 0.7954, 0.7660, 0.7955, 0.7748, 0.7726, 0.7938, 0.7897, 0.8122,\n",
      "        0.7119, 0.7727, 0.7991, 0.7812, 0.8127, 0.7824, 0.8016, 0.8099, 0.7697,\n",
      "        0.8076, 0.8081, 0.7892, 0.8014, 0.7611, 0.7594, 0.7700, 0.7005, 0.7180,\n",
      "        0.7815, 0.7727, 0.7928, 0.8005, 0.7572, 0.7977, 0.7356, 0.7868, 0.7995,\n",
      "        0.7865, 0.7350, 0.8103, 0.7860, 0.7824, 0.7827, 0.7936, 0.7836, 0.7407,\n",
      "        0.7972, 0.7933, 0.7953, 0.8083, 0.7774, 0.7851, 0.7695, 0.7985, 0.8022,\n",
      "        0.8313, 0.8084, 0.7946, 0.7970, 0.7793, 0.8104, 0.7735, 0.7930, 0.8125,\n",
      "        0.7724, 0.8044, 0.7684, 0.7959, 0.8077, 0.7670, 0.7531, 0.7936, 0.7501,\n",
      "        0.8129, 0.7825, 0.7030, 0.7753, 0.7793, 0.7799, 0.6968, 0.7956, 0.7174,\n",
      "        0.7795, 0.8048, 0.7972, 0.8027, 0.7939, 0.7610, 0.7791, 0.7968, 0.7908,\n",
      "        0.7778, 0.8281, 0.7296, 0.7822, 0.7924, 0.7823, 0.7706, 0.7807, 0.7948,\n",
      "        0.7910, 0.7462, 0.8039, 0.7278, 0.7157, 0.7307, 0.7888, 0.7778, 0.8163,\n",
      "        0.7914, 0.8088, 0.7728, 0.7876, 0.7865, 0.7619, 0.7973, 0.8080, 0.7998,\n",
      "        0.7946, 0.7805, 0.7732, 0.7660, 0.7672, 0.7378, 0.7176, 0.7732, 0.7786,\n",
      "        0.8099, 0.7651, 0.8226, 0.7856, 0.7709, 0.7977, 0.7423, 0.8140, 0.7760,\n",
      "        0.7856, 0.7823, 0.7759, 0.7824, 0.7880, 0.7823, 0.7685, 0.8091, 0.7760,\n",
      "        0.8059, 0.6936, 0.7452, 0.7706, 0.7742, 0.8066, 0.8099, 0.8133, 0.7929,\n",
      "        0.8049, 0.8129, 0.7921, 0.7981, 0.8110, 0.7885, 0.7349, 0.7716, 0.7611,\n",
      "        0.7765, 0.7626, 0.7616, 0.7615, 0.7455, 0.7452, 0.8021, 0.7241, 0.7175,\n",
      "        0.7550, 0.7711, 0.7892, 0.7657, 0.7957, 0.7786, 0.7512, 0.8057, 0.8138,\n",
      "        0.7599, 0.7787, 0.8102, 0.7864, 0.7975, 0.7481, 0.7430, 0.7913, 0.7706,\n",
      "        0.7367, 0.7800, 0.7480, 0.7728, 0.7662, 0.7658, 0.7909, 0.7162, 0.7919,\n",
      "        0.7871, 0.7261, 0.8025, 0.7866, 0.7852, 0.7111, 0.8118, 0.7827, 0.8139,\n",
      "        0.7865, 0.7897, 0.7838, 0.7235, 0.7163, 0.7689, 0.7265, 0.8102, 0.7899,\n",
      "        0.7890, 0.7860, 0.7904, 0.7862, 0.8083, 0.7742, 0.8096, 0.7603, 0.7454,\n",
      "        0.7805, 0.7883, 0.7867, 0.6892, 0.7651, 0.8080, 0.8139, 0.7756, 0.7187,\n",
      "        0.7632, 0.7742, 0.7431, 0.7773, 0.7443, 0.6991, 0.7966, 0.7841, 0.7920,\n",
      "        0.7804, 0.8120, 0.8123, 0.7774, 0.8218, 0.8165, 0.8142, 0.7821, 0.7855,\n",
      "        0.7703, 0.7379, 0.6585, 0.7645, 0.7922, 0.7270, 0.7084, 0.7156, 0.7387,\n",
      "        0.7502, 0.7321, 0.6739, 0.6889, 0.7572, 0.7137, 0.7991, 0.7050, 0.7290,\n",
      "        0.7196, 0.7130, 0.6915, 0.7011, 0.7250, 0.7112, 0.7135, 0.8104, 0.7562,\n",
      "        0.7056, 0.7408, 0.7529, 0.7164, 0.7282, 0.6907, 0.7789, 0.7419, 0.6987,\n",
      "        0.7234, 0.7458, 0.7409, 0.7477, 0.7082, 0.8153, 0.6982, 0.7130, 0.7229,\n",
      "        0.6550, 0.7165, 0.7342, 0.6907, 0.7215, 0.7511, 0.6960, 0.7641, 0.7961,\n",
      "        0.7423, 0.7111, 0.7695, 0.6729, 0.6675, 0.6594, 0.7238, 0.7006, 0.7653,\n",
      "        0.7418, 0.7444, 0.7824, 0.7226, 0.8129, 0.7683, 0.7402, 0.6286, 0.6614,\n",
      "        0.6293, 0.7356, 0.7044, 0.6785, 0.7203, 0.7596, 0.7008, 0.6823, 0.6853,\n",
      "        0.7680, 0.6914, 0.7243, 0.7142, 0.7396, 0.7433, 0.8036, 0.7547, 0.7272,\n",
      "        0.7497, 0.7259, 0.6843, 0.6837, 0.6606, 0.6807, 0.6689, 0.6698, 0.7687,\n",
      "        0.7646, 0.6488, 0.7039, 0.7473, 0.6917, 0.7576, 0.6639, 0.7535, 0.6992,\n",
      "        0.7404, 0.7048, 0.7249, 0.7610, 0.7479, 0.6787, 0.6762, 0.6586, 0.7254,\n",
      "        0.7012, 0.7129, 0.7664, 0.7743, 0.7896, 0.7438, 0.7235, 0.7121, 0.7144,\n",
      "        0.7269, 0.6855, 0.7464, 0.7413, 0.7314, 0.7529, 0.7543, 0.7424, 0.7275,\n",
      "        0.6734, 0.7168, 0.7647, 0.7846, 0.7447, 0.7277, 0.7536, 0.7383, 0.7659,\n",
      "        0.7377, 0.7493, 0.7320, 0.7333, 0.7043, 0.6996, 0.6986, 0.7176, 0.7215,\n",
      "        0.7041, 0.7285, 0.7340, 0.7531, 0.7213, 0.7798, 0.7484, 0.7510, 0.6684,\n",
      "        0.7527, 0.7715, 0.7627, 0.7633, 0.7733, 0.6869, 0.7260, 0.6935, 0.7499,\n",
      "        0.7235, 0.7115, 0.7932, 0.7768, 0.6473, 0.7232, 0.6855, 0.7179, 0.7041,\n",
      "        0.7436, 0.7468, 0.7327, 0.7638, 0.7345, 0.7731, 0.7349, 0.7144, 0.7540,\n",
      "        0.7710, 0.7498, 0.8067, 0.7462, 0.7882, 0.6301, 0.8102, 0.7641, 0.7584,\n",
      "        0.6905, 0.7744, 0.7645, 0.6364, 0.6041, 0.7367, 0.6924, 0.7523, 0.7486,\n",
      "        0.6835, 0.6363, 0.6444, 0.6756, 0.7105, 0.6981, 0.6698, 0.7468, 0.7643,\n",
      "        0.7268, 0.7281, 0.7369, 0.7022, 0.7282, 0.7337, 0.7545, 0.7667, 0.7310,\n",
      "        0.7498, 0.7267, 0.7537, 0.7391, 0.7721, 0.7070, 0.7054, 0.7088, 0.7623,\n",
      "        0.7240, 0.7903, 0.7366, 0.7547, 0.6936, 0.6889, 0.7211, 0.6974, 0.7784,\n",
      "        0.7099, 0.7608, 0.7602, 0.7450, 0.7293, 0.6932, 0.8123, 0.7571, 0.7332,\n",
      "        0.8185, 0.7499, 0.7458, 0.7792, 0.8028, 0.7319, 0.7560, 0.7217, 0.7285,\n",
      "        0.7555, 0.7549, 0.7521, 0.7341, 0.7506, 0.6833, 0.7559, 0.7342, 0.7567,\n",
      "        0.7689, 0.7546, 0.7443, 0.7542, 0.7720, 0.7519, 0.7452, 0.7460, 0.7252,\n",
      "        0.7962, 0.7295, 0.7080, 0.7379, 0.7678, 0.7275, 0.7134, 0.7594, 0.7553,\n",
      "        0.7899, 0.7575, 0.6897, 0.7762, 0.7762, 0.7611, 0.7777, 0.7758, 0.7338,\n",
      "        0.7797, 0.7607, 0.7367, 0.7296, 0.7588, 0.6708, 0.7419, 0.7279, 0.7905,\n",
      "        0.6977, 0.6928, 0.7583, 0.7468, 0.7789, 0.7537, 0.7168, 0.7231, 0.7542,\n",
      "        0.7739, 0.7563, 0.6847, 0.7403, 0.7241, 0.7611, 0.7582, 0.7439, 0.7197,\n",
      "        0.7497, 0.7471, 0.7549, 0.7581, 0.7100, 0.7213, 0.6860, 0.7056, 0.7285,\n",
      "        0.7213, 0.6971, 0.7014, 0.7360, 0.7779, 0.6172, 0.7080, 0.7594, 0.6654,\n",
      "        0.7644, 0.7852, 0.7234, 0.7426, 0.7224, 0.7443, 0.6792, 0.7511, 0.7186,\n",
      "        0.7169, 0.7540, 0.7222, 0.6940, 0.7068, 0.7502, 0.6946, 0.6700, 0.7320,\n",
      "        0.7228, 0.7283, 0.7283, 0.7277, 0.7521, 0.7698, 0.7528, 0.7529, 0.7173,\n",
      "        0.7741, 0.7878, 0.7483, 0.7300, 0.7465, 0.7278, 0.7385, 0.7404, 0.7345,\n",
      "        0.7364, 0.7145, 0.7165, 0.7219, 0.7522, 0.7670, 0.7320, 0.7408, 0.7531,\n",
      "        0.7281, 0.7271, 0.7680, 0.7046, 0.7495, 0.6697, 0.6561, 0.7705, 0.6868,\n",
      "        0.7166, 0.6261, 0.7568, 0.7581, 0.7060, 0.7663, 0.7075, 0.7416, 0.7803,\n",
      "        0.7494, 0.6876, 0.7619, 0.7102, 0.7451, 0.7550, 0.7107, 0.7202, 0.7937,\n",
      "        0.6962, 0.7143, 0.7100, 0.7182, 0.7238, 0.7742, 0.7098, 0.7241, 0.7391,\n",
      "        0.7100, 0.7410, 0.7348, 0.6876, 0.7102, 0.7152, 0.7219, 0.7361, 0.7497,\n",
      "        0.7204, 0.7430, 0.7631, 0.7585, 0.7341, 0.7477, 0.7651, 0.7624, 0.6967,\n",
      "        0.7627, 0.7317, 0.7224, 0.7588, 0.6729, 0.7366, 0.7294, 0.7370, 0.7787,\n",
      "        0.7566, 0.7659, 0.7531, 0.7782, 0.7627, 0.7433, 0.7530, 0.7521, 0.6571,\n",
      "        0.7418, 0.7579, 0.7317, 0.7503, 0.7428, 0.6657, 0.7145, 0.7441, 0.7157,\n",
      "        0.7586, 0.7390, 0.7193, 0.7503, 0.7190, 0.7486, 0.7402, 0.7195, 0.7224,\n",
      "        0.7500, 0.7474, 0.6985, 0.7600, 0.7723, 0.7745, 0.7740, 0.6974, 0.7275,\n",
      "        0.7013, 0.6957, 0.7886, 0.8101, 0.7183, 0.7837, 0.6994, 0.7355, 0.7486,\n",
      "        0.7508, 0.7612, 0.7505, 0.7765, 0.7918], device='cuda:0')\n",
      "tensor([0.7572, 0.8139, 0.8104, 0.7184, 0.7217, 0.8018, 0.7909, 0.7480, 0.7952,\n",
      "        0.7642, 0.7671, 0.7671, 0.7974, 0.7721, 0.7891, 0.7826, 0.8105, 0.7548,\n",
      "        0.7175, 0.8094, 0.7317, 0.7922, 0.7359, 0.7989, 0.7698, 0.7992, 0.8109,\n",
      "        0.7986, 0.8041, 0.8100, 0.7532, 0.6973, 0.7740, 0.6923, 0.7937, 0.7961,\n",
      "        0.7867, 0.7960, 0.7709, 0.7965, 0.7904, 0.7219, 0.7317, 0.7885, 0.7833,\n",
      "        0.8008, 0.7670, 0.7804, 0.7861, 0.7991, 0.7877, 0.7834, 0.7688, 0.7379,\n",
      "        0.8088, 0.8045, 0.7715, 0.7633, 0.7791, 0.7636, 0.7430, 0.7967, 0.7973,\n",
      "        0.8074, 0.7646, 0.7722, 0.7670, 0.7869, 0.7119, 0.8151, 0.7181, 0.7120,\n",
      "        0.7828, 0.7830, 0.7625, 0.8075, 0.7979, 0.8101, 0.7823, 0.7684, 0.8132,\n",
      "        0.7679, 0.8146, 0.7913, 0.7805, 0.8146, 0.8129, 0.7751, 0.7306, 0.7253,\n",
      "        0.7243, 0.8121, 0.7938, 0.6824, 0.7925, 0.7014, 0.8123, 0.7973, 0.7336,\n",
      "        0.7148, 0.8092, 0.7423, 0.7212, 0.7478, 0.6873, 0.7981, 0.6623, 0.6576,\n",
      "        0.7164, 0.7611, 0.8018, 0.6214, 0.7792, 0.6707, 0.7582, 0.8144, 0.6800,\n",
      "        0.6828, 0.7751, 0.8053, 0.7366, 0.7533, 0.8167, 0.8097, 0.6817, 0.6524,\n",
      "        0.6729, 0.6650, 0.7594, 0.8106, 0.8186, 0.7448, 0.6579, 0.6919, 0.7814,\n",
      "        0.6994, 0.7784, 0.6662, 0.7228, 0.6993, 0.8013, 0.7958, 0.7782, 0.7137,\n",
      "        0.7331, 0.7322, 0.7722, 0.8009, 0.7831, 0.7376, 0.6690, 0.7620, 0.7301,\n",
      "        0.7738, 0.7856, 0.8130, 0.8041, 0.7839, 0.7816, 0.7770, 0.7867, 0.6952,\n",
      "        0.7229, 0.7503, 0.7452, 0.7960, 0.7769, 0.7996, 0.6649, 0.6649, 0.8025,\n",
      "        0.7657, 0.7722, 0.7978, 0.8021, 0.6788, 0.7186, 0.7914, 0.7650, 0.7794,\n",
      "        0.8124, 0.6838, 0.7413, 0.7917, 0.6382, 0.6760, 0.7110, 0.7414, 0.7617,\n",
      "        0.7951, 0.7751, 0.7693, 0.7713, 0.7969, 0.7748, 0.7764, 0.7105, 0.7340,\n",
      "        0.8095, 0.8086, 0.6221, 0.8048, 0.7869, 0.8073, 0.7963, 0.8102, 0.6857,\n",
      "        0.7488, 0.6778, 0.6255, 0.6341, 0.7640, 0.7989, 0.7831, 0.7042, 0.7684,\n",
      "        0.7796, 0.7377, 0.7003, 0.7301, 0.7741, 0.7653, 0.7686, 0.7841, 0.8058,\n",
      "        0.7696, 0.7476, 0.7772, 0.7994, 0.7833, 0.6991, 0.7300, 0.7588, 0.7838,\n",
      "        0.7664, 0.7992, 0.7664, 0.7783, 0.7981, 0.8056, 0.7780, 0.7701, 0.7744,\n",
      "        0.7285, 0.7992, 0.7704, 0.7927, 0.7555, 0.7228, 0.7546, 0.7488, 0.7779,\n",
      "        0.7916, 0.7637, 0.7544, 0.7486, 0.7664, 0.7658, 0.7869, 0.7436, 0.8025,\n",
      "        0.7398, 0.7670, 0.7973, 0.7903, 0.7871, 0.7817, 0.8127, 0.8102, 0.7957,\n",
      "        0.7853, 0.7661, 0.7826, 0.7825, 0.7783, 0.7953, 0.7713, 0.7717, 0.7259,\n",
      "        0.7750, 0.7364, 0.8092, 0.8022, 0.7201, 0.7790, 0.7615, 0.7824, 0.8029,\n",
      "        0.7824, 0.7857, 0.7719, 0.7827, 0.7896, 0.8101, 0.8109, 0.7146, 0.7113,\n",
      "        0.7835, 0.6976, 0.7936, 0.7362, 0.7818, 0.7749, 0.7848, 0.7855, 0.7037,\n",
      "        0.7920, 0.6835, 0.6499, 0.7295, 0.8045, 0.7581, 0.7996, 0.6547, 0.7951,\n",
      "        0.7216, 0.7410, 0.8025, 0.7721, 0.7900, 0.7181, 0.7102, 0.7207, 0.7979,\n",
      "        0.7645, 0.7674, 0.7792, 0.7982, 0.7910, 0.7665, 0.7496, 0.8090, 0.7386,\n",
      "        0.7886, 0.7282, 0.7106, 0.7488, 0.7713, 0.7641, 0.7285, 0.7295, 0.8006,\n",
      "        0.6508, 0.7844, 0.7556, 0.7750, 0.6983, 0.7334, 0.7751, 0.8087, 0.8016,\n",
      "        0.8081, 0.8048, 0.7107, 0.7813, 0.8090, 0.7847, 0.7112, 0.7163, 0.7702,\n",
      "        0.7223, 0.7363, 0.7309, 0.8093, 0.6793, 0.7939, 0.7072, 0.8052, 0.7296,\n",
      "        0.7466, 0.7751, 0.7647, 0.7359, 0.7916, 0.8130, 0.8091, 0.7797, 0.7881,\n",
      "        0.7603, 0.7720, 0.7886, 0.7876, 0.7872, 0.7817, 0.7692, 0.7915, 0.7827,\n",
      "        0.7851, 0.6466, 0.7348, 0.7736, 0.6595, 0.7389, 0.8022, 0.7646, 0.8040,\n",
      "        0.7586, 0.7812, 0.8030, 0.8063, 0.7829, 0.6921, 0.7854, 0.7730, 0.7790,\n",
      "        0.7492, 0.8011, 0.7948, 0.7692, 0.7701, 0.8177, 0.7857, 0.7568, 0.6970,\n",
      "        0.7022, 0.7945, 0.7336, 0.7849, 0.7637, 0.7547, 0.7153, 0.7141, 0.7135,\n",
      "        0.7635, 0.7660, 0.7756, 0.7824, 0.7088, 0.7288, 0.7441, 0.7616, 0.7596,\n",
      "        0.7418, 0.7833, 0.7341, 0.7520, 0.7402, 0.7378, 0.7521, 0.7642, 0.7340,\n",
      "        0.7330, 0.7733, 0.7923, 0.7566, 0.7605, 0.6550, 0.7073, 0.6698, 0.7790,\n",
      "        0.7340, 0.7512, 0.7552, 0.7450, 0.7232, 0.7617, 0.7468, 0.7463, 0.7282,\n",
      "        0.7236, 0.7740, 0.7598, 0.7308, 0.7582, 0.7301, 0.7624, 0.6592, 0.6795,\n",
      "        0.7453, 0.7064, 0.7613, 0.7359, 0.7426, 0.7191, 0.8092, 0.7326, 0.7173,\n",
      "        0.7532, 0.7561, 0.7592, 0.6827, 0.7492, 0.7577, 0.7276, 0.7168, 0.7575,\n",
      "        0.6986, 0.7235, 0.7241, 0.7178, 0.7346, 0.8040, 0.7526, 0.7505, 0.7803,\n",
      "        0.7955, 0.7994, 0.7281, 0.6820, 0.7561, 0.7692, 0.6899, 0.7149, 0.7615,\n",
      "        0.7154, 0.7897, 0.7582, 0.7156, 0.7039, 0.7419, 0.7282, 0.7804, 0.7741,\n",
      "        0.8070, 0.7127, 0.7269, 0.7265, 0.7429, 0.7277, 0.7444, 0.7167, 0.7440,\n",
      "        0.7443, 0.7442, 0.7579, 0.7161, 0.6659, 0.7277, 0.7586, 0.7280, 0.6698,\n",
      "        0.7352, 0.7534, 0.7332, 0.7465, 0.7392, 0.7689, 0.7516, 0.7536, 0.7360,\n",
      "        0.7570, 0.7924, 0.6798, 0.7143, 0.7142, 0.7301, 0.7089, 0.7158, 0.6974,\n",
      "        0.7099, 0.6976, 0.7230, 0.6922, 0.7422, 0.7240, 0.8098, 0.7894, 0.7512,\n",
      "        0.7628, 0.6899, 0.6972, 0.7759, 0.7636, 0.7390, 0.7302, 0.7058, 0.7237,\n",
      "        0.6854, 0.7622, 0.7566, 0.7454, 0.7430, 0.6738, 0.7427, 0.7288, 0.7213,\n",
      "        0.6700, 0.7758, 0.7574, 0.7571, 0.7488, 0.6856, 0.7568, 0.7448, 0.6953,\n",
      "        0.7118, 0.7117, 0.7275, 0.7283, 0.7274, 0.7268, 0.7613, 0.7613, 0.7166,\n",
      "        0.6734, 0.7740, 0.6667, 0.6538, 0.6538, 0.7683, 0.7316, 0.6941, 0.6817,\n",
      "        0.7594, 0.7437, 0.7432, 0.7647, 0.6950, 0.7249, 0.7641, 0.7281, 0.7571,\n",
      "        0.7050, 0.6951, 0.7112, 0.6729, 0.7155, 0.6633, 0.6598, 0.7039, 0.7288,\n",
      "        0.6896, 0.7099, 0.7387, 0.7451, 0.7387, 0.7377, 0.7652, 0.8003, 0.7208,\n",
      "        0.7508, 0.7697, 0.7345, 0.6976, 0.7289, 0.7244, 0.7112, 0.7857, 0.7198,\n",
      "        0.7639, 0.7585, 0.7490, 0.7212, 0.7299, 0.7177, 0.7536, 0.7477, 0.7519,\n",
      "        0.7473, 0.6586, 0.7270, 0.7857, 0.7576, 0.7397, 0.7716, 0.7879, 0.7960,\n",
      "        0.7022, 0.7431, 0.7698, 0.7205, 0.7145, 0.7699, 0.7685, 0.7401, 0.7401,\n",
      "        0.7552, 0.7823, 0.7624, 0.7128, 0.7187, 0.7074, 0.7400, 0.7496, 0.7731,\n",
      "        0.7682, 0.6753, 0.7237, 0.7493, 0.7365, 0.7632, 0.7809, 0.7794, 0.7509,\n",
      "        0.7526, 0.7514, 0.7587, 0.7219, 0.7402, 0.7591, 0.7219, 0.7396, 0.7652,\n",
      "        0.7145, 0.7198, 0.6947, 0.7223, 0.7557, 0.7318, 0.7560, 0.8076, 0.7529,\n",
      "        0.7635, 0.6574, 0.7471, 0.7740, 0.7479, 0.7566, 0.7694, 0.6974, 0.7219,\n",
      "        0.7331, 0.7562, 0.8097, 0.7636, 0.7432, 0.7661, 0.7538, 0.7652, 0.7683,\n",
      "        0.7634, 0.7607, 0.7417, 0.7459, 0.7033, 0.7245, 0.7296, 0.7295, 0.7022,\n",
      "        0.6823, 0.6618, 0.8155, 0.7837, 0.7099, 0.6979, 0.6625, 0.7625, 0.7855,\n",
      "        0.7630, 0.7166, 0.7465, 0.7161, 0.7484, 0.7212, 0.7461, 0.7469, 0.7511,\n",
      "        0.7520, 0.7352, 0.7912, 0.7014, 0.7453, 0.7147, 0.7990, 0.7458, 0.7513,\n",
      "        0.7506, 0.7513, 0.7546, 0.7416, 0.7694, 0.7832, 0.7578, 0.7740, 0.7415,\n",
      "        0.7586, 0.7532, 0.7483, 0.7463, 0.7507, 0.7277, 0.6891, 0.6822, 0.7146,\n",
      "        0.7332, 0.7562, 0.7294, 0.8117, 0.7463, 0.7685, 0.7453, 0.7404, 0.7613,\n",
      "        0.7460, 0.6797, 0.7353, 0.7258, 0.6831, 0.7382, 0.7547, 0.7107, 0.8069,\n",
      "        0.7395, 0.7539, 0.7606, 0.7581, 0.7565, 0.7191, 0.7277, 0.7094, 0.6895,\n",
      "        0.7344, 0.7283, 0.7523, 0.7679, 0.6961, 0.7655, 0.7630, 0.7459, 0.7296,\n",
      "        0.7277, 0.7605, 0.6898, 0.7464, 0.7152, 0.7090, 0.7275, 0.7648, 0.7523,\n",
      "        0.7344, 0.7129, 0.7087, 0.7238, 0.7421, 0.7671, 0.7043, 0.7602, 0.7606,\n",
      "        0.7561, 0.7337, 0.7209, 0.7558, 0.6830, 0.7743, 0.7645, 0.7268, 0.7421,\n",
      "        0.7527, 0.8131, 0.8038, 0.7099, 0.7586, 0.7171, 0.7524, 0.7657, 0.7215,\n",
      "        0.7456, 0.7264, 0.7137, 0.6956, 0.7437, 0.7372, 0.7147, 0.7277, 0.7506,\n",
      "        0.7506, 0.7643, 0.7797, 0.7636, 0.7816, 0.7596, 0.7298, 0.7381, 0.7330,\n",
      "        0.7534, 0.7949, 0.7099, 0.7594, 0.7566, 0.7931, 0.6728, 0.6722, 0.7123,\n",
      "        0.7572, 0.7483, 0.7220, 0.7480, 0.7269, 0.7377, 0.6974, 0.7756, 0.7915,\n",
      "        0.7736, 0.7753, 0.7784, 0.7528, 0.7651, 0.7686, 0.8151, 0.7995, 0.7467,\n",
      "        0.7514, 0.7239, 0.7842, 0.8053, 0.7552, 0.8122, 0.7957, 0.7488, 0.7525,\n",
      "        0.7620, 0.7728, 0.7391, 0.7455, 0.7017, 0.7597, 0.7523, 0.7550, 0.8085,\n",
      "        0.7338, 0.7496, 0.7024, 0.7402, 0.7193, 0.7591, 0.7504, 0.7505, 0.7600,\n",
      "        0.7095, 0.8199, 0.7404, 0.7493, 0.7847, 0.7900, 0.8011, 0.7429, 0.7518,\n",
      "        0.8122, 0.6755, 0.7683, 0.7757, 0.7362, 0.7859, 0.7031, 0.7517, 0.7201,\n",
      "        0.7265, 0.6877, 0.7637, 0.7468, 0.7426, 0.7557, 0.7996, 0.7428, 0.7857,\n",
      "        0.7415, 0.7633, 0.8117, 0.7321, 0.7361, 0.7171, 0.6816, 0.7372, 0.7272,\n",
      "        0.7788, 0.7406, 0.7339, 0.7578, 0.7284, 0.8136, 0.7171, 0.7297, 0.7217,\n",
      "        0.8129, 0.6478, 0.7578, 0.7515, 0.7735, 0.7743, 0.7599, 0.7416, 0.7687,\n",
      "        0.7913, 0.7555, 0.7523, 0.7281, 0.7880, 0.8121, 0.8088, 0.7431, 0.7472,\n",
      "        0.6540, 0.7282, 0.7449, 0.8115, 0.7219, 0.7187, 0.7205, 0.6926, 0.7637,\n",
      "        0.7341, 0.7888, 0.7549, 0.6997, 0.7256, 0.6523, 0.6132, 0.6345, 0.7470],\n",
      "       device='cuda:0')\n",
      "tensor([0.8159, 0.8135, 0.7678,  ..., 0.6499, 0.7476, 0.6959], device='cuda:0')\n",
      "tensor([0.7712, 0.7888, 0.7974,  ..., 0.6335, 0.7489, 0.7550], device='cuda:0')\n",
      "tensor([0.6925, 0.6597, 0.8034,  ..., 0.8016, 0.7618, 0.7191], device='cuda:0')\n",
      "tensor([0.8000, 0.5889, 0.6477,  ..., 0.7038, 0.7524, 0.7824], device='cuda:0')\n",
      "tensor([0.7929, 0.5503, 0.7557,  ..., 0.7395, 0.7839, 0.8062], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOUNTERS\u001b[38;5;241m.\u001b[39mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode\u001b[39m\u001b[38;5;124m\"\u001b[39m, COUNTERS\u001b[38;5;241m.\u001b[39mepisodes, COUNTERS\u001b[38;5;241m.\u001b[39mepisodes)\n\u001b[0;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished episode, starting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate(results)\n",
      "Cell \u001b[0;32mIn[40], line 131\u001b[0m, in \u001b[0;36mCadicalEnv.run_episode\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruns_per_episode):\n\u001b[1;32m    130\u001b[0m     instance \u001b[38;5;241m=\u001b[39m get_random_instance()\n\u001b[0;32m--> 131\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m EpisodeResult\u001b[38;5;241m.\u001b[39mmerge_all(results)\n",
      "Cell \u001b[0;32mIn[40], line 117\u001b[0m, in \u001b[0;36mCadicalEnv.run_instance\u001b[0;34m(self, agent, instance)\u001b[0m\n\u001b[1;32m    114\u001b[0m     result\u001b[38;5;241m.\u001b[39madd_reward(\u001b[38;5;241m-\u001b[39m(stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflicts\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m last_conflicts) \u001b[38;5;241m*\u001b[39m HP\u001b[38;5;241m.\u001b[39mpenalty_per_conflict)\n\u001b[1;32m    115\u001b[0m     result\u001b[38;5;241m.\u001b[39mcomplete(stats)\n\u001b[0;32m--> 117\u001b[0m \u001b[43msrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_instance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcadical_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--reduce-mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroutes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/projects/sat-rl-exploration/py/srunner.py:238\u001b[0m, in \u001b[0;36mrun_instance\u001b[0;34m(solver, args, cnf_path, routes, silent, timeout_seconds, data, valgrind)\u001b[0m\n\u001b[1;32m    230\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(args))\n\u001b[1;32m    232\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m    233\u001b[0m     args,\n\u001b[1;32m    234\u001b[0m     stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mDEVNULL \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    235\u001b[0m     stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mDEVNULL \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    236\u001b[0m )\n\u001b[0;32m--> 238\u001b[0m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroutes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m process\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/sat-rl-exploration/py/srunner.py:52\u001b[0m, in \u001b[0;36mConnection.start\u001b[0;34m(self, routes, run_info, data)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unknown request through pipe: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m fn \u001b[38;5;241m=\u001b[39m routes[request]\n\u001b[0;32m---> 52\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush()\n",
      "Cell \u001b[0;32mIn[40], line 32\u001b[0m, in \u001b[0;36mCadicalEnv.run_instance.<locals>._reduce_route\u001b[0;34m(conn, info, _data)\u001b[0m\n\u001b[1;32m     30\u001b[0m     levels[i] \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mread_i32()\n\u001b[1;32m     31\u001b[0m num_clauses \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mread_u64()\n\u001b[0;32m---> 32\u001b[0m clauses \u001b[38;5;241m=\u001b[39m [\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_clause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_clauses)]\n\u001b[1;32m     33\u001b[0m num_reducible \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mread_u64()\n\u001b[1;32m     34\u001b[0m num_target \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mread_u64()\n",
      "File \u001b[0;32m~/projects/sat-rl-exploration/py/srunner.py:116\u001b[0m, in \u001b[0;36mConnection.read_clause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m activity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_f32()\n\u001b[1;32m    115\u001b[0m conflicts_on_creation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_f32()\n\u001b[0;32m--> 116\u001b[0m times_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_u32\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Clause(\n\u001b[1;32m    118\u001b[0m     id_,\n\u001b[1;32m    119\u001b[0m     lits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     times_reason\u001b[38;5;241m=\u001b[39mtimes_reason,\n\u001b[1;32m    127\u001b[0m )\n",
      "File \u001b[0;32m~/projects/sat-rl-exploration/py/srunner.py:81\u001b[0m, in \u001b[0;36mConnection.read_u32\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_u32\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstruct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    COUNTERS.episodes += 1\n",
    "    print(f\"Episode {COUNTERS.episodes}\")\n",
    "    writer.add_scalar(\"episode\", COUNTERS.episodes, COUNTERS.episodes)\n",
    "    results = env.run_episode(agent)\n",
    "    logger.info(\"Finished episode, starting training\")\n",
    "    agent.update(results)\n",
    "\n",
    "    torch.save({\n",
    "        \"model\": agent.model.state_dict(),\n",
    "        \"optim\": agent.optim.state_dict(),\n",
    "        \"counters\": dataclasses.asdict(COUNTERS),\n",
    "    }, f\"../checkpoints-shallow/agent-{COUNTERS.episodes}-{uuid.uuid4()}.pt\")\n",
    "\n",
    "    writer.add_scalar(\"episode_reward\", sum(results.rewards), COUNTERS.episodes)\n",
    "    print(f\"Rewards: {sum(results.rewards)}\")\n",
    "    del results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory._dump_snapshot(f\"memory.pickle\")\n",
    "\n",
    "# Stop recording memory snapshot history.\n",
    "torch.cuda.memory._record_memory_history(enabled=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
