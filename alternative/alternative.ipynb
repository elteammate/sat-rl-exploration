{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch_geometric.nn as gnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import pyvis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Protocol, cast\n",
    "import enum\n",
    "import dataclasses\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import uuid\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Hyperparameters:\n",
    "    batch_size: int = 64\n",
    "    runs_per_episode: int = 64\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 1e-5\n",
    "    eps_clip: float = 0.2\n",
    "    entropy_coef: float = 0.01\n",
    "    weight_decay: float = 1e-3\n",
    "    value_weight: float = 0.5\n",
    "    policy_weight: float = 1.0\n",
    "    gae_gamma: float = 0.95\n",
    "    gae_lambda: float = 0.8\n",
    "    penalty_per_conflict: float = 5e-5\n",
    "    temperature: float = 4.0\n",
    "\n",
    "\n",
    "HP = Hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"notebook\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "np.seterr(all='raise')\n",
    "DEV = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Counters:\n",
    "    episodes: int = 0 \n",
    "    epochs: int = 0\n",
    "    runs: int = 0\n",
    "    steps: int = 0\n",
    "    batches: int = 0\n",
    "    train_steps: int = 0\n",
    "\n",
    "    def from_dict(self, d):\n",
    "        for k, v in d.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "COUNTERS = Counters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedValueNormalizationLogits(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits: torch.Tensor, ex: torch.Tensor):\n",
    "        ex = torch.as_tensor(ex)\n",
    "\n",
    "        b = torch.zeros(logits.shape[:-1], device=logits.device)\n",
    "\n",
    "        for _ in range(100):\n",
    "            normalized = torch.sigmoid(logits + b.unsqueeze(-1))\n",
    "            f_gamma = normalized.sum(dim=-1) - ex\n",
    "            f_prime_gamma = (normalized * (1 - normalized)).sum(dim=-1)\n",
    "            diff = torch.clamp(f_gamma / f_prime_gamma, -2, 2)\n",
    "            if torch.all(diff.abs() < 1e-6):\n",
    "                break\n",
    "            b = b - diff\n",
    "\n",
    "        normalized = torch.sigmoid(logits + b.unsqueeze(-1))\n",
    "        ctx.save_for_backward(normalized)\n",
    "        return normalized\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        normalized, = ctx.saved_tensors\n",
    "        p_grad = normalized * (1 - normalized)\n",
    "        denom = p_grad.sum(dim=-1)\n",
    "        coordwise = p_grad * g\n",
    "\n",
    "        grad = coordwise - p_grad * coordwise.sum(axis=-1).unsqueeze(-1) / denom.unsqueeze(-1)\n",
    "\n",
    "        return grad, None\n",
    "\n",
    "\n",
    "probs = torch.tensor([\n",
    "    [0.999, 0.5, 0.5, 0.5, 0.1],\n",
    "    [0.3, 0.5, 0.5, 0.8, 0.2],\n",
    "], requires_grad=True)\n",
    "x = -(1 / probs - 1).log()\n",
    "y = ExpectedValueNormalizationLogits.apply(x, torch.tensor([2.0, 1.0]))\n",
    "# print(x, y, y.sum(axis=-1), sep=\"\\n\")\n",
    "y.sum().backward()\n",
    "# print(probs.grad)\n",
    "\n",
    "optim = torch.optim.SGD([probs], lr=0.1)\n",
    "for _ in range(100):\n",
    "    optim.zero_grad()\n",
    "    x = -(1 / probs - 1).log()\n",
    "    y = ExpectedValueNormalizationLogits.apply(x, torch.tensor([2.0, 1.0]))\n",
    "    loss = y.pow(3.0).sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    # print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphProblem(Protocol):\n",
    "    global_data: torch.Tensor\n",
    "    x: torch.Tensor\n",
    "    edge_index: torch.Tensor\n",
    "    edge_attr: torch.Tensor\n",
    "    reducible: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, edge_dim: int, disambiguate: bool):\n",
    "        super().__init__()\n",
    "\n",
    "        self.disambiguate = disambiguate\n",
    "\n",
    "        if disambiguate:\n",
    "            self.conv_reducible = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "            self.conv_irreducible = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "        else:\n",
    "            self.conv = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, h, data):\n",
    "        edge_index, edge_attr, reducible = data.edge_index, data.edge_attr, data.reducible\n",
    "\n",
    "        out = torch.zeros_like(h)\n",
    "\n",
    "        if self.disambiguate:\n",
    "            out[reducible, :] = self.conv_reducible(h, edge_index, edge_attr)[reducible, :]\n",
    "            out[~reducible, :] = self.conv_irreducible(h, edge_index, edge_attr)[~reducible, :]\n",
    "        else:\n",
    "            out = self.conv(h, edge_index, edge_attr)\n",
    "\n",
    "        out = self.activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: list[int],\n",
    "        disambiguate_clauses_in_first: int = 2,\n",
    "        edge_dim: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.input_fc = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "        self.processing_blocks = nn.ModuleList([\n",
    "            ProcessingBlock(\n",
    "                hidden_dims[i],\n",
    "                hidden_dims[i + 1],\n",
    "                edge_dim,\n",
    "                i < disambiguate_clauses_in_first\n",
    "            )\n",
    "            for i in range(len(hidden_dims) - 1)\n",
    "        ])\n",
    "\n",
    "        self.output_fc = nn.Linear(hidden_dims[-1], 1, bias=False)\n",
    "        self.value_fc_reducible = nn.Linear(hidden_dims[-1], 1, bias=False)\n",
    "        self.value_fc_irreducible = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, graph: GraphProblem, ex: torch.Tensor | float):\n",
    "        logger.info(\"Starting model forward pass\")\n",
    "\n",
    "        input_ = torch.cat([\n",
    "            graph.global_data.view((1, -1)).expand(graph.x.shape[0], -1),\n",
    "            graph.x,\n",
    "        ], dim=-1)\n",
    "\n",
    "        x = self.silu(self.input_fc(input_))\n",
    "\n",
    "        for block in self.processing_blocks:\n",
    "            x = block(x, graph)\n",
    "\n",
    "        logits = self.output_fc(x[graph.reducible]).view(-1)\n",
    "\n",
    "        probs = ExpectedValueNormalizationLogits.apply(logits * HP.temperature, ex)\n",
    "        if torch.isnan(probs).any():\n",
    "            print(logits)\n",
    "            print(probs)\n",
    "            raise AssertionError()\n",
    "\n",
    "        value_reducible = self.value_fc_reducible(x[graph.reducible]).view(-1)\n",
    "        value_irreducible = self.value_fc_irreducible(x[~graph.reducible]).view(-1)\n",
    "        value = torch.cat([value_reducible, value_irreducible]).sum()\n",
    "\n",
    "        logger.info(\"Finished model forward pass\")\n",
    "\n",
    "        return probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_advantages(rewards: list[float], values: list[float]) -> tuple[list[float], list[float]]:\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + HP.gae_gamma * (values[i + 1] if i + 1 < len(values) else 0) - values[i]\n",
    "        gae = delta + HP.gae_gamma * HP.gae_lambda * gae\n",
    "        returns.append(gae + values[i])\n",
    "        advantages.append(gae)\n",
    "    return returns[::-1], advantages[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class EpisodeResult:\n",
    "    states: list[GraphProblem]\n",
    "    dists: list[torch.distributions.Bernoulli]\n",
    "    actions: list[torch.Tensor]\n",
    "    rewards: list[float]\n",
    "    values: list[float]\n",
    "    returns: list[float]\n",
    "    advantages: list[float]\n",
    "\n",
    "    stats: list[dict]\n",
    "\n",
    "    @staticmethod\n",
    "    def empty() -> 'EpisodeResult':\n",
    "        return EpisodeResult([], [], [], [], [], [], [], [])\n",
    "\n",
    "    def merge_with(self, other: 'EpisodeResult') -> 'EpisodeResult':\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards) == len(self.returns) ==\n",
    "            len(self.advantages)\n",
    "        )\n",
    "        assert (\n",
    "            len(other.states) == len(other.dists) == len(other.actions) ==\n",
    "            len(other.values) == len(other.rewards) == len(other.returns) ==\n",
    "            len(other.advantages)\n",
    "        )\n",
    "        return EpisodeResult(\n",
    "            self.states + other.states,\n",
    "            self.dists + other.dists,\n",
    "            self.actions + other.actions,\n",
    "            self.rewards + other.rewards,\n",
    "            self.values + other.values,\n",
    "            self.returns + other.returns,\n",
    "            self.advantages + other.advantages,\n",
    "            self.stats + other.stats,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_all(results: list['EpisodeResult']) -> 'EpisodeResult':\n",
    "        result = results[0]\n",
    "        for other in results[1:]:\n",
    "            result = result.merge_with(other)\n",
    "        return result\n",
    "\n",
    "    def add(self, *, state, dist, action, reward, value):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards)\n",
    "        )\n",
    "        self.states.append(state)\n",
    "        self.dists.append(dist)\n",
    "        self.actions.append(action)\n",
    "        if reward is not None:\n",
    "            self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def add_reward(self, reward):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards) + 1\n",
    "        )\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def complete(self, stats: dict):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards)\n",
    "        )\n",
    "        assert len(self.stats) == 0\n",
    "        self.returns, self.advantages = compute_returns_advantages(self.rewards, self.values)\n",
    "        self.stats = [stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.to(DEV)\n",
    "        self.optim = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=HP.learning_rate,\n",
    "            weight_decay=HP.weight_decay\n",
    "        )\n",
    "\n",
    "    def act(self, graph: GraphProblem, ex: float):\n",
    "        COUNTERS.steps += 1\n",
    "        logger.info(\"Running agent act\")\n",
    "        with torch.no_grad():\n",
    "            graph = graph.to(DEV)\n",
    "            assert torch.isnan(graph.x).sum() == 0\n",
    "            assert torch.isnan(graph.edge_attr).sum() == 0\n",
    "            assert torch.isnan(graph.global_data).sum() == 0\n",
    "            probs, value = self.model(graph, ex)\n",
    "            logger.info(\"Finished agent act\")\n",
    "            return torch.distributions.Bernoulli(probs), value.item()\n",
    "\n",
    "    def update(self, results: EpisodeResult, silent: bool = False):\n",
    "        logger.info(\"Training agent\")\n",
    "        r = results\n",
    "\n",
    "        n = len(r.states)\n",
    "        batch_size = HP.batch_size\n",
    "        advantages = torch.tensor(r.advantages, dtype=torch.float32)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for num_epoch in range(HP.epochs):\n",
    "            COUNTERS.epochs += 1\n",
    "            logger.info(\"Training agent, epoch %d\", num_epoch)\n",
    "            for batch_start in range(0, n, batch_size):\n",
    "                COUNTERS.batches += 1\n",
    "                sample_weight = np.abs(np.array(advantages))\n",
    "                sample_weight = sample_weight / sample_weight.sum()\n",
    "                idx = np.random.choice(range(n), size=batch_size, replace=False, p=sample_weight)\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "                for i in idx:\n",
    "                    COUNTERS.train_steps += 1\n",
    "                    graph, ex = r.states[i]\n",
    "                    new_probs, value = self.model(graph, ex)\n",
    "                    value = value.cpu()\n",
    "                    dist = torch.distributions.Bernoulli(probs=new_probs)\n",
    "                    log_probs = dist.log_prob(r.actions[i].float())\n",
    "                    entropy = dist.entropy().mean()\n",
    "\n",
    "                    ratio = torch.exp(log_probs - r.dists[i].log_prob(r.actions[i].float()))\n",
    "                    surr1 = ratio * advantages[i]\n",
    "                    surr2 = torch.clamp(ratio, 1.0 - HP.eps_clip, 1.0 + HP.eps_clip) * advantages[i]\n",
    "\n",
    "                    value_loss = torch.nn.functional.huber_loss(value, torch.tensor(r.returns[i], dtype=torch.float32))\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                    if not silent:\n",
    "                        writer.add_scalar(\"value loss\", value_loss.item(), COUNTERS.train_steps)\n",
    "\n",
    "                    loss = (\n",
    "                        HP.policy_weight * policy_loss\n",
    "                        + HP.value_weight * value_loss\n",
    "                        - HP.entropy_coef * entropy\n",
    "                    ) / len(idx)\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class GameConfig:\n",
    "    number_of_bits: int = 4\n",
    "    number_of_cards_to_add: tuple[int] = (16, 8, 12, 16, 24, 32, 48, 48, 48, 48)\n",
    "    fraction_to_remove: float = 0.5\n",
    "    fraction_to_make_reducible: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEnv:\n",
    "    def __init__(self, game_cfg: GameConfig, runs_per_episode: int = 16):\n",
    "        self.runs_per_episode = runs_per_episode\n",
    "        self.game_cfg = game_cfg\n",
    "\n",
    "    def run_instance(self, agent: Agent):\n",
    "        COUNTERS.runs += 1\n",
    "        result = EpisodeResult.empty()\n",
    "\n",
    "        max_card = 2 ** self.game_cfg.number_of_bits\n",
    "        deck = np.random.randint(0, max_card, size=self.game_cfg.number_of_cards_to_add[0])\n",
    "        reducible = np.ones_like(deck, dtype=bool)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(len(self.game_cfg.number_of_cards_to_add) - 1):\n",
    "            COUNTERS.steps += 1\n",
    "            logger.info(\"Running instance step %d\", step)\n",
    "\n",
    "            problem = Data(\n",
    "                global_data=torch.tensor([\n",
    "                    len(deck) / sum(self.game_cfg.number_of_cards_to_add),\n",
    "                ], dtype=torch.float32),\n",
    "                x=torch.tensor(np.array([\n",
    "                    deck / (max_card - 1),\n",
    "                    *[np.bitwise_and(deck, 1 << i) >> i for i in range(self.game_cfg.number_of_bits)],\n",
    "                ]), dtype=torch.float32).permute(1, 0),\n",
    "                edge_index=torch.tensor(np.array([\n",
    "                    np.arange(len(deck)),\n",
    "                    np.arange(1, len(deck) + 1) % len(deck),\n",
    "                ]), dtype=torch.long),\n",
    "                edge_attr=torch.zeros((len(deck), 0), dtype=torch.float32),\n",
    "                reducible=torch.tensor(reducible, dtype=torch.bool),\n",
    "            )\n",
    "\n",
    "            ex = self.game_cfg.fraction_to_remove * sum(reducible)\n",
    "            dist, value = agent.act(problem, ex)\n",
    "            action = dist.sample()\n",
    "\n",
    "            indices = np.arange(len(deck))[reducible][action.cpu().numpy() == 1]\n",
    "            deck = np.delete(deck, indices)\n",
    "            reducible = np.delete(reducible, indices)\n",
    "\n",
    "            reducible = np.random.rand(*reducible.shape) < self.game_cfg.fraction_to_make_reducible\n",
    "\n",
    "            reward = deck.sum() + np.bitwise_xor(deck[1:], deck[:-1]).sum() + (np.bitwise_xor(deck[-1], deck[0]) if len(deck) > 1 else 0)\n",
    "            reward /= max_card * sum(self.game_cfg.number_of_cards_to_add)\n",
    "            total_reward += reward\n",
    "\n",
    "            result.add(state=(problem, ex), dist=dist, action=action, reward=reward, value=value)\n",
    "\n",
    "            deck = list(deck)\n",
    "            reducible = list(reducible)\n",
    "\n",
    "            for _ in range(self.game_cfg.number_of_cards_to_add[step + 1]):\n",
    "                card = random.randrange(max_card)\n",
    "                index = random.randrange(len(deck) + 1) \n",
    "                deck.insert(index, card)\n",
    "                reducible.insert(index, True)\n",
    "\n",
    "            deck = np.array(deck)\n",
    "            reducible = np.array(reducible)\n",
    "\n",
    "            logger.info(\"Finished instance step %d\", step)\n",
    "\n",
    "        result.complete({\"total_reward\": total_reward})\n",
    "        return result\n",
    "\n",
    "    def run_episode(self, agent: Agent) -> EpisodeResult:\n",
    "        results = []\n",
    "        for _ in range(self.runs_per_episode):\n",
    "            results.append(self.run_instance(agent))\n",
    "\n",
    "        return EpisodeResult.merge_all(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "game_cfg = GameConfig()\n",
    "\n",
    "model = GraphModel(\n",
    "    input_dim=game_cfg.number_of_bits + 2,\n",
    "    edge_dim=0,\n",
    "    disambiguate_clauses_in_first=0,\n",
    "    hidden_dims=[32],\n",
    ")\n",
    "\n",
    "agent = Agent(model)\n",
    "\n",
    "env = ToyEnv(\n",
    "    game_cfg=game_cfg,\n",
    "    runs_per_episode=HP.runs_per_episode,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31799/4266688745.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(saved_checkpoints[-1])\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_ROOT = Path(\"alternative-checkpoints\")\n",
    "CHECKPOINT_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "saved_checkpoints = list(CHECKPOINT_ROOT.glob(\"*.pt\"))\n",
    "saved_checkpoints.sort(key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "if len(saved_checkpoints) > 0:\n",
    "    checkpoint = torch.load(saved_checkpoints[-1])\n",
    "    agent.model.load_state_dict(checkpoint[\"model\"])\n",
    "    agent.optim.load_state_dict(checkpoint[\"optim\"])\n",
    "    COUNTERS.from_dict(checkpoint[\"counters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31799/186580183.py:36: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  sample_weight = np.abs(np.array(advantages))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: 81.15791666666667\n",
      "Episode 736\n",
      "Rewards: 79.8608333333334\n",
      "Episode 737\n",
      "Rewards: 81.69020833333332\n",
      "Episode 738\n",
      "Rewards: 80.62229166666658\n",
      "Episode 739\n",
      "Rewards: 81.52062500000008\n",
      "Episode 740\n",
      "Rewards: 81.7918749999999\n",
      "Episode 741\n",
      "Rewards: 80.82020833333335\n",
      "Episode 742\n",
      "Rewards: 80.51187500000003\n",
      "Episode 743\n",
      "Rewards: 80.98791666666672\n",
      "Episode 744\n",
      "Rewards: 81.28145833333332\n",
      "Episode 745\n",
      "Rewards: 80.56645833333337\n",
      "Episode 746\n",
      "Rewards: 81.84645833333335\n",
      "Episode 747\n",
      "Rewards: 79.38437499999999\n",
      "Episode 748\n",
      "Rewards: 82.57562500000007\n",
      "Episode 749\n",
      "Rewards: 80.02541666666667\n",
      "Episode 750\n",
      "Rewards: 80.61145833333337\n",
      "Episode 751\n",
      "Rewards: 80.85395833333338\n",
      "Episode 752\n",
      "Rewards: 80.33770833333325\n",
      "Episode 753\n",
      "Rewards: 81.40666666666668\n",
      "Episode 754\n",
      "Rewards: 81.83666666666662\n",
      "Episode 755\n",
      "Rewards: 80.30770833333342\n",
      "Episode 756\n",
      "Rewards: 81.03562499999998\n",
      "Episode 757\n",
      "Rewards: 82.14645833333333\n",
      "Episode 758\n",
      "Rewards: 79.46937499999996\n",
      "Episode 759\n",
      "Rewards: 81.85249999999996\n",
      "Episode 760\n",
      "Rewards: 80.87520833333333\n",
      "Episode 761\n",
      "Rewards: 81.935625\n",
      "Episode 762\n",
      "Rewards: 80.508125\n",
      "Episode 763\n",
      "Rewards: 79.62250000000003\n",
      "Episode 764\n",
      "Rewards: 81.45249999999997\n",
      "Episode 765\n",
      "Rewards: 80.71562500000005\n",
      "Episode 766\n",
      "Rewards: 83.05708333333328\n",
      "Episode 767\n",
      "Rewards: 80.63041666666669\n",
      "Episode 768\n",
      "Rewards: 78.95895833333343\n",
      "Episode 769\n",
      "Rewards: 80.53416666666666\n",
      "Episode 770\n",
      "Rewards: 80.91437499999996\n",
      "Episode 771\n",
      "Rewards: 80.98374999999992\n",
      "Episode 772\n",
      "Rewards: 81.10604166666664\n",
      "Episode 773\n",
      "Rewards: 81.22333333333319\n",
      "Episode 774\n",
      "Rewards: 81.92854166666675\n",
      "Episode 775\n",
      "Rewards: 81.41916666666665\n",
      "Episode 776\n",
      "Rewards: 80.39874999999999\n",
      "Episode 777\n",
      "Rewards: 82.99624999999983\n",
      "Episode 778\n",
      "Rewards: 81.5210416666666\n",
      "Episode 779\n",
      "Rewards: 80.55812499999993\n",
      "Episode 780\n",
      "Rewards: 82.27312499999992\n",
      "Episode 781\n",
      "Rewards: 81.05562500000002\n",
      "Episode 782\n",
      "Rewards: 80.5672916666666\n",
      "Episode 783\n",
      "Rewards: 81.36041666666665\n",
      "Episode 784\n",
      "Rewards: 80.73562500000001\n",
      "Episode 785\n",
      "Rewards: 80.34291666666671\n",
      "Episode 786\n",
      "Rewards: 81.99708333333325\n",
      "Episode 787\n",
      "Rewards: 80.22979166666667\n",
      "Episode 788\n",
      "Rewards: 80.78958333333334\n",
      "Episode 789\n",
      "Rewards: 80.50875\n",
      "Episode 790\n",
      "Rewards: 81.9712499999999\n",
      "Episode 791\n",
      "Rewards: 80.5850000000001\n",
      "Episode 792\n",
      "Rewards: 80.91854166666664\n",
      "Episode 793\n",
      "Rewards: 81.9820833333334\n",
      "Episode 794\n",
      "Rewards: 81.15541666666655\n",
      "Episode 795\n",
      "Rewards: 80.40208333333344\n",
      "Episode 796\n",
      "Rewards: 79.36041666666671\n",
      "Episode 797\n",
      "Rewards: 81.08354166666676\n",
      "Episode 798\n",
      "Rewards: 80.4902083333334\n",
      "Episode 799\n",
      "Rewards: 80.24375000000002\n",
      "Episode 800\n",
      "Rewards: 80.57541666666668\n",
      "Episode 801\n",
      "Rewards: 82.29479166666673\n",
      "Episode 802\n",
      "Rewards: 80.84041666666654\n",
      "Episode 803\n",
      "Rewards: 82.24999999999999\n",
      "Episode 804\n",
      "Rewards: 81.99312500000008\n",
      "Episode 805\n",
      "Rewards: 82.07770833333332\n",
      "Episode 806\n",
      "Rewards: 81.17854166666662\n",
      "Episode 807\n",
      "Rewards: 80.81583333333332\n",
      "Episode 808\n",
      "Rewards: 82.53916666666673\n",
      "Episode 809\n",
      "Rewards: 81.36687499999995\n",
      "Episode 810\n",
      "Rewards: 82.27395833333331\n",
      "Episode 811\n",
      "Rewards: 82.7229166666667\n",
      "Episode 812\n",
      "Rewards: 79.47145833333335\n",
      "Episode 813\n",
      "Rewards: 80.89875000000005\n",
      "Episode 814\n",
      "Rewards: 79.87333333333335\n",
      "Episode 815\n",
      "Rewards: 81.71812500000003\n",
      "Episode 816\n",
      "Rewards: 83.05875000000006\n",
      "Episode 817\n",
      "Rewards: 82.96416666666676\n",
      "Episode 818\n",
      "Rewards: 80.4481249999999\n",
      "Episode 819\n",
      "Rewards: 80.83583333333331\n",
      "Episode 820\n",
      "Rewards: 80.61916666666667\n",
      "Episode 821\n",
      "Rewards: 80.15791666666658\n",
      "Episode 822\n",
      "Rewards: 80.82124999999996\n",
      "Episode 823\n",
      "Rewards: 82.71437500000003\n",
      "Episode 824\n",
      "Rewards: 81.29625000000003\n",
      "Episode 825\n",
      "Rewards: 80.84875000000008\n",
      "Episode 826\n",
      "Rewards: 81.54291666666661\n",
      "Episode 827\n",
      "Rewards: 80.2441666666666\n",
      "Episode 828\n",
      "Rewards: 80.99854166666665\n",
      "Episode 829\n",
      "Rewards: 80.6360416666666\n",
      "Episode 830\n",
      "Rewards: 81.18500000000002\n",
      "Episode 831\n",
      "Rewards: 82.53229166666664\n",
      "Episode 832\n",
      "Rewards: 83.15000000000015\n",
      "Episode 833\n",
      "Rewards: 81.53708333333329\n",
      "Episode 834\n",
      "Rewards: 81.6158333333334\n",
      "Episode 835\n",
      "Rewards: 81.60083333333338\n",
      "Episode 836\n",
      "Rewards: 80.32229166666664\n",
      "Episode 837\n",
      "Rewards: 81.51374999999994\n",
      "Episode 838\n",
      "Rewards: 80.40124999999985\n",
      "Episode 839\n",
      "Rewards: 80.75229166666655\n",
      "Episode 840\n",
      "Rewards: 80.74708333333335\n",
      "Episode 841\n",
      "Rewards: 81.36958333333334\n",
      "Episode 842\n",
      "Rewards: 80.60562499999997\n",
      "Episode 843\n",
      "Rewards: 81.38958333333336\n",
      "Episode 844\n",
      "Rewards: 81.17479166666664\n",
      "Episode 845\n",
      "Rewards: 81.28625000000007\n",
      "Episode 846\n",
      "Rewards: 82.37375000000003\n",
      "Episode 847\n",
      "Rewards: 81.30333333333336\n",
      "Episode 848\n",
      "Rewards: 80.30166666666663\n",
      "Episode 849\n",
      "Rewards: 81.49999999999993\n",
      "Episode 850\n",
      "Rewards: 80.33520833333331\n",
      "Episode 851\n",
      "Rewards: 80.51750000000008\n",
      "Episode 852\n",
      "Rewards: 80.67479166666669\n",
      "Episode 853\n",
      "Rewards: 80.22812499999998\n",
      "Episode 854\n",
      "Rewards: 80.45354166666674\n",
      "Episode 855\n",
      "Rewards: 81.45229166666668\n",
      "Episode 856\n",
      "Rewards: 82.0464583333333\n",
      "Episode 857\n",
      "Rewards: 81.49624999999993\n",
      "Episode 858\n",
      "Rewards: 80.21979166666672\n",
      "Episode 859\n",
      "Rewards: 80.65604166666661\n",
      "Episode 860\n",
      "Rewards: 80.51666666666672\n",
      "Episode 861\n",
      "Rewards: 80.43395833333334\n",
      "Episode 862\n",
      "Rewards: 80.41416666666666\n",
      "Episode 863\n",
      "Rewards: 79.81125000000004\n",
      "Episode 864\n",
      "Rewards: 83.38708333333328\n",
      "Episode 865\n",
      "Rewards: 81.65833333333335\n",
      "Episode 866\n",
      "Rewards: 79.91187499999998\n",
      "Episode 867\n",
      "Rewards: 81.14999999999998\n",
      "Episode 868\n",
      "Rewards: 81.33166666666664\n",
      "Episode 869\n",
      "Rewards: 80.77604166666659\n",
      "Episode 870\n",
      "Rewards: 81.15229166666668\n",
      "Episode 871\n",
      "Rewards: 81.32333333333331\n",
      "Episode 872\n",
      "Rewards: 81.78854166666662\n",
      "Episode 873\n",
      "Rewards: 81.70249999999996\n",
      "Episode 874\n",
      "Rewards: 81.52458333333333\n",
      "Episode 875\n",
      "Rewards: 81.07250000000005\n",
      "Episode 876\n",
      "Rewards: 81.23874999999995\n",
      "Episode 877\n",
      "Rewards: 81.48333333333345\n",
      "Episode 878\n",
      "Rewards: 80.59187500000003\n",
      "Episode 879\n",
      "Rewards: 80.48875000000002\n",
      "Episode 880\n",
      "Rewards: 81.69020833333333\n",
      "Episode 881\n",
      "Rewards: 82.23312500000003\n",
      "Episode 882\n",
      "Rewards: 82.08020833333332\n",
      "Episode 883\n",
      "Rewards: 80.95916666666666\n",
      "Episode 884\n",
      "Rewards: 81.47083333333335\n",
      "Episode 885\n",
      "Rewards: 80.87750000000004\n",
      "Episode 886\n",
      "Rewards: 82.01854166666666\n",
      "Episode 887\n",
      "Rewards: 80.50083333333339\n",
      "Episode 888\n",
      "Rewards: 81.76541666666674\n",
      "Episode 889\n",
      "Rewards: 81.77354166666665\n",
      "Episode 890\n",
      "Rewards: 80.10166666666679\n",
      "Episode 891\n",
      "Rewards: 80.33729166666674\n",
      "Episode 892\n",
      "Rewards: 80.9189583333333\n",
      "Episode 893\n",
      "Rewards: 80.94354166666663\n",
      "Episode 894\n",
      "Rewards: 81.60750000000003\n",
      "Episode 895\n",
      "Rewards: 82.153125\n",
      "Episode 896\n",
      "Rewards: 81.42770833333341\n",
      "Episode 897\n",
      "Rewards: 81.19333333333324\n",
      "Episode 898\n",
      "Rewards: 81.73708333333332\n",
      "Episode 899\n",
      "Rewards: 80.56062500000004\n",
      "Episode 900\n",
      "Rewards: 81.01958333333329\n",
      "Episode 901\n",
      "Rewards: 80.61187499999998\n",
      "Episode 902\n",
      "Rewards: 81.82124999999999\n",
      "Episode 903\n",
      "Rewards: 81.43479166666665\n",
      "Episode 904\n",
      "Rewards: 80.80291666666675\n",
      "Episode 905\n",
      "Rewards: 81.03458333333329\n",
      "Episode 906\n",
      "Rewards: 82.46625000000003\n",
      "Episode 907\n",
      "Rewards: 81.18583333333332\n",
      "Episode 908\n",
      "Rewards: 82.2370833333334\n",
      "Episode 909\n",
      "Rewards: 81.79166666666663\n",
      "Episode 910\n",
      "Rewards: 81.74812500000003\n",
      "Episode 911\n",
      "Rewards: 80.43604166666667\n",
      "Episode 912\n",
      "Rewards: 80.79854166666674\n",
      "Episode 913\n",
      "Rewards: 81.26520833333325\n",
      "Episode 914\n",
      "Rewards: 81.28479166666669\n",
      "Episode 915\n",
      "Rewards: 82.6539583333333\n",
      "Episode 916\n",
      "Rewards: 83.03333333333335\n",
      "Episode 917\n",
      "Rewards: 81.17479166666669\n",
      "Episode 918\n",
      "Rewards: 80.91291666666662\n",
      "Episode 919\n",
      "Rewards: 81.79541666666664\n",
      "Episode 920\n",
      "Rewards: 81.36000000000011\n",
      "Episode 921\n",
      "Rewards: 82.24749999999992\n",
      "Episode 922\n",
      "Rewards: 82.12291666666657\n",
      "Episode 923\n",
      "Rewards: 82.5210416666666\n",
      "Episode 924\n",
      "Rewards: 79.03937499999995\n",
      "Episode 925\n",
      "Rewards: 82.4191666666667\n",
      "Episode 926\n",
      "Rewards: 82.40249999999997\n",
      "Episode 927\n",
      "Rewards: 80.84708333333336\n",
      "Episode 928\n",
      "Rewards: 80.81833333333329\n",
      "Episode 929\n",
      "Rewards: 81.12875000000001\n",
      "Episode 930\n",
      "Rewards: 80.13479166666663\n",
      "Episode 931\n",
      "Rewards: 81.11937499999993\n",
      "Episode 932\n",
      "Rewards: 80.18895833333335\n",
      "Episode 933\n",
      "Rewards: 81.28541666666676\n",
      "Episode 934\n",
      "Rewards: 81.8604166666666\n",
      "Episode 935\n",
      "Rewards: 81.47062500000008\n",
      "Episode 936\n",
      "Rewards: 81.94999999999999\n",
      "Episode 937\n",
      "Rewards: 82.5422916666666\n",
      "Episode 938\n",
      "Rewards: 80.99562499999993\n",
      "Episode 939\n",
      "Rewards: 79.89270833333326\n",
      "Episode 940\n",
      "Rewards: 80.52020833333339\n",
      "Episode 941\n",
      "Rewards: 82.32229166666657\n",
      "Episode 942\n",
      "Rewards: 82.11833333333331\n",
      "Episode 943\n",
      "Rewards: 80.24312500000012\n",
      "Episode 944\n",
      "Rewards: 81.94416666666669\n",
      "Episode 945\n",
      "Rewards: 82.2972916666667\n",
      "Episode 946\n",
      "Rewards: 81.68562500000004\n",
      "Episode 947\n",
      "Rewards: 82.47104166666676\n",
      "Episode 948\n",
      "Rewards: 81.9625\n",
      "Episode 949\n",
      "Rewards: 81.36812499999994\n",
      "Episode 950\n",
      "Rewards: 80.9647916666666\n",
      "Episode 951\n",
      "Rewards: 81.55104166666663\n",
      "Episode 952\n",
      "Rewards: 81.54875000000006\n",
      "Episode 953\n",
      "Rewards: 81.24249999999999\n",
      "Episode 954\n",
      "Rewards: 80.505625\n",
      "Episode 955\n",
      "Rewards: 82.36145833333327\n",
      "Episode 956\n",
      "Rewards: 81.27395833333318\n",
      "Episode 957\n",
      "Rewards: 81.77333333333335\n",
      "Episode 958\n",
      "Rewards: 81.11750000000006\n",
      "Episode 959\n",
      "Rewards: 82.88208333333336\n",
      "Episode 960\n",
      "Rewards: 81.29458333333329\n",
      "Episode 961\n",
      "Rewards: 81.52354166666676\n",
      "Episode 962\n",
      "Rewards: 81.94354166666659\n",
      "Episode 963\n",
      "Rewards: 81.97000000000004\n",
      "Episode 964\n",
      "Rewards: 82.14729166666675\n",
      "Episode 965\n",
      "Rewards: 82.7654166666667\n",
      "Episode 966\n",
      "Rewards: 81.45062500000004\n",
      "Episode 967\n",
      "Rewards: 82.53416666666679\n",
      "Episode 968\n",
      "Rewards: 82.39729166666669\n",
      "Episode 969\n",
      "Rewards: 80.7222916666667\n",
      "Episode 970\n",
      "Rewards: 82.08604166666672\n",
      "Episode 971\n",
      "Rewards: 82.22729166666663\n",
      "Episode 972\n",
      "Rewards: 82.02958333333338\n",
      "Episode 973\n",
      "Rewards: 81.38520833333335\n",
      "Episode 974\n",
      "Rewards: 81.20645833333334\n",
      "Episode 975\n",
      "Rewards: 82.5760416666667\n",
      "Episode 976\n",
      "Rewards: 81.37583333333329\n",
      "Episode 977\n",
      "Rewards: 81.57854166666662\n",
      "Episode 978\n",
      "Rewards: 81.95770833333334\n",
      "Episode 979\n",
      "Rewards: 82.08833333333344\n",
      "Episode 980\n",
      "Rewards: 82.68729166666668\n",
      "Episode 981\n",
      "Rewards: 82.78791666666669\n",
      "Episode 982\n",
      "Rewards: 82.01374999999999\n",
      "Episode 983\n",
      "Rewards: 81.76979166666665\n",
      "Episode 984\n",
      "Rewards: 82.95729166666673\n",
      "Episode 985\n",
      "Rewards: 83.90375\n",
      "Episode 986\n",
      "Rewards: 80.83229166666662\n",
      "Episode 987\n",
      "Rewards: 82.39291666666666\n",
      "Episode 988\n",
      "Rewards: 81.42750000000008\n",
      "Episode 989\n",
      "Rewards: 81.63708333333331\n",
      "Episode 990\n",
      "Rewards: 80.8566666666667\n",
      "Episode 991\n",
      "Rewards: 82.01458333333335\n",
      "Episode 992\n",
      "Rewards: 83.16416666666667\n",
      "Episode 993\n",
      "Rewards: 82.17229166666678\n",
      "Episode 994\n",
      "Rewards: 82.11562500000002\n",
      "Episode 995\n",
      "Rewards: 81.24749999999995\n",
      "Episode 996\n",
      "Rewards: 80.92937500000004\n",
      "Episode 997\n",
      "Rewards: 80.98666666666671\n",
      "Episode 998\n",
      "Rewards: 83.67270833333338\n",
      "Episode 999\n",
      "Rewards: 80.90250000000007\n",
      "Episode 1000\n",
      "Rewards: 81.95875000000001\n",
      "Episode 1001\n",
      "Rewards: 82.21104166666674\n",
      "Episode 1002\n",
      "Rewards: 81.41916666666664\n",
      "Episode 1003\n",
      "Rewards: 81.81479166666664\n",
      "Episode 1004\n",
      "Rewards: 82.53166666666668\n",
      "Episode 1005\n",
      "Rewards: 82.74812500000003\n",
      "Episode 1006\n",
      "Rewards: 81.77125000000002\n",
      "Episode 1007\n",
      "Rewards: 82.42875000000006\n",
      "Episode 1008\n",
      "Rewards: 81.00916666666674\n",
      "Episode 1009\n",
      "Rewards: 82.00208333333339\n",
      "Episode 1010\n",
      "Rewards: 82.8639583333333\n",
      "Episode 1011\n",
      "Rewards: 83.12041666666676\n",
      "Episode 1012\n",
      "Rewards: 81.82791666666677\n",
      "Episode 1013\n",
      "Rewards: 81.83041666666674\n",
      "Episode 1014\n",
      "Rewards: 82.34395833333346\n",
      "Episode 1015\n",
      "Rewards: 81.34000000000003\n",
      "Episode 1016\n",
      "Rewards: 81.79145833333344\n",
      "Episode 1017\n",
      "Rewards: 83.45208333333326\n",
      "Episode 1018\n",
      "Rewards: 80.88999999999999\n",
      "Episode 1019\n",
      "Rewards: 81.49375000000003\n",
      "Episode 1020\n",
      "Rewards: 81.13416666666673\n",
      "Episode 1021\n",
      "Rewards: 81.20312499999994\n",
      "Episode 1022\n",
      "Rewards: 80.86499999999994\n",
      "Episode 1023\n",
      "Rewards: 81.71937499999989\n",
      "Episode 1024\n",
      "Rewards: 81.25833333333337\n",
      "Episode 1025\n",
      "Rewards: 82.11500000000004\n",
      "Episode 1026\n",
      "Rewards: 81.75000000000004\n",
      "Episode 1027\n",
      "Rewards: 81.923125\n",
      "Episode 1028\n",
      "Rewards: 82.35104166666669\n",
      "Episode 1029\n",
      "Rewards: 82.82187499999993\n",
      "Episode 1030\n",
      "Rewards: 81.38583333333328\n",
      "Episode 1031\n",
      "Rewards: 81.42229166666664\n",
      "Episode 1032\n",
      "Rewards: 82.74666666666671\n",
      "Episode 1033\n",
      "Rewards: 81.42729166666669\n",
      "Episode 1034\n",
      "Rewards: 82.82583333333334\n",
      "Episode 1035\n",
      "Rewards: 81.58729166666662\n",
      "Episode 1036\n",
      "Rewards: 80.68916666666668\n",
      "Episode 1037\n",
      "Rewards: 81.1341666666667\n",
      "Episode 1038\n",
      "Rewards: 81.38750000000005\n",
      "Episode 1039\n",
      "Rewards: 81.19500000000001\n",
      "Episode 1040\n",
      "Rewards: 82.90083333333324\n",
      "Episode 1041\n",
      "Rewards: 83.455625\n",
      "Episode 1042\n",
      "Rewards: 82.79375000000003\n",
      "Episode 1043\n",
      "Rewards: 81.42979166666663\n",
      "Episode 1044\n",
      "Rewards: 82.00708333333334\n",
      "Episode 1045\n",
      "Rewards: 81.46895833333336\n",
      "Episode 1046\n",
      "Rewards: 81.75708333333331\n",
      "Episode 1047\n",
      "Rewards: 82.61770833333337\n",
      "Episode 1048\n",
      "Rewards: 80.8052083333333\n",
      "Episode 1049\n",
      "Rewards: 82.55687499999999\n",
      "Episode 1050\n",
      "Rewards: 81.35958333333333\n",
      "Episode 1051\n",
      "Rewards: 82.82499999999989\n",
      "Episode 1052\n",
      "Rewards: 80.07375000000012\n",
      "Episode 1053\n",
      "Rewards: 80.7747916666667\n",
      "Episode 1054\n",
      "Rewards: 81.91500000000002\n",
      "Episode 1055\n",
      "Rewards: 82.45812500000002\n",
      "Episode 1056\n",
      "Rewards: 80.60187500000005\n",
      "Episode 1057\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrun_episode(agent)\n\u001b[1;32m      5\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished episode, starting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptim\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounters\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataclasses\u001b[38;5;241m.\u001b[39masdict(COUNTERS),\n\u001b[1;32m     12\u001b[0m }, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malternative-checkpoints/agent-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOUNTERS\u001b[38;5;241m.\u001b[39mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(results\u001b[38;5;241m.\u001b[39mrewards), COUNTERS\u001b[38;5;241m.\u001b[39mepisodes)\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m, in \u001b[0;36mAgent.update\u001b[0;34m(self, results, silent)\u001b[0m\n\u001b[1;32m     45\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     46\u001b[0m dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mBernoulli(probs\u001b[38;5;241m=\u001b[39mnew_probs)\n\u001b[0;32m---> 47\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m entropy \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mentropy()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     50\u001b[0m ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_probs \u001b[38;5;241m-\u001b[39m r\u001b[38;5;241m.\u001b[39mdists[i]\u001b[38;5;241m.\u001b[39mlog_prob(r\u001b[38;5;241m.\u001b[39mactions[i]\u001b[38;5;241m.\u001b[39mfloat()))\n",
      "File \u001b[0;32m~/projects/sat-rl-exploration/.venv/lib/python3.12/site-packages/torch/distributions/bernoulli.py:112\u001b[0m, in \u001b[0;36mBernoulli.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[0;32m--> 112\u001b[0m logits, value \u001b[38;5;241m=\u001b[39m broadcast_all(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m, value)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mbinary_cross_entropy_with_logits(logits, value, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/sat-rl-exploration/.venv/lib/python3.12/site-packages/torch/distributions/utils.py:145\u001b[0m, in \u001b[0;36mlazy_property.__get__\u001b[0;34m(self, instance, obj_type)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped \u001b[38;5;241m=\u001b[39m wrapped\n\u001b[1;32m    143\u001b[0m     update_wrapper(\u001b[38;5;28mself\u001b[39m, wrapped)  \u001b[38;5;66;03m# type:ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _lazy_property_and_property(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(f\"Episode {COUNTERS.episodes}\")\n",
    "    writer.add_scalar(\"episode\", COUNTERS.episodes, COUNTERS.episodes)\n",
    "    results = env.run_episode(agent)\n",
    "    logger.info(\"Finished episode, starting training\")\n",
    "    agent.update(results)\n",
    "\n",
    "    torch.save({\n",
    "        \"model\": agent.model.state_dict(),\n",
    "        \"optim\": agent.optim.state_dict(),\n",
    "        \"counters\": dataclasses.asdict(COUNTERS),\n",
    "    }, f\"alternative-checkpoints/agent-{COUNTERS.episodes}-{uuid.uuid4()}.pt\")\n",
    "\n",
    "    writer.add_scalar(\"episode_reward\", sum(results.rewards), COUNTERS.episodes)\n",
    "    print(f\"Rewards: {sum(results.rewards)}\")\n",
    "    COUNTERS.episodes += 1\n",
    "    del results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
