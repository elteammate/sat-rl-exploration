{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch_geometric.nn as gnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import pyvis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Protocol, cast\n",
    "import enum\n",
    "import dataclasses\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import uuid\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Hyperparameters:\n",
    "    batch_size: int = 64\n",
    "    runs_per_episode: int = 64\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 1e-5\n",
    "    eps_clip: float = 0.2\n",
    "    entropy_coef: float = 0.01\n",
    "    weight_decay: float = 1e-3\n",
    "    value_weight: float = 0.5\n",
    "    policy_weight: float = 1.0\n",
    "    gae_gamma: float = 0.95\n",
    "    gae_lambda: float = 0.8\n",
    "    penalty_per_conflict: float = 5e-5\n",
    "    temperature: float = 4.0\n",
    "\n",
    "\n",
    "HP = Hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"notebook\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "np.seterr(all='raise')\n",
    "DEV = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Counters:\n",
    "    episodes: int = 0 \n",
    "    epochs: int = 0\n",
    "    runs: int = 0\n",
    "    steps: int = 0\n",
    "    batches: int = 0\n",
    "    train_steps: int = 0\n",
    "\n",
    "    def from_dict(self, d):\n",
    "        for k, v in d.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "COUNTERS = Counters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedValueNormalizationLogits(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits: torch.Tensor, ex: torch.Tensor):\n",
    "        ex = torch.as_tensor(ex)\n",
    "\n",
    "        b = torch.zeros(logits.shape[:-1], device=logits.device)\n",
    "\n",
    "        for _ in range(100):\n",
    "            normalized = torch.sigmoid(logits + b.unsqueeze(-1))\n",
    "            f_gamma = normalized.sum(dim=-1) - ex\n",
    "            f_prime_gamma = (normalized * (1 - normalized)).sum(dim=-1)\n",
    "            diff = torch.clamp(f_gamma / f_prime_gamma, -2, 2)\n",
    "            if torch.all(diff.abs() < 1e-6):\n",
    "                break\n",
    "            b = b - diff\n",
    "\n",
    "        normalized = torch.sigmoid(logits + b.unsqueeze(-1))\n",
    "        ctx.save_for_backward(normalized)\n",
    "        return normalized\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        normalized, = ctx.saved_tensors\n",
    "        p_grad = normalized * (1 - normalized)\n",
    "        denom = p_grad.sum(dim=-1)\n",
    "        coordwise = p_grad * g\n",
    "\n",
    "        grad = coordwise - p_grad * coordwise.sum(axis=-1).unsqueeze(-1) / denom.unsqueeze(-1)\n",
    "\n",
    "        return grad, None\n",
    "\n",
    "\n",
    "probs = torch.tensor([\n",
    "    [0.999, 0.5, 0.5, 0.5, 0.1],\n",
    "    [0.3, 0.5, 0.5, 0.8, 0.2],\n",
    "], requires_grad=True)\n",
    "x = -(1 / probs - 1).log()\n",
    "y = ExpectedValueNormalizationLogits.apply(x, torch.tensor([2.0, 1.0]))\n",
    "# print(x, y, y.sum(axis=-1), sep=\"\\n\")\n",
    "y.sum().backward()\n",
    "# print(probs.grad)\n",
    "\n",
    "optim = torch.optim.SGD([probs], lr=0.1)\n",
    "for _ in range(100):\n",
    "    optim.zero_grad()\n",
    "    x = -(1 / probs - 1).log()\n",
    "    y = ExpectedValueNormalizationLogits.apply(x, torch.tensor([2.0, 1.0]))\n",
    "    loss = y.pow(3.0).sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    # print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphProblem(Protocol):\n",
    "    global_data: torch.Tensor\n",
    "    x: torch.Tensor\n",
    "    edge_index: torch.Tensor\n",
    "    edge_attr: torch.Tensor\n",
    "    reducible: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, edge_dim: int, disambiguate: bool):\n",
    "        super().__init__()\n",
    "\n",
    "        self.disambiguate = disambiguate\n",
    "\n",
    "        if disambiguate:\n",
    "            self.conv_reducible = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "            self.conv_irreducible = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "        else:\n",
    "            self.conv = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, h, data):\n",
    "        edge_index, edge_attr, reducible = data.edge_index, data.edge_attr, data.reducible\n",
    "\n",
    "        out = torch.zeros_like(h)\n",
    "\n",
    "        if self.disambiguate:\n",
    "            out[reducible, :] = self.conv_reducible(h, edge_index, edge_attr)[reducible, :]\n",
    "            out[~reducible, :] = self.conv_irreducible(h, edge_index, edge_attr)[~reducible, :]\n",
    "        else:\n",
    "            out = self.conv(h, edge_index, edge_attr)\n",
    "\n",
    "        out = self.activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: list[int],\n",
    "        disambiguate_clauses_in_first: int = 2,\n",
    "        edge_dim: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.input_fc = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "        self.processing_blocks = nn.ModuleList([\n",
    "            ProcessingBlock(\n",
    "                hidden_dims[i],\n",
    "                hidden_dims[i + 1],\n",
    "                edge_dim,\n",
    "                i < disambiguate_clauses_in_first\n",
    "            )\n",
    "            for i in range(len(hidden_dims) - 1)\n",
    "        ])\n",
    "\n",
    "        self.output_fc = nn.Linear(hidden_dims[-1], 1, bias=False)\n",
    "        self.value_fc_reducible = nn.Linear(hidden_dims[-1], 1, bias=False)\n",
    "        self.value_fc_irreducible = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, graph: GraphProblem, ex: torch.Tensor | float):\n",
    "        logger.info(\"Starting model forward pass\")\n",
    "\n",
    "        input_ = torch.cat([\n",
    "            graph.global_data.view((1, -1)).expand(graph.x.shape[0], -1),\n",
    "            graph.x,\n",
    "        ], dim=-1)\n",
    "\n",
    "        x = self.silu(self.input_fc(input_))\n",
    "\n",
    "        for block in self.processing_blocks:\n",
    "            x = block(x, graph)\n",
    "\n",
    "        logits = self.output_fc(x[graph.reducible]).view(-1)\n",
    "\n",
    "        probs = ExpectedValueNormalizationLogits.apply(logits * HP.temperature, ex)\n",
    "        if torch.isnan(probs).any():\n",
    "            print(logits)\n",
    "            print(probs)\n",
    "            raise AssertionError()\n",
    "\n",
    "        value_reducible = self.value_fc_reducible(x[graph.reducible]).view(-1)\n",
    "        value_irreducible = self.value_fc_irreducible(x[~graph.reducible]).view(-1)\n",
    "        value = torch.cat([value_reducible, value_irreducible]).sum()\n",
    "\n",
    "        logger.info(\"Finished model forward pass\")\n",
    "\n",
    "        return probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_advantages(rewards: list[float], values: list[float]) -> tuple[list[float], list[float]]:\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + HP.gae_gamma * (values[i + 1] if i + 1 < len(values) else 0) - values[i]\n",
    "        gae = delta + HP.gae_gamma * HP.gae_lambda * gae\n",
    "        returns.append(gae + values[i])\n",
    "        advantages.append(gae)\n",
    "    return returns[::-1], advantages[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class EpisodeResult:\n",
    "    states: list[GraphProblem]\n",
    "    dists: list[torch.distributions.Bernoulli]\n",
    "    actions: list[torch.Tensor]\n",
    "    rewards: list[float]\n",
    "    values: list[float]\n",
    "    returns: list[float]\n",
    "    advantages: list[float]\n",
    "\n",
    "    stats: list[dict]\n",
    "\n",
    "    @staticmethod\n",
    "    def empty() -> 'EpisodeResult':\n",
    "        return EpisodeResult([], [], [], [], [], [], [], [])\n",
    "\n",
    "    def merge_with(self, other: 'EpisodeResult') -> 'EpisodeResult':\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards) == len(self.returns) ==\n",
    "            len(self.advantages)\n",
    "        )\n",
    "        assert (\n",
    "            len(other.states) == len(other.dists) == len(other.actions) ==\n",
    "            len(other.values) == len(other.rewards) == len(other.returns) ==\n",
    "            len(other.advantages)\n",
    "        )\n",
    "        return EpisodeResult(\n",
    "            self.states + other.states,\n",
    "            self.dists + other.dists,\n",
    "            self.actions + other.actions,\n",
    "            self.rewards + other.rewards,\n",
    "            self.values + other.values,\n",
    "            self.returns + other.returns,\n",
    "            self.advantages + other.advantages,\n",
    "            self.stats + other.stats,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_all(results: list['EpisodeResult']) -> 'EpisodeResult':\n",
    "        result = results[0]\n",
    "        for other in results[1:]:\n",
    "            result = result.merge_with(other)\n",
    "        return result\n",
    "\n",
    "    def add(self, *, state, dist, action, reward, value):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards)\n",
    "        )\n",
    "        self.states.append(state)\n",
    "        self.dists.append(dist)\n",
    "        self.actions.append(action)\n",
    "        if reward is not None:\n",
    "            self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def add_reward(self, reward):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards) + 1\n",
    "        )\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def complete(self, stats: dict):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards)\n",
    "        )\n",
    "        assert len(self.stats) == 0\n",
    "        self.returns, self.advantages = compute_returns_advantages(self.rewards, self.values)\n",
    "        self.stats = [stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.to(DEV)\n",
    "        self.optim = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=HP.learning_rate,\n",
    "            weight_decay=HP.weight_decay\n",
    "        )\n",
    "\n",
    "    def act(self, graph: GraphProblem, ex: float):\n",
    "        COUNTERS.steps += 1\n",
    "        logger.info(\"Running agent act\")\n",
    "        with torch.no_grad():\n",
    "            graph = graph.to(DEV)\n",
    "            assert torch.isnan(graph.x).sum() == 0\n",
    "            assert torch.isnan(graph.edge_attr).sum() == 0\n",
    "            assert torch.isnan(graph.global_data).sum() == 0\n",
    "            probs, value = self.model(graph, ex)\n",
    "            logger.info(\"Finished agent act\")\n",
    "            return torch.distributions.Bernoulli(probs), value.item()\n",
    "\n",
    "    def update(self, results: EpisodeResult, silent: bool = False):\n",
    "        logger.info(\"Training agent\")\n",
    "        r = results\n",
    "\n",
    "        n = len(r.states)\n",
    "        batch_size = HP.batch_size\n",
    "        advantages = torch.tensor(r.advantages, dtype=torch.float32)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for num_epoch in range(HP.epochs):\n",
    "            COUNTERS.epochs += 1\n",
    "            logger.info(\"Training agent, epoch %d\", num_epoch)\n",
    "            for batch_start in range(0, n, batch_size):\n",
    "                COUNTERS.batches += 1\n",
    "                sample_weight = np.abs(np.array(advantages))\n",
    "                sample_weight = sample_weight / sample_weight.sum()\n",
    "                idx = np.random.choice(range(n), size=batch_size, replace=False, p=sample_weight)\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "                for i in idx:\n",
    "                    COUNTERS.train_steps += 1\n",
    "                    graph, ex = r.states[i]\n",
    "                    new_probs, value = self.model(graph, ex)\n",
    "                    value = value.cpu()\n",
    "                    dist = torch.distributions.Bernoulli(probs=new_probs)\n",
    "                    log_probs = dist.log_prob(r.actions[i].float())\n",
    "                    entropy = dist.entropy().mean()\n",
    "\n",
    "                    ratio = torch.exp(log_probs - r.dists[i].log_prob(r.actions[i].float()))\n",
    "                    surr1 = ratio * advantages[i]\n",
    "                    surr2 = torch.clamp(ratio, 1.0 - HP.eps_clip, 1.0 + HP.eps_clip) * advantages[i]\n",
    "\n",
    "                    value_loss = torch.nn.functional.huber_loss(value, torch.tensor(r.returns[i], dtype=torch.float32))\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                    if not silent:\n",
    "                        writer.add_scalar(\"value loss\", value_loss.item(), COUNTERS.train_steps)\n",
    "\n",
    "                    loss = (\n",
    "                        HP.policy_weight * policy_loss\n",
    "                        + HP.value_weight * value_loss\n",
    "                        - HP.entropy_coef * entropy\n",
    "                    ) / len(idx)\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class GameConfig:\n",
    "    number_of_bits: int = 4\n",
    "    number_of_cards_to_add: tuple[int] = (16, 8, 12, 16, 24, 32, 48, 48, 48, 48)\n",
    "    fraction_to_remove: float = 0.5\n",
    "    fraction_to_make_reducible: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEnv:\n",
    "    def __init__(self, game_cfg: GameConfig, runs_per_episode: int = 16):\n",
    "        self.runs_per_episode = runs_per_episode\n",
    "        self.game_cfg = game_cfg\n",
    "\n",
    "    def run_instance(self, agent: Agent):\n",
    "        COUNTERS.runs += 1\n",
    "        result = EpisodeResult.empty()\n",
    "\n",
    "        max_card = 2 ** self.game_cfg.number_of_bits\n",
    "        deck = np.random.randint(0, max_card, size=self.game_cfg.number_of_cards_to_add[0])\n",
    "        reducible = np.ones_like(deck, dtype=bool)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(len(self.game_cfg.number_of_cards_to_add) - 1):\n",
    "            COUNTERS.steps += 1\n",
    "            logger.info(\"Running instance step %d\", step)\n",
    "\n",
    "            problem = Data(\n",
    "                global_data=torch.tensor([\n",
    "                    len(deck) / sum(self.game_cfg.number_of_cards_to_add),\n",
    "                ], dtype=torch.float32),\n",
    "                x=torch.tensor(np.array([\n",
    "                    deck / (max_card - 1),\n",
    "                    *[np.bitwise_and(deck, 1 << i) >> i for i in range(self.game_cfg.number_of_bits)],\n",
    "                ]), dtype=torch.float32).permute(1, 0),\n",
    "                edge_index=torch.tensor(np.array([\n",
    "                    np.arange(len(deck)),\n",
    "                    np.arange(1, len(deck) + 1) % len(deck),\n",
    "                ]), dtype=torch.long),\n",
    "                edge_attr=torch.zeros((len(deck), 0), dtype=torch.float32),\n",
    "                reducible=torch.tensor(reducible, dtype=torch.bool),\n",
    "            )\n",
    "\n",
    "            ex = self.game_cfg.fraction_to_remove * sum(reducible)\n",
    "            dist, value = agent.act(problem, ex)\n",
    "            action = dist.sample()\n",
    "\n",
    "            indices = np.arange(len(deck))[reducible][action.cpu().numpy() == 1]\n",
    "            deck = np.delete(deck, indices)\n",
    "            reducible = np.delete(reducible, indices)\n",
    "\n",
    "            reducible = np.random.rand(*reducible.shape) < self.game_cfg.fraction_to_make_reducible\n",
    "\n",
    "            reward = deck.sum() + np.bitwise_xor(deck[1:], deck[:-1]).sum() + (np.bitwise_xor(deck[-1], deck[0]) if len(deck) > 1 else 0)\n",
    "            reward /= max_card * sum(self.game_cfg.number_of_cards_to_add)\n",
    "            total_reward += reward\n",
    "\n",
    "            result.add(state=(problem, ex), dist=dist, action=action, reward=reward, value=value)\n",
    "\n",
    "            deck = list(deck)\n",
    "            reducible = list(reducible)\n",
    "\n",
    "            for _ in range(self.game_cfg.number_of_cards_to_add[step + 1]):\n",
    "                card = random.randrange(max_card)\n",
    "                index = random.randrange(len(deck) + 1) \n",
    "                deck.insert(index, card)\n",
    "                reducible.insert(index, True)\n",
    "\n",
    "            deck = np.array(deck)\n",
    "            reducible = np.array(reducible)\n",
    "\n",
    "            logger.info(\"Finished instance step %d\", step)\n",
    "\n",
    "        result.complete({\"total_reward\": total_reward})\n",
    "        return result\n",
    "\n",
    "    def run_episode(self, agent: Agent) -> EpisodeResult:\n",
    "        results = []\n",
    "        for _ in range(self.runs_per_episode):\n",
    "            results.append(self.run_instance(agent))\n",
    "\n",
    "        return EpisodeResult.merge_all(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "game_cfg = GameConfig()\n",
    "\n",
    "model = GraphModel(\n",
    "    input_dim=game_cfg.number_of_bits + 2,\n",
    "    edge_dim=0,\n",
    "    disambiguate_clauses_in_first=1,\n",
    "    hidden_dims=[32, 32, 32]\n",
    ")\n",
    "\n",
    "agent = Agent(model)\n",
    "\n",
    "env = ToyEnv(\n",
    "    game_cfg=game_cfg,\n",
    "    runs_per_episode=HP.runs_per_episode,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64026/3484666577.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(saved_checkpoints[-1])\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_ROOT = Path(\"alternative-checkpoints-graph\")\n",
    "CHECKPOINT_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "saved_checkpoints = list(CHECKPOINT_ROOT.glob(\"*.pt\"))\n",
    "saved_checkpoints.sort(key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "if len(saved_checkpoints) > 0:\n",
    "    checkpoint = torch.load(saved_checkpoints[-1])\n",
    "    agent.model.load_state_dict(checkpoint[\"model\"])\n",
    "    agent.optim.load_state_dict(checkpoint[\"optim\"])\n",
    "    COUNTERS.from_dict(checkpoint[\"counters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64026/186580183.py:36: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  sample_weight = np.abs(np.array(advantages))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: 80.11520833333333\n",
      "Episode 215\n",
      "Rewards: 81.60729166666668\n",
      "Episode 216\n",
      "Rewards: 80.033125\n",
      "Episode 217\n",
      "Rewards: 78.89291666666665\n",
      "Episode 218\n",
      "Rewards: 80.76208333333332\n",
      "Episode 219\n",
      "Rewards: 80.41333333333341\n",
      "Episode 220\n",
      "Rewards: 79.44916666666674\n",
      "Episode 221\n",
      "Rewards: 80.04979166666666\n",
      "Episode 222\n",
      "Rewards: 80.3916666666667\n",
      "Episode 223\n",
      "Rewards: 81.16708333333334\n",
      "Episode 224\n",
      "Rewards: 79.56520833333332\n",
      "Episode 225\n",
      "Rewards: 79.75604166666662\n",
      "Episode 226\n",
      "Rewards: 79.1308333333333\n",
      "Episode 227\n",
      "Rewards: 79.854375\n",
      "Episode 228\n",
      "Rewards: 80.17749999999988\n",
      "Episode 229\n",
      "Rewards: 80.88645833333338\n",
      "Episode 230\n",
      "Rewards: 80.31083333333329\n",
      "Episode 231\n",
      "Rewards: 81.37354166666667\n",
      "Episode 232\n",
      "Rewards: 79.600625\n",
      "Episode 233\n",
      "Rewards: 80.10354166666666\n",
      "Episode 234\n",
      "Rewards: 81.03229166666672\n",
      "Episode 235\n",
      "Rewards: 80.85708333333332\n",
      "Episode 236\n",
      "Rewards: 80.22500000000008\n",
      "Episode 237\n",
      "Rewards: 80.3489583333333\n",
      "Episode 238\n",
      "Rewards: 79.89749999999998\n",
      "Episode 239\n",
      "Rewards: 80.40458333333345\n",
      "Episode 240\n",
      "Rewards: 80.66875\n",
      "Episode 241\n",
      "Rewards: 80.97937500000008\n",
      "Episode 242\n",
      "Rewards: 81.64187500000003\n",
      "Episode 243\n",
      "Rewards: 80.88020833333336\n",
      "Episode 244\n",
      "Rewards: 79.85374999999995\n",
      "Episode 245\n",
      "Rewards: 80.59374999999997\n",
      "Episode 246\n",
      "Rewards: 79.87312499999999\n",
      "Episode 247\n",
      "Rewards: 80.74458333333334\n",
      "Episode 248\n",
      "Rewards: 81.5285416666667\n",
      "Episode 249\n",
      "Rewards: 81.51145833333338\n",
      "Episode 250\n",
      "Rewards: 80.99354166666676\n",
      "Episode 251\n",
      "Rewards: 81.71687500000003\n",
      "Episode 252\n",
      "Rewards: 80.77937499999996\n",
      "Episode 253\n",
      "Rewards: 81.03458333333343\n",
      "Episode 254\n",
      "Rewards: 82.11416666666669\n",
      "Episode 255\n",
      "Rewards: 83.15395833333345\n",
      "Episode 256\n",
      "Rewards: 80.38583333333321\n",
      "Episode 257\n",
      "Rewards: 82.18333333333337\n",
      "Episode 258\n",
      "Rewards: 80.52875000000007\n",
      "Episode 259\n",
      "Rewards: 78.78645833333326\n",
      "Episode 260\n",
      "Rewards: 80.91208333333333\n",
      "Episode 261\n",
      "Rewards: 79.98354166666681\n",
      "Episode 262\n",
      "Rewards: 82.07708333333333\n",
      "Episode 263\n",
      "Rewards: 78.80625000000006\n",
      "Episode 264\n",
      "Rewards: 81.44937499999997\n",
      "Episode 265\n",
      "Rewards: 81.48958333333333\n",
      "Episode 266\n",
      "Rewards: 81.94020833333344\n",
      "Episode 267\n",
      "Rewards: 80.94479166666669\n",
      "Episode 268\n",
      "Rewards: 81.0085416666667\n",
      "Episode 269\n",
      "Rewards: 81.66333333333323\n",
      "Episode 270\n",
      "Rewards: 82.4699999999999\n",
      "Episode 271\n",
      "Rewards: 80.60437500000002\n",
      "Episode 272\n",
      "Rewards: 80.84145833333334\n",
      "Episode 273\n",
      "Rewards: 81.87270833333338\n",
      "Episode 274\n",
      "Rewards: 81.25937499999998\n",
      "Episode 275\n",
      "Rewards: 82.15520833333336\n",
      "Episode 276\n",
      "Rewards: 81.69020833333335\n",
      "Episode 277\n",
      "Rewards: 81.21124999999996\n",
      "Episode 278\n",
      "Rewards: 80.64104166666672\n",
      "Episode 279\n",
      "Rewards: 79.64208333333322\n",
      "Episode 280\n",
      "Rewards: 80.21666666666667\n",
      "Episode 281\n",
      "Rewards: 81.2847916666666\n",
      "Episode 282\n",
      "Rewards: 81.75437499999998\n",
      "Episode 283\n",
      "Rewards: 81.40333333333338\n",
      "Episode 284\n",
      "Rewards: 82.15812500000001\n",
      "Episode 285\n",
      "Rewards: 81.90854166666664\n",
      "Episode 286\n",
      "Rewards: 81.04750000000001\n",
      "Episode 287\n",
      "Rewards: 82.16750000000005\n",
      "Episode 288\n",
      "Rewards: 79.78041666666668\n",
      "Episode 289\n",
      "Rewards: 81.48124999999989\n",
      "Episode 290\n",
      "Rewards: 81.4389583333333\n",
      "Episode 291\n",
      "Rewards: 81.12729166666666\n",
      "Episode 292\n",
      "Rewards: 82.14437500000011\n",
      "Episode 293\n",
      "Rewards: 82.37270833333329\n",
      "Episode 294\n",
      "Rewards: 81.23375000000006\n",
      "Episode 295\n",
      "Rewards: 81.65625\n",
      "Episode 296\n",
      "Rewards: 80.96708333333345\n",
      "Episode 297\n",
      "Rewards: 82.64812500000002\n",
      "Episode 298\n",
      "Rewards: 80.90499999999997\n",
      "Episode 299\n",
      "Rewards: 81.29499999999999\n",
      "Episode 300\n",
      "Rewards: 81.88624999999999\n",
      "Episode 301\n",
      "Rewards: 82.30249999999995\n",
      "Episode 302\n",
      "Rewards: 82.11124999999998\n",
      "Episode 303\n",
      "Rewards: 81.760625\n",
      "Episode 304\n",
      "Rewards: 82.23562500000003\n",
      "Episode 305\n",
      "Rewards: 81.13874999999994\n",
      "Episode 306\n",
      "Rewards: 82.52020833333331\n",
      "Episode 307\n",
      "Rewards: 82.06458333333325\n",
      "Episode 308\n",
      "Rewards: 82.57791666666667\n",
      "Episode 309\n",
      "Rewards: 82.09229166666671\n",
      "Episode 310\n",
      "Rewards: 82.12874999999993\n",
      "Episode 311\n",
      "Rewards: 81.19666666666676\n",
      "Episode 312\n",
      "Rewards: 81.82645833333336\n",
      "Episode 313\n",
      "Rewards: 83.05645833333345\n",
      "Episode 314\n",
      "Rewards: 81.89604166666673\n",
      "Episode 315\n",
      "Rewards: 80.76104166666673\n",
      "Episode 316\n",
      "Rewards: 81.22833333333334\n",
      "Episode 317\n",
      "Rewards: 81.22979166666669\n",
      "Episode 318\n",
      "Rewards: 82.318125\n",
      "Episode 319\n",
      "Rewards: 82.03375000000007\n",
      "Episode 320\n",
      "Rewards: 81.8483333333333\n",
      "Episode 321\n",
      "Rewards: 82.82166666666667\n",
      "Episode 322\n",
      "Rewards: 82.8635416666666\n",
      "Episode 323\n",
      "Rewards: 83.1816666666667\n",
      "Episode 324\n",
      "Rewards: 82.44479166666672\n",
      "Episode 325\n",
      "Rewards: 82.91020833333334\n",
      "Episode 326\n",
      "Rewards: 82.36104166666662\n",
      "Episode 327\n",
      "Rewards: 82.50729166666675\n",
      "Episode 328\n",
      "Rewards: 82.32958333333336\n",
      "Episode 329\n",
      "Rewards: 82.07791666666662\n",
      "Episode 330\n",
      "Rewards: 80.74520833333325\n",
      "Episode 331\n",
      "Rewards: 82.91437500000002\n",
      "Episode 332\n",
      "Rewards: 81.26416666666667\n",
      "Episode 333\n",
      "Rewards: 81.27937500000003\n",
      "Episode 334\n",
      "Rewards: 83.2266666666666\n",
      "Episode 335\n",
      "Rewards: 81.90937499999997\n",
      "Episode 336\n",
      "Rewards: 81.28645833333323\n",
      "Episode 337\n",
      "Rewards: 82.44729166666659\n",
      "Episode 338\n",
      "Rewards: 82.24770833333343\n",
      "Episode 339\n",
      "Rewards: 81.12645833333323\n",
      "Episode 340\n",
      "Rewards: 82.29625000000004\n",
      "Episode 341\n",
      "Rewards: 82.48458333333326\n",
      "Episode 342\n",
      "Rewards: 83.07958333333339\n",
      "Episode 343\n",
      "Rewards: 82.50229166666676\n",
      "Episode 344\n",
      "Rewards: 82.30708333333338\n",
      "Episode 345\n",
      "Rewards: 82.66249999999991\n",
      "Episode 346\n",
      "Rewards: 83.10333333333345\n",
      "Episode 347\n",
      "Rewards: 82.22791666666664\n",
      "Episode 348\n",
      "Rewards: 83.40124999999993\n",
      "Episode 349\n",
      "Rewards: 82.68291666666666\n",
      "Episode 350\n",
      "Rewards: 82.57104166666669\n",
      "Episode 351\n",
      "Rewards: 81.56145833333332\n",
      "Episode 352\n",
      "Rewards: 82.01604166666665\n",
      "Episode 353\n",
      "Rewards: 81.76166666666667\n",
      "Episode 354\n",
      "Rewards: 81.47208333333333\n",
      "Episode 355\n",
      "Rewards: 82.02499999999985\n",
      "Episode 356\n",
      "Rewards: 82.48958333333329\n",
      "Episode 357\n",
      "Rewards: 82.84541666666664\n",
      "Episode 358\n",
      "Rewards: 82.25437499999994\n",
      "Episode 359\n",
      "Rewards: 82.47812500000008\n",
      "Episode 360\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrun_episode(agent)\n\u001b[1;32m      5\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished episode, starting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptim\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounters\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataclasses\u001b[38;5;241m.\u001b[39masdict(COUNTERS),\n\u001b[1;32m     12\u001b[0m }, CHECKPOINT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent-\u001b[39m\u001b[38;5;132;01m{COUNTERS.episodes}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124muuid.uuid4()}.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(results\u001b[38;5;241m.\u001b[39mrewards), COUNTERS\u001b[38;5;241m.\u001b[39mepisodes)\n",
      "Cell \u001b[0;32mIn[10], line 66\u001b[0m, in \u001b[0;36mAgent.update\u001b[0;34m(self, results, silent)\u001b[0m\n\u001b[1;32m     58\u001b[0m         writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_loss\u001b[38;5;241m.\u001b[39mitem(), COUNTERS\u001b[38;5;241m.\u001b[39mtrain_steps)\n\u001b[1;32m     60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m         HP\u001b[38;5;241m.\u001b[39mpolicy_weight \u001b[38;5;241m*\u001b[39m policy_loss\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;241m+\u001b[39m HP\u001b[38;5;241m.\u001b[39mvalue_weight \u001b[38;5;241m*\u001b[39m value_loss\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;241m-\u001b[39m HP\u001b[38;5;241m.\u001b[39mentropy_coef \u001b[38;5;241m*\u001b[39m entropy\n\u001b[1;32m     64\u001b[0m     ) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(idx)\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/projects/sat-rl-exploration/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/sat-rl-exploration/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/sat-rl-exploration/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(f\"Episode {COUNTERS.episodes}\")\n",
    "    writer.add_scalar(\"episode\", COUNTERS.episodes, COUNTERS.episodes)\n",
    "    results = env.run_episode(agent)\n",
    "    logger.info(\"Finished episode, starting training\")\n",
    "    agent.update(results)\n",
    "\n",
    "    torch.save({\n",
    "        \"model\": agent.model.state_dict(),\n",
    "        \"optim\": agent.optim.state_dict(),\n",
    "        \"counters\": dataclasses.asdict(COUNTERS),\n",
    "    }, CHECKPOINT_ROOT / \"agent-{COUNTERS.episodes}-{uuid.uuid4()}.pt\")\n",
    "\n",
    "    writer.add_scalar(\"episode_reward\", sum(results.rewards), COUNTERS.episodes)\n",
    "    print(f\"Rewards: {sum(results.rewards)}\")\n",
    "    COUNTERS.episodes += 1\n",
    "    del results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
