{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch_geometric.nn as gnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import pyvis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Protocol, cast\n",
    "import enum\n",
    "import dataclasses\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import uuid\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Hyperparameters:\n",
    "    batch_size: int = 64\n",
    "    runs_per_episode: int = 64\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 1e-5\n",
    "    eps_clip: float = 0.2\n",
    "    entropy_coef: float = 0.01\n",
    "    weight_decay: float = 1e-3\n",
    "    value_weight: float = 0.5\n",
    "    policy_weight: float = 1.0\n",
    "    gae_gamma: float = 0.95\n",
    "    gae_lambda: float = 0.8\n",
    "    penalty_per_conflict: float = 5e-5\n",
    "    temperature: float = 4.0\n",
    "\n",
    "\n",
    "HP = Hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"notebook\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "np.seterr(all='raise')\n",
    "DEV = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Counters:\n",
    "    episodes: int = 0 \n",
    "    epochs: int = 0\n",
    "    runs: int = 0\n",
    "    steps: int = 0\n",
    "    batches: int = 0\n",
    "    train_steps: int = 0\n",
    "\n",
    "    def from_dict(self, d):\n",
    "        for k, v in d.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "COUNTERS = Counters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedValueNormalizationLogits(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits: torch.Tensor, ex: torch.Tensor):\n",
    "        ex = torch.as_tensor(ex)\n",
    "\n",
    "        b = torch.zeros(logits.shape[:-1], device=logits.device)\n",
    "\n",
    "        for _ in range(100):\n",
    "            normalized = torch.sigmoid(logits + b.unsqueeze(-1))\n",
    "            f_gamma = normalized.sum(dim=-1) - ex\n",
    "            f_prime_gamma = (normalized * (1 - normalized)).sum(dim=-1)\n",
    "            diff = torch.clamp(f_gamma / f_prime_gamma, -2, 2)\n",
    "            if torch.all(diff.abs() < 1e-6):\n",
    "                break\n",
    "            b = b - diff\n",
    "\n",
    "        normalized = torch.sigmoid(logits + b.unsqueeze(-1))\n",
    "        ctx.save_for_backward(normalized)\n",
    "        return normalized\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        normalized, = ctx.saved_tensors\n",
    "        p_grad = normalized * (1 - normalized)\n",
    "        denom = p_grad.sum(dim=-1)\n",
    "        coordwise = p_grad * g\n",
    "\n",
    "        grad = coordwise - p_grad * coordwise.sum(axis=-1).unsqueeze(-1) / denom.unsqueeze(-1)\n",
    "\n",
    "        return grad, None\n",
    "\n",
    "\n",
    "probs = torch.tensor([\n",
    "    [0.999, 0.5, 0.5, 0.5, 0.1],\n",
    "    [0.3, 0.5, 0.5, 0.8, 0.2],\n",
    "], requires_grad=True)\n",
    "x = -(1 / probs - 1).log()\n",
    "y = ExpectedValueNormalizationLogits.apply(x, torch.tensor([2.0, 1.0]))\n",
    "# print(x, y, y.sum(axis=-1), sep=\"\\n\")\n",
    "y.sum().backward()\n",
    "# print(probs.grad)\n",
    "\n",
    "optim = torch.optim.SGD([probs], lr=0.1)\n",
    "for _ in range(100):\n",
    "    optim.zero_grad()\n",
    "    x = -(1 / probs - 1).log()\n",
    "    y = ExpectedValueNormalizationLogits.apply(x, torch.tensor([2.0, 1.0]))\n",
    "    loss = y.pow(3.0).sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    # print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphProblem(Protocol):\n",
    "    global_data: torch.Tensor\n",
    "    x: torch.Tensor\n",
    "    edge_index: torch.Tensor\n",
    "    edge_attr: torch.Tensor\n",
    "    reducible: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, edge_dim: int, disambiguate: bool):\n",
    "        super().__init__()\n",
    "\n",
    "        self.disambiguate = disambiguate\n",
    "\n",
    "        if disambiguate:\n",
    "            self.conv_reducible = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "            self.conv_irreducible = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "        else:\n",
    "            self.conv = gnn.GATConv(in_channels, out_channels, edge_dim=edge_dim, residual=True)\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, h, data):\n",
    "        edge_index, edge_attr, reducible = data.edge_index, data.edge_attr, data.reducible\n",
    "\n",
    "        out = torch.zeros_like(h)\n",
    "\n",
    "        if self.disambiguate:\n",
    "            out[reducible, :] = self.conv_reducible(h, edge_index, edge_attr)[reducible, :]\n",
    "            out[~reducible, :] = self.conv_irreducible(h, edge_index, edge_attr)[~reducible, :]\n",
    "        else:\n",
    "            out = self.conv(h, edge_index, edge_attr)\n",
    "\n",
    "        out = self.activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: list[int],\n",
    "        disambiguate_clauses_in_first: int = 2,\n",
    "        edge_dim: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.input_fc = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "        self.processing_blocks = nn.ModuleList([\n",
    "            ProcessingBlock(\n",
    "                hidden_dims[i],\n",
    "                hidden_dims[i + 1],\n",
    "                edge_dim,\n",
    "                i < disambiguate_clauses_in_first\n",
    "            )\n",
    "            for i in range(len(hidden_dims) - 1)\n",
    "        ])\n",
    "\n",
    "        self.output_fc = nn.Linear(hidden_dims[-1], 1, bias=False)\n",
    "        self.value_fc_reducible = nn.Linear(hidden_dims[-1], 1, bias=False)\n",
    "        self.value_fc_irreducible = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, graph: GraphProblem, ex: torch.Tensor | float):\n",
    "        logger.info(\"Starting model forward pass\")\n",
    "\n",
    "        input_ = torch.cat([\n",
    "            graph.global_data.view((1, -1)).expand(graph.x.shape[0], -1),\n",
    "            graph.x,\n",
    "        ], dim=-1)\n",
    "\n",
    "        x = self.silu(self.input_fc(input_))\n",
    "\n",
    "        for block in self.processing_blocks:\n",
    "            x = block(x, graph)\n",
    "\n",
    "        logits = self.output_fc(x[graph.reducible]).view(-1)\n",
    "\n",
    "        # probs = ExpectedValueNormalizationLogits.apply(logits * HP.temperature, ex)\n",
    "        probs = torch.sigmoid(logits * HP.temperature)\n",
    "        if torch.isnan(probs).any():\n",
    "            print(logits)\n",
    "            print(probs)\n",
    "            raise AssertionError()\n",
    "\n",
    "        value_reducible = self.value_fc_reducible(x[graph.reducible]).view(-1)\n",
    "        value_irreducible = self.value_fc_irreducible(x[~graph.reducible]).view(-1)\n",
    "        value = torch.cat([value_reducible, value_irreducible]).sum()\n",
    "\n",
    "        logger.info(\"Finished model forward pass\")\n",
    "\n",
    "        return probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_advantages(rewards: list[float], values: list[float]) -> tuple[list[float], list[float]]:\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + HP.gae_gamma * (values[i + 1] if i + 1 < len(values) else 0) - values[i]\n",
    "        gae = delta + HP.gae_gamma * HP.gae_lambda * gae\n",
    "        returns.append(gae + values[i])\n",
    "        advantages.append(gae)\n",
    "    return returns[::-1], advantages[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class EpisodeResult:\n",
    "    states: list[GraphProblem]\n",
    "    dists: list[torch.distributions.Bernoulli]\n",
    "    actions: list[torch.Tensor]\n",
    "    rewards: list[float]\n",
    "    values: list[float]\n",
    "    returns: list[float]\n",
    "    advantages: list[float]\n",
    "\n",
    "    stats: list[dict]\n",
    "\n",
    "    @staticmethod\n",
    "    def empty() -> 'EpisodeResult':\n",
    "        return EpisodeResult([], [], [], [], [], [], [], [])\n",
    "\n",
    "    def merge_with(self, other: 'EpisodeResult') -> 'EpisodeResult':\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards) == len(self.returns) ==\n",
    "            len(self.advantages)\n",
    "        )\n",
    "        assert (\n",
    "            len(other.states) == len(other.dists) == len(other.actions) ==\n",
    "            len(other.values) == len(other.rewards) == len(other.returns) ==\n",
    "            len(other.advantages)\n",
    "        )\n",
    "        return EpisodeResult(\n",
    "            self.states + other.states,\n",
    "            self.dists + other.dists,\n",
    "            self.actions + other.actions,\n",
    "            self.rewards + other.rewards,\n",
    "            self.values + other.values,\n",
    "            self.returns + other.returns,\n",
    "            self.advantages + other.advantages,\n",
    "            self.stats + other.stats,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_all(results: list['EpisodeResult']) -> 'EpisodeResult':\n",
    "        result = results[0]\n",
    "        for other in results[1:]:\n",
    "            result = result.merge_with(other)\n",
    "        return result\n",
    "\n",
    "    def add(self, *, state, dist, action, reward, value):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards)\n",
    "        )\n",
    "        self.states.append(state)\n",
    "        self.dists.append(dist)\n",
    "        self.actions.append(action)\n",
    "        if reward is not None:\n",
    "            self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def add_reward(self, reward):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards) + 1\n",
    "        )\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def complete(self, stats: dict):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards)\n",
    "        )\n",
    "        assert len(self.stats) == 0\n",
    "        self.returns, self.advantages = compute_returns_advantages(self.rewards, self.values)\n",
    "        self.stats = [stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.to(DEV)\n",
    "        self.optim = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=HP.learning_rate,\n",
    "            weight_decay=HP.weight_decay\n",
    "        )\n",
    "\n",
    "    def act(self, graph: GraphProblem, ex: float):\n",
    "        COUNTERS.steps += 1\n",
    "        logger.info(\"Running agent act\")\n",
    "        with torch.no_grad():\n",
    "            graph = graph.to(DEV)\n",
    "            assert torch.isnan(graph.x).sum() == 0\n",
    "            assert torch.isnan(graph.edge_attr).sum() == 0\n",
    "            assert torch.isnan(graph.global_data).sum() == 0\n",
    "            probs, value = self.model(graph, ex)\n",
    "            logger.info(\"Finished agent act\")\n",
    "            return torch.distributions.Bernoulli(probs), value.item()\n",
    "\n",
    "    def update(self, results: EpisodeResult, silent: bool = False):\n",
    "        logger.info(\"Training agent\")\n",
    "        r = results\n",
    "\n",
    "        n = len(r.states)\n",
    "        batch_size = HP.batch_size\n",
    "        advantages = torch.tensor(r.advantages, dtype=torch.float32)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        for num_epoch in range(HP.epochs):\n",
    "            COUNTERS.epochs += 1\n",
    "            logger.info(\"Training agent, epoch %d\", num_epoch)\n",
    "            # TODO: resamples based on advantages\n",
    "            indices = random.sample(range(n), n)\n",
    "            for batch_start in range(0, n, batch_size):\n",
    "                COUNTERS.batches += 1\n",
    "                idx = indices[batch_start:batch_start + batch_size]\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "                for i in idx:\n",
    "                    COUNTERS.train_steps += 1\n",
    "                    graph, ex = r.states[i]\n",
    "                    new_probs, value = self.model(graph, ex)\n",
    "                    value = value.cpu()\n",
    "                    dist = torch.distributions.Bernoulli(probs=new_probs)\n",
    "                    log_probs = dist.log_prob(r.actions[i].float())\n",
    "                    entropy = dist.entropy().mean()\n",
    "\n",
    "                    ratio = torch.exp(log_probs - r.dists[i].log_prob(r.actions[i].float()))\n",
    "                    surr1 = ratio * advantages[i]\n",
    "                    surr2 = torch.clamp(ratio, 1.0 - HP.eps_clip, 1.0 + HP.eps_clip) * advantages[i]\n",
    "\n",
    "                    value_loss = torch.nn.functional.huber_loss(value, torch.tensor(r.returns[i], dtype=torch.float32))\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                    if not silent:\n",
    "                        writer.add_scalar(\"value loss\", value_loss.item(), COUNTERS.train_steps)\n",
    "\n",
    "                    loss = (\n",
    "                        HP.policy_weight * policy_loss\n",
    "                        + HP.value_weight * value_loss\n",
    "                        - HP.entropy_coef * entropy\n",
    "                    ) / len(idx)\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class GameConfig:\n",
    "    number_of_bits: int = 4\n",
    "    number_of_cards_to_add: tuple[int] = (16, 8, 12, 16, 24, 32, 48, 48, 48, 48)\n",
    "    fraction_to_remove: float = 0.5\n",
    "    fraction_to_make_reducible: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEnv:\n",
    "    def __init__(self, game_cfg: GameConfig, runs_per_episode: int = 16):\n",
    "        self.runs_per_episode = runs_per_episode\n",
    "        self.game_cfg = game_cfg\n",
    "\n",
    "    def run_instance(self, agent: Agent):\n",
    "        COUNTERS.runs += 1\n",
    "        result = EpisodeResult.empty()\n",
    "\n",
    "        max_card = 2 ** self.game_cfg.number_of_bits\n",
    "        deck = np.random.randint(0, max_card, size=self.game_cfg.number_of_cards_to_add[0])\n",
    "        reducible = np.ones_like(deck, dtype=bool)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(len(self.game_cfg.number_of_cards_to_add) - 1):\n",
    "            COUNTERS.steps += 1\n",
    "            logger.info(\"Running instance step %d\", step)\n",
    "\n",
    "            problem = Data(\n",
    "                global_data=torch.tensor([\n",
    "                    len(deck) / sum(self.game_cfg.number_of_cards_to_add),\n",
    "                ], dtype=torch.float32),\n",
    "                x=torch.tensor(np.array([\n",
    "                    deck / (max_card - 1),\n",
    "                    *[np.bitwise_and(deck, 1 << i) >> i for i in range(self.game_cfg.number_of_bits)],\n",
    "                ]), dtype=torch.float32).permute(1, 0),\n",
    "                edge_index=torch.tensor(np.array([\n",
    "                    np.arange(len(deck)),\n",
    "                    np.arange(1, len(deck) + 1) % len(deck),\n",
    "                ]), dtype=torch.long),\n",
    "                edge_attr=torch.zeros((len(deck), 0), dtype=torch.float32),\n",
    "                reducible=torch.tensor(reducible, dtype=torch.bool),\n",
    "            )\n",
    "\n",
    "            ex = self.game_cfg.fraction_to_remove * sum(reducible)\n",
    "            dist, value = agent.act(problem, ex)\n",
    "            action = dist.sample()\n",
    "\n",
    "            indices = np.arange(len(deck))[reducible][action.cpu().numpy() == 1]\n",
    "            deck = np.delete(deck, indices)\n",
    "            reducible = np.delete(reducible, indices)\n",
    "\n",
    "            reducible = np.random.rand(*reducible.shape) < self.game_cfg.fraction_to_make_reducible\n",
    "\n",
    "            reward = deck.sum() + np.bitwise_xor(deck[1:], deck[:-1]).sum() + (np.bitwise_xor(deck[-1], deck[0]) if len(deck) > 0 else 0)\n",
    "            reward /= max_card * sum(self.game_cfg.number_of_cards_to_add)\n",
    "            total_reward += reward\n",
    "\n",
    "            result.add(state=(problem, ex), dist=dist, action=action, reward=reward, value=value)\n",
    "\n",
    "            deck = list(deck)\n",
    "            reducible = list(reducible)\n",
    "\n",
    "            for _ in range(self.game_cfg.number_of_cards_to_add[step + 1]):\n",
    "                card = random.randrange(max_card)\n",
    "                index = random.randrange(len(deck) + 1) \n",
    "                deck.insert(index, card)\n",
    "                reducible.insert(index, True)\n",
    "\n",
    "            deck = np.array(deck)\n",
    "            reducible = np.array(reducible)\n",
    "\n",
    "            logger.info(\"Finished instance step %d\", step)\n",
    "\n",
    "        result.complete({\"total_reward\": total_reward})\n",
    "        return result\n",
    "\n",
    "    def run_episode(self, agent: Agent) -> EpisodeResult:\n",
    "        results = []\n",
    "        for _ in range(self.runs_per_episode):\n",
    "            results.append(self.run_instance(agent))\n",
    "\n",
    "        return EpisodeResult.merge_all(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "game_cfg = GameConfig()\n",
    "\n",
    "model = GraphModel(\n",
    "    input_dim=game_cfg.number_of_bits + 2,\n",
    "    edge_dim=0,\n",
    "    disambiguate_clauses_in_first=0,\n",
    "    hidden_dims=[32],\n",
    ")\n",
    "\n",
    "agent = Agent(model)\n",
    "\n",
    "env = ToyEnv(\n",
    "    game_cfg=game_cfg,\n",
    "    runs_per_episode=HP.runs_per_episode,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27849/1311558052.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(saved_checkpoints[-1])\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_ROOT = Path(\"alternative-checkpoints-no-norm\")\n",
    "CHECKPOINT_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "saved_checkpoints = list(CHECKPOINT_ROOT.glob(\"*.pt\"))\n",
    "saved_checkpoints.sort(key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "if len(saved_checkpoints) > 0:\n",
    "    checkpoint = torch.load(saved_checkpoints[-1])\n",
    "    agent.model.load_state_dict(checkpoint[\"model\"])\n",
    "    agent.optim.load_state_dict(checkpoint[\"optim\"])\n",
    "    COUNTERS.from_dict(checkpoint[\"counters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrun_episode(agent)\n\u001b[1;32m      5\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished episode, starting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptim\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounters\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataclasses\u001b[38;5;241m.\u001b[39masdict(COUNTERS),\n\u001b[1;32m     12\u001b[0m }, CHECKPOINT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent-\u001b[39m\u001b[38;5;132;01m{COUNTERS.episodes}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124muuid.uuid4()}.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(results\u001b[38;5;241m.\u001b[39mrewards), COUNTERS\u001b[38;5;241m.\u001b[39mepisodes)\n",
      "Cell \u001b[0;32mIn[10], line 66\u001b[0m, in \u001b[0;36mAgent.update\u001b[0;34m(self, results, silent)\u001b[0m\n\u001b[1;32m     58\u001b[0m         writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, value_loss\u001b[38;5;241m.\u001b[39mitem(), COUNTERS\u001b[38;5;241m.\u001b[39mtrain_steps)\n\u001b[1;32m     60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m         HP\u001b[38;5;241m.\u001b[39mpolicy_weight \u001b[38;5;241m*\u001b[39m policy_loss\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;241m+\u001b[39m HP\u001b[38;5;241m.\u001b[39mvalue_weight \u001b[38;5;241m*\u001b[39m value_loss\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;241m-\u001b[39m HP\u001b[38;5;241m.\u001b[39mentropy_coef \u001b[38;5;241m*\u001b[39m entropy\n\u001b[1;32m     64\u001b[0m     ) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(idx)\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/projects/sat-rl-exploration/.venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    527\u001b[0m ):\n\u001b[1;32m    528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m            used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(f\"Episode {COUNTERS.episodes}\")\n",
    "    writer.add_scalar(\"episode\", COUNTERS.episodes, COUNTERS.episodes)\n",
    "    results = env.run_episode(agent)\n",
    "    logger.info(\"Finished episode, starting training\")\n",
    "    agent.update(results)\n",
    "\n",
    "    torch.save({\n",
    "        \"model\": agent.model.state_dict(),\n",
    "        \"optim\": agent.optim.state_dict(),\n",
    "        \"counters\": dataclasses.asdict(COUNTERS),\n",
    "    }, CHECKPOINT_ROOT / \"agent-{COUNTERS.episodes}-{uuid.uuid4()}.pt\")\n",
    "\n",
    "    writer.add_scalar(\"episode_reward\", sum(results.rewards), COUNTERS.episodes)\n",
    "    print(f\"Rewards: {sum(results.rewards)}\")\n",
    "    COUNTERS.episodes += 1\n",
    "    del results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
