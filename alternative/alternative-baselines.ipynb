{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch_geometric.nn as gnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import pyvis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Protocol, cast\n",
    "import enum\n",
    "import dataclasses\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import uuid\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Hyperparameters:\n",
    "    batch_size: int = 64\n",
    "    runs_per_episode: int = 64\n",
    "    epochs: int = 10\n",
    "    learning_rate: float = 1e-5\n",
    "    eps_clip: float = 0.2\n",
    "    entropy_coef: float = 0.01\n",
    "    weight_decay: float = 1e-3\n",
    "    value_weight: float = 0.5\n",
    "    policy_weight: float = 1.0\n",
    "    gae_gamma: float = 0.95\n",
    "    gae_lambda: float = 0.8\n",
    "    penalty_per_conflict: float = 5e-5\n",
    "    temperature: float = 4.0\n",
    "\n",
    "\n",
    "HP = Hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"notebook\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "np.seterr(all='raise')\n",
    "DEV = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Counters:\n",
    "    episodes: int = 0 \n",
    "    epochs: int = 0\n",
    "    runs: int = 0\n",
    "    steps: int = 0\n",
    "    batches: int = 0\n",
    "    train_steps: int = 0\n",
    "\n",
    "    def from_dict(self, d):\n",
    "        for k, v in d.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "COUNTERS = Counters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedValueNormalizationLogits(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits: torch.Tensor, ex: torch.Tensor):\n",
    "        ex = torch.as_tensor(ex)\n",
    "\n",
    "        b = torch.zeros(logits.shape[:-1], device=logits.device)\n",
    "\n",
    "        for _ in range(100):\n",
    "            normalized = torch.sigmoid(logits + b.unsqueeze(-1))\n",
    "            f_gamma = normalized.sum(dim=-1) - ex\n",
    "            f_prime_gamma = (normalized * (1 - normalized)).sum(dim=-1)\n",
    "            diff = torch.clamp(f_gamma / f_prime_gamma, -2, 2)\n",
    "            if torch.all(diff.abs() < 1e-6):\n",
    "                break\n",
    "            b = b - diff\n",
    "\n",
    "        normalized = torch.sigmoid(logits + b.unsqueeze(-1))\n",
    "        ctx.save_for_backward(normalized)\n",
    "        return normalized\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        normalized, = ctx.saved_tensors\n",
    "        p_grad = normalized * (1 - normalized)\n",
    "        denom = p_grad.sum(dim=-1)\n",
    "        coordwise = p_grad * g\n",
    "\n",
    "        grad = coordwise - p_grad * coordwise.sum(axis=-1).unsqueeze(-1) / denom.unsqueeze(-1)\n",
    "\n",
    "        return grad, None\n",
    "\n",
    "\n",
    "probs = torch.tensor([\n",
    "    [0.999, 0.5, 0.5, 0.5, 0.1],\n",
    "    [0.3, 0.5, 0.5, 0.8, 0.2],\n",
    "], requires_grad=True)\n",
    "x = -(1 / probs - 1).log()\n",
    "y = ExpectedValueNormalizationLogits.apply(x, torch.tensor([2.0, 1.0]))\n",
    "# print(x, y, y.sum(axis=-1), sep=\"\\n\")\n",
    "y.sum().backward()\n",
    "# print(probs.grad)\n",
    "\n",
    "optim = torch.optim.SGD([probs], lr=0.1)\n",
    "for _ in range(100):\n",
    "    optim.zero_grad()\n",
    "    x = -(1 / probs - 1).log()\n",
    "    y = ExpectedValueNormalizationLogits.apply(x, torch.tensor([2.0, 1.0]))\n",
    "    loss = y.pow(3.0).sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    # print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphProblem(Protocol):\n",
    "    global_data: torch.Tensor\n",
    "    x: torch.Tensor\n",
    "    edge_index: torch.Tensor\n",
    "    edge_attr: torch.Tensor\n",
    "    reducible: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_advantages(rewards: list[float], values: list[float]) -> tuple[list[float], list[float]]:\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + HP.gae_gamma * (values[i + 1] if i + 1 < len(values) else 0) - values[i]\n",
    "        gae = delta + HP.gae_gamma * HP.gae_lambda * gae\n",
    "        returns.append(gae + values[i])\n",
    "        advantages.append(gae)\n",
    "    return returns[::-1], advantages[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class EpisodeResult:\n",
    "    states: list[GraphProblem]\n",
    "    dists: list[torch.distributions.Bernoulli]\n",
    "    actions: list[torch.Tensor]\n",
    "    rewards: list[float]\n",
    "    values: list[float]\n",
    "    returns: list[float]\n",
    "    advantages: list[float]\n",
    "\n",
    "    stats: list[dict]\n",
    "\n",
    "    @staticmethod\n",
    "    def empty() -> 'EpisodeResult':\n",
    "        return EpisodeResult([], [], [], [], [], [], [], [])\n",
    "\n",
    "    def merge_with(self, other: 'EpisodeResult') -> 'EpisodeResult':\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards) == len(self.returns) ==\n",
    "            len(self.advantages)\n",
    "        )\n",
    "        assert (\n",
    "            len(other.states) == len(other.dists) == len(other.actions) ==\n",
    "            len(other.values) == len(other.rewards) == len(other.returns) ==\n",
    "            len(other.advantages)\n",
    "        )\n",
    "        return EpisodeResult(\n",
    "            self.states + other.states,\n",
    "            self.dists + other.dists,\n",
    "            self.actions + other.actions,\n",
    "            self.rewards + other.rewards,\n",
    "            self.values + other.values,\n",
    "            self.returns + other.returns,\n",
    "            self.advantages + other.advantages,\n",
    "            self.stats + other.stats,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_all(results: list['EpisodeResult']) -> 'EpisodeResult':\n",
    "        result = results[0]\n",
    "        for other in results[1:]:\n",
    "            result = result.merge_with(other)\n",
    "        return result\n",
    "\n",
    "    def add(self, *, state, dist, action, reward, value):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards)\n",
    "        )\n",
    "        self.states.append(state)\n",
    "        self.dists.append(dist)\n",
    "        self.actions.append(action)\n",
    "        if reward is not None:\n",
    "            self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def add_reward(self, reward):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards) + 1\n",
    "        )\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def complete(self, stats: dict):\n",
    "        assert (\n",
    "            len(self.states) == len(self.dists) == len(self.actions) ==\n",
    "            len(self.values) == len(self.rewards)\n",
    "        )\n",
    "        assert len(self.stats) == 0\n",
    "        self.returns, self.advantages = compute_returns_advantages(self.rewards, self.values)\n",
    "        self.stats = [stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, strategy: str):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def act(self, graph: GraphProblem, ex: float):\n",
    "        values = graph.x[:, 0]\n",
    "        reducible = graph.reducible\n",
    "        reducible_values = values[reducible] + torch.randn_like(values[reducible]) * 1e-3\n",
    "        n = len(values)\n",
    "        m = len(reducible_values)\n",
    "        if self.strategy == \"uniform\":\n",
    "            return torch.distributions.Bernoulli(torch.full((m,), ex / m)), 0.0\n",
    "        elif self.strategy == \"max\":\n",
    "            return torch.distributions.Bernoulli((reducible_values > torch.quantile(reducible_values, ex / n)).float()), 0.0\n",
    "        elif self.strategy == \"min\":\n",
    "            return torch.distributions.Bernoulli((reducible_values < torch.quantile(reducible_values, 1 - ex / n)).float()), 0.0\n",
    "        elif self.strategy == \"none\":\n",
    "            return torch.distributions.Bernoulli(torch.zeros((m,))), 0.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy {self.strategy}\")\n",
    "\n",
    "    def update(self, results: EpisodeResult, silent: bool = False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class GameConfig:\n",
    "    number_of_bits: int = 4\n",
    "    number_of_cards_to_add: tuple[int] = (16, 8, 12, 16, 24, 32, 48, 48, 48, 48)\n",
    "    fraction_to_remove: float = 0.5\n",
    "    fraction_to_make_reducible: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEnv:\n",
    "    def __init__(self, game_cfg: GameConfig, runs_per_episode: int = 16):\n",
    "        self.runs_per_episode = runs_per_episode\n",
    "        self.game_cfg = game_cfg\n",
    "\n",
    "    def run_instance(self, agent: Agent):\n",
    "        COUNTERS.runs += 1\n",
    "        result = EpisodeResult.empty()\n",
    "\n",
    "        max_card = 2 ** self.game_cfg.number_of_bits\n",
    "        deck = np.random.randint(0, max_card, size=self.game_cfg.number_of_cards_to_add[0])\n",
    "        reducible = np.ones_like(deck, dtype=bool)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(len(self.game_cfg.number_of_cards_to_add) - 1):\n",
    "            COUNTERS.steps += 1\n",
    "            logger.info(\"Running instance step %d\", step)\n",
    "\n",
    "            problem = Data(\n",
    "                global_data=torch.tensor([\n",
    "                    len(deck) / sum(self.game_cfg.number_of_cards_to_add),\n",
    "                ], dtype=torch.float32),\n",
    "                x=torch.tensor(np.array([\n",
    "                    deck / (max_card - 1),\n",
    "                    *[np.bitwise_and(deck, 1 << i) >> i for i in range(self.game_cfg.number_of_bits)],\n",
    "                ]), dtype=torch.float32).permute(1, 0),\n",
    "                edge_index=torch.tensor(np.array([\n",
    "                    np.arange(len(deck)),\n",
    "                    np.arange(1, len(deck) + 1) % len(deck),\n",
    "                ]), dtype=torch.long),\n",
    "                edge_attr=torch.zeros((len(deck), 0), dtype=torch.float32),\n",
    "                reducible=torch.tensor(reducible, dtype=torch.bool),\n",
    "            )\n",
    "\n",
    "            ex = self.game_cfg.fraction_to_remove * sum(reducible)\n",
    "            dist, value = agent.act(problem, ex)\n",
    "            action = dist.sample()\n",
    "\n",
    "            indices = np.arange(len(deck))[reducible][action.cpu().numpy() == 1]\n",
    "            deck = np.delete(deck, indices)\n",
    "            reducible = np.delete(reducible, indices)\n",
    "\n",
    "            reducible = np.random.rand(*reducible.shape) < self.game_cfg.fraction_to_make_reducible\n",
    "\n",
    "            reward = deck.sum() + np.bitwise_xor(deck[1:], deck[:-1]).sum() + np.bitwise_xor(deck[-1], deck[0])\n",
    "            reward /= max_card * sum(self.game_cfg.number_of_cards_to_add)\n",
    "            total_reward += reward\n",
    "\n",
    "            result.add(state=(problem, ex), dist=dist, action=action, reward=reward, value=value)\n",
    "\n",
    "            deck = list(deck)\n",
    "            reducible = list(reducible)\n",
    "\n",
    "            for _ in range(self.game_cfg.number_of_cards_to_add[step + 1]):\n",
    "                card = random.randrange(max_card)\n",
    "                index = random.randrange(len(deck) + 1) \n",
    "                deck.insert(index, card)\n",
    "                reducible.insert(index, True)\n",
    "\n",
    "            deck = np.array(deck)\n",
    "            reducible = np.array(reducible)\n",
    "\n",
    "            logger.info(\"Finished instance step %d\", step)\n",
    "\n",
    "        result.complete({\"total_reward\": total_reward})\n",
    "        return result\n",
    "\n",
    "    def run_episode(self, agent: Agent) -> EpisodeResult:\n",
    "        results = []\n",
    "        for _ in range(self.runs_per_episode):\n",
    "            results.append(self.run_instance(agent))\n",
    "\n",
    "        return EpisodeResult.merge_all(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 247\n",
      "Rewards: 76.5054166666666\n",
      "Episode 248\n",
      "Rewards: 75.53895833333331\n",
      "Episode 249\n",
      "Rewards: 75.85958333333345\n",
      "Episode 250\n",
      "Rewards: 75.59041666666666\n",
      "Episode 251\n",
      "Rewards: 74.77791666666674\n",
      "Episode 252\n",
      "Rewards: 75.49187500000004\n",
      "Episode 253\n",
      "Rewards: 75.82125\n",
      "Episode 254\n",
      "Rewards: 74.77145833333331\n",
      "Episode 255\n",
      "Rewards: 75.58125000000004\n",
      "Episode 256\n",
      "Rewards: 74.77395833333331\n",
      "Episode 257\n",
      "Rewards: 77.010625\n",
      "Episode 258\n",
      "Rewards: 75.09770833333339\n",
      "Episode 259\n",
      "Rewards: 77.62312499999992\n",
      "Episode 260\n",
      "Rewards: 76.50333333333329\n",
      "Episode 261\n",
      "Rewards: 76.58354166666676\n",
      "Episode 262\n",
      "Rewards: 76.91020833333327\n",
      "Episode 263\n",
      "Rewards: 74.75062500000006\n",
      "Episode 264\n",
      "Rewards: 76.25249999999987\n",
      "Episode 265\n",
      "Rewards: 75.76166666666654\n",
      "Episode 266\n",
      "Rewards: 75.26458333333336\n",
      "Episode 267\n",
      "Rewards: 74.69291666666668\n",
      "Episode 268\n",
      "Rewards: 74.3166666666666\n",
      "Episode 269\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOUNTERS\u001b[38;5;241m.\u001b[39mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished episode, starting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate(results)\n",
      "Cell \u001b[0;32mIn[11], line 71\u001b[0m, in \u001b[0;36mToyEnv.run_episode\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     69\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruns_per_episode):\n\u001b[0;32m---> 71\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m EpisodeResult\u001b[38;5;241m.\u001b[39mmerge_all(results)\n",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36mToyEnv.run_instance\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     16\u001b[0m COUNTERS\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning instance step \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, step)\n\u001b[1;32m     19\u001b[0m problem \u001b[38;5;241m=\u001b[39m Data(\n\u001b[1;32m     20\u001b[0m     global_data\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mlen\u001b[39m(deck) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_cfg\u001b[38;5;241m.\u001b[39mnumber_of_cards_to_add),\n\u001b[1;32m     22\u001b[0m     ], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     23\u001b[0m     x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m     24\u001b[0m         deck \u001b[38;5;241m/\u001b[39m (max_card \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;241m*\u001b[39m[np\u001b[38;5;241m.\u001b[39mbitwise_and(deck, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m i) \u001b[38;5;241m>>\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_cfg\u001b[38;5;241m.\u001b[39mnumber_of_bits)],\n\u001b[1;32m     26\u001b[0m     ]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     27\u001b[0m     edge_index\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m     28\u001b[0m         np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(deck)),\n\u001b[1;32m     29\u001b[0m         np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(deck) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(deck),\n\u001b[1;32m     30\u001b[0m     ]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[1;32m     31\u001b[0m     edge_attr\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(deck), \u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m---> 32\u001b[0m     reducible\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreducible\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m ex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_cfg\u001b[38;5;241m.\u001b[39mfraction_to_remove \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(reducible)\n\u001b[1;32m     36\u001b[0m dist, value \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(problem, ex)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "game_cfg = GameConfig()\n",
    "\n",
    "agent = Agent(strategy=\"uniform\")\n",
    "\n",
    "env = ToyEnv(\n",
    "    game_cfg=game_cfg,\n",
    "    runs_per_episode=HP.runs_per_episode,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    print(f\"Episode {COUNTERS.episodes}\")\n",
    "    results = env.run_episode(agent)\n",
    "    logger.info(\"Finished episode, starting training\")\n",
    "    agent.update(results)\n",
    "\n",
    "    print(f\"Rewards: {sum(results.rewards)}\")\n",
    "    COUNTERS.episodes += 1\n",
    "    del results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
